---
title: "Chapter04_Part1"
excerpt: "MachineLearning_Chap04_Part1"

categories:
  - MachineLearning
tags:
  - [Machine Learning, ë¨¸ì‹ ëŸ¬ë‹]

permalink: /categories/MachineLearning/Chapter04_Part2_MachineLearning

toc: true
toc_sticky: true

date: 2025-05-27
last_modified_at: 2025-05-27
---

# ğŸ“˜ Chapter 4: Unsupervised Learning & Dimensionality Reduction

## Part 1. Principal Component Analysis (PCA)
1. Introduction to Dimensionality Reduction
2. Feature Selection vs. Feature Extraction
3. Linear Algebra Review for PCA
4. Principal Component Analysis (PCA)
   - Formulation 1: Linear Approximation
   - Formulation 2: Maximum Variance
   - Equivalence of Two Formulations
   - Eigen-decomposition and Eckartâ€“Young Theorem
5. Choosing the Number of Components
6. Preprocessing and Limitations

---

## Part 2. Model Selection
1. Introduction to Model Selection
2. Tuning Parameters
3. Risk Estimation and Training Error
4. Holdout Method
5. Cross-Validation (K-fold, Stratified)
6. Bootstrap Method
7. Final Model Fitting
8. Beyond Grid Search (Random Search, Bayesian Optimization)

---

## Part 3. k-Means Clustering
1. Introduction to Clustering
2. k-Means Criterion and Algorithm
3. Optimization and Convergence
4. Initialization (Random, k-Means++)
5. Geometric Interpretation
6. Choosing the Number of Clusters

---

## Part 4. Gaussian Mixture Models (GMMs)
1. Limitations of k-Means
2. Gaussian Mixture Model Definition
3. Maximum Likelihood Estimation
4. Expectation-Maximization (EM) Algorithm for GMMs
5. E-Step and M-Step Derivations
6. GMM vs. k-Means

---

## Part 5. Expectation-Maximization Algorithm (General)
1. General EM Framework
2. ELBO (Evidence Lower Bound)
3. KL Divergence Interpretation
4. EM in General Notation
5. Summary and Applications

---

# Part 1. Principal Component Analysis (PCA)

## âœ… ì£¼ìš” ê°œë… íë¦„
PCAëŠ” ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œí•˜ë©´ì„œë„ **ì •ë³´ ì†ì‹¤ì„ ìµœì†Œí™”**í•˜ëŠ” ëŒ€í‘œì ì¸ ë¹„ì§€ë„ í•™ìŠµ ë°©ë²•ì´ë‹¤.  
ë‘ ê°€ì§€ ê´€ì ì—ì„œ ì„¤ëª… ê°€ëŠ¥:
1. **Linear Approximation ê´€ì **: ë°ì´í„°ì— ê°€ì¥ ì˜ ë§ëŠ” kì°¨ì› ì„ í˜• ë¶€ë¶„ê³µê°„ì„ ì°¾ìŒ
2. **Maximum Variance ê´€ì **: ë¶„ì‚°ì„ ê°€ì¥ ë§ì´ ë³´ì¡´í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ íˆ¬ì˜

---

## âœ… ê°œë…ê³¼ ìˆ˜ì‹ ì •ë¦¬

### ğŸ¯ Dimensionality Reduction ëª©ì 
ê³ ì°¨ì› ë°ì´í„° $$ x_1, \ldots, x_n \in \mathbb{R}^d $$ë¥¼  
ì €ì°¨ì› í‘œí˜„ $$ \theta \in \mathbb{R}^k \quad (k < d) $$ë¡œ ë³€í™˜

---

### ğŸ¯ Feature Selection vs. Feature Extraction
- Feature Selection: ê¸°ì¡´ íŠ¹ì„±ì—ì„œ ì¼ë¶€ë§Œ ì„ íƒ
- Feature Extraction: ê¸°ì¡´ íŠ¹ì„±ì˜ **ì„ í˜•/ë¹„ì„ í˜• ê²°í•©**ìœ¼ë¡œ ìƒˆë¡œìš´ íŠ¹ì„± ìƒì„±

---

### ğŸ¯ PCA ì„¤ì •: Linear Approximation ê´€ì 

ê° ë°ì´í„°ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ ê·¼ì‚¬:
$$ x_i \approx \mu + A\theta_i $$

- $$ \mu \in \mathbb{R}^d $$: í‰ê·  ë²¡í„°
- $$ A \in \mathbb{R}^{d \times k}, \quad A^\top A = I_k $$: ì •ê·œ ì§êµ ê¸°ì € (orthonormal)
- $$ \theta_i \in \mathbb{R}^k $$: íˆ¬ì˜ ì¢Œí‘œ

---

### ğŸ¯ PCA ìµœì í™” ë¬¸ì œ (Least Squares Formulation)

$$
\min_{\mu, A, \{\theta_i\}} \sum_{i=1}^n \left\| x_i - \mu - A \theta_i \right\|^2
$$

---

### ğŸ¯ í•´ (Solution)

- í‰ê· :  
  $$ \mu = \bar{x} = \frac{1}{n} \sum_{i=1}^n x_i $$
- íˆ¬ì˜ ì¢Œí‘œ:  
  $$ \theta_i = A^\top (x_i - \bar{x}) $$
- íˆ¬ì˜ ê¸°ì €:  
  $$ A = [u_1, \ldots, u_k] $$  
  â†’ ê³µë¶„ì‚° í–‰ë ¬ $$ S $$ì˜ ìƒìœ„ kê°œ ê³ ìœ ë²¡í„°

---

### ğŸ¯ PCA = ìµœì†Œ ì œê³± ê·¼ì‚¬ (Eckartâ€“Young Theorem)

SVD ë¶„í•´:  
$$ X = U\Sigma V^\top $$  
ìƒìœ„ kê°œ ì„±ë¶„ë§Œ ìœ ì§€í•œ ê·¼ì‚¬ í–‰ë ¬:  
$$ \hat{X} = U_k \Sigma_k V_k^\top $$  
â†’ ê°€ì¥ ë‚®ì€ ì˜¤ë¥˜ì˜ rank-k ê·¼ì‚¬

---

### ğŸ¯ PCA = ë¶„ì‚° ê·¹ëŒ€í™” ë¬¸ì œ

ëª©í‘œ: íˆ¬ì˜ëœ ë°ì´í„°ì˜ ì´ ë¶„ì‚° ìµœëŒ€í™”  
$$
\max_{A \in \mathbb{R}^{d \times k}, A^\top A = I_k} \operatorname{tr}(A^\top S A)
$$

- $$ S $$: ê³µë¶„ì‚° í–‰ë ¬  
  $$ S = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(x_i - \bar{x})^\top $$

- í•´:  
  $$ A = [u_1, \ldots, u_k] $$ (Sì˜ ìƒìœ„ kê°œ ê³ ìœ ë²¡í„°)

- ë¶„ì‚° ì´í•©:  
  $$ \sum_{j=1}^k \lambda_j $$

---

### ğŸ¯ PCAì˜ ë‘ ê´€ì ì˜ ë“±ê°€ì„±

- Reconstruction error ìµœì†Œí™”  
  $$ \| X - AA^\top X \|_F^2 \longleftrightarrow \min $$
- íˆ¬ì˜ ë¶„ì‚° ìµœëŒ€í™”  
  $$ \operatorname{tr}(A^\top S A) \longleftrightarrow \max $$

â†’ ë™ì¼í•œ A (top-k eigenvectors) ì–»ìŒ

---

### ğŸ¯ Component ìˆ˜ k ì„ íƒ ê¸°ì¤€

ì „ì²´ ë¶„ì‚° ì¤‘ ë¹„ìœ¨:  
$$
\frac{\lambda_1 + \cdots + \lambda_k}{\lambda_1 + \cdots + \lambda_d} \geq 0.95
$$

â†’ 95% ì´ìƒ ë¶„ì‚°ì„ ì„¤ëª…í•˜ëŠ” ìµœì†Œ k ì„ íƒ

---

### ğŸ¯ PCA ì „ì²˜ë¦¬

- **Centering** (í‰ê·  ì œê±°):  
  $$ x_i \leftarrow x_i - \bar{x} $$
- **Scaling** (í‘œì¤€í™”):  
  $$ x_i \leftarrow \frac{x_i}{\sigma} $$

â†’ feature ê°„ scale ì°¨ì´ ì œê±°

---

### ğŸ¯ PCAì˜ í•œê³„ì™€ í™•ì¥

- ì„ í˜•ì„±ë§Œ ëª¨ë¸ë§ ê°€ëŠ¥ â†’ ì›í˜•, ê³¡ì„ í˜• êµ¬ì¡° ì„¤ëª… ë¶ˆê°€
- í™•ì¥: **Kernel PCA**, **Nonlinear PCA**

---

## âœ… ì •ë¦¬ í¬ì¸íŠ¸
- PCAëŠ” ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì› ì„ í˜• ê³µê°„ìœ¼ë¡œ íˆ¬ì˜í•˜ì—¬ ì •ë³´ë¥¼ ìµœëŒ€í•œ ë³´ì¡´
- ë‘ ê´€ì : **ì¬êµ¬ì„± ì˜¤ë¥˜ ìµœì†Œí™” vs. ë¶„ì‚° ìµœëŒ€í™”** â†’ ë™ì¼í•œ í•´
- ê³µë¶„ì‚° í–‰ë ¬ì˜ ê³ ìœ ê°’ ë¶„í•´ë¥¼ í†µí•´ ì£¼ì„±ë¶„ ë°©í–¥ ì¶”ì¶œ
- ë°ì´í„° ì •ê·œí™” ë° ê³ ìœ ê°’ ëˆ„ì  ë¹„ìœ¨ë¡œ k ì„ íƒ

---

# Part 2. Model Selection

## âœ… ì£¼ìš” ê°œë… íë¦„
Model Selectionì€ ì£¼ì–´ì§„ ë°ì´í„°ì—ì„œ ê°€ì¥ ì¼ë°˜í™” ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ ë˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„ íƒí•˜ëŠ” ê³¼ì •ì´ë‹¤.  
Training Errorë§Œìœ¼ë¡œ íŒë‹¨í•˜ë©´ Overfitting ìœ„í—˜ì´ ìˆìœ¼ë¯€ë¡œ, **ì¶”ì •ëœ ì¼ë°˜í™” ì˜¤ë¥˜(Risk)** ê¸°ë°˜ì˜ í‰ê°€ê°€ ì¤‘ìš”í•˜ë‹¤.  
ì´ ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì£¼ìš” ë°©ë²•ì€ Holdout, Cross-Validation, Bootstrap ë“±ì´ë‹¤.

---

## âœ… ê°œë…ê³¼ ìˆ˜ì‹ ì •ë¦¬

### ğŸ¯ ëª¨ë¸ ì„ íƒì´ë€?
- ì—¬ëŸ¬ í›„ë³´ ëª¨ë¸ ì¤‘ **ê°€ì¥ ì ì ˆí•œ ëª¨ë¸**ì„ ì„ íƒí•˜ëŠ” ê²ƒ
- Overfittingê³¼ Underfitting ê°„ì˜ trade-off ì¡°ì ˆ

---

### ğŸ¯ íŠœë‹ íŒŒë¼ë¯¸í„°ë€?

- í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì´ ì§ì ‘ í•™ìŠµí•˜ì§€ ì•ŠëŠ” ì™¸ë¶€ ì„¤ì •ê°’
- ì˜ˆì‹œ:
  - $$ \lambda $$ (regularization in ridge)
  - $$ \sigma $$ (kernel width)
  - $$ k $$ (in k-NN)

---

### ğŸ¯ Risk ì •ì˜

- **True Risk (expected risk)**:
$$
R(\hat{f}_\theta) = \mathbb{E}_{X,Y} \left[ L(Y, \hat{f}_\theta(X)) \right]
$$

- ë¬¸ì œ: ì‹¤ì œ ë¶„í¬ë¥¼ ëª¨ë¥´ë¯€ë¡œ $$ R(\hat{f}_\theta) $$ëŠ” ê³„ì‚° ë¶ˆê°€

---

### ğŸ¯ ëŒ€ì•ˆ: Empirical Risk (í›ˆë ¨ ì˜¤ì°¨)

- **Training Error**:
$$
\hat{R}_{\text{train}}(\hat{f}_\theta) = \frac{1}{n} \sum_{i=1}^n L(y_i, \hat{f}_\theta(x_i))
$$

- ë¬¸ì œì : í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ê³¼ì í•© ë°œìƒ ê°€ëŠ¥

---

### ğŸ¯ Holdout Validation

1. ë°ì´í„° $$ D $$ë¥¼ train / validationìœ¼ë¡œ ë¶„í• : $$ D = D_{\text{train}} \cup D_{\text{val}} $$
2. $$ \hat{f}_\theta $$ í•™ìŠµ â†’ validation setì—ì„œ ì„±ëŠ¥ ì¸¡ì •

- **Holdout Risk Estimate**:
$$
\hat{R}_{\text{holdout}}(\hat{f}_\theta) = \frac{1}{|D_{\text{val}}|} \sum_{(x_i, y_i) \in D_{\text{val}}} L(y_i, \hat{f}_\theta(x_i))
$$

- ì¥ì : êµ¬í˜„ ê°„ë‹¨
- ë‹¨ì : ë°ì´í„° ë‚­ë¹„, ë¶„í• ì— ë”°ë¼ í¸í–¥ í¼

---

### ğŸ¯ K-Fold Cross-Validation

1. ë°ì´í„°ë¥¼ Kê°œ ë¸”ë¡ìœ¼ë¡œ ë‚˜ëˆ”
2. ê° foldë¥¼ validationìœ¼ë¡œ ì‚¬ìš©í•˜ë©° ë‚˜ë¨¸ì§€ë¡œ í•™ìŠµ
3. í‰ê·  ì˜¤ì°¨ ê³„ì‚°

- **K-Fold Estimate**:
$$
\hat{R}_{\text{CV}}(\hat{f}_\theta) = \frac{1}{K} \sum_{j=1}^K \hat{R}^{(j)}(\hat{f}_\theta^{(j)})
$$

- ì¼ë°˜ì ìœ¼ë¡œ $$ K=5 $$ ë˜ëŠ” $$ 10 $$ ì‚¬ìš©
- **Stratified CV**: í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€ â†’ ë¶„ë¥˜ ë¬¸ì œì—ì„œ ì¤‘ìš”

---

### ğŸ¯ Bootstrap Method

1. ë°ì´í„°ì—ì„œ Bë²ˆ ë³µì› ì¶”ì¶œë¡œ í•™ìŠµ ë°ì´í„° ìƒì„±
2. OOB(Out-Of-Bag) ë°ì´í„°ë¡œ í‰ê°€
3. í‰ê· í•˜ì—¬ ìµœì¢… ìœ„í—˜ ì¶”ì •

- **Bootstrap Risk**:
$$
\hat{R}_{\text{BS}}(\hat{f}_\theta) = \frac{1}{B} \sum_{b=1}^B \hat{R}^{(b)}_{\text{OOB}}(\hat{f}_\theta^{(b)})
$$

- ì¥ì : ë°ì´í„° í™œìš© íš¨ìœ¨ â†‘
- ë‹¨ì : ê³„ì‚°ëŸ‰ ë§ê³  ì˜¤ì°¨ ê³¼ì¶”ì • ê²½í–¥

---

### ğŸ¯ ìµœì¢… ëª¨ë¸ í•™ìŠµ

- íŠœë‹ íŒŒë¼ë¯¸í„° ì„ íƒ í›„ â†’ **ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ**  
â†’ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ ê¸°ëŒ€

---

### ğŸ¯ Beyond Grid Search

- **Grid Search**: ì „ë¶€ íƒìƒ‰ â†’ ë¹„íš¨ìœ¨
- **Random Search**: ëœë¤ ìƒ˜í”Œë§ â†’ íš¨ìœ¨ì 
- **Bayesian Optimization**: ì´ì „ í‰ê°€ ê²°ê³¼ ê¸°ë°˜ íƒìƒ‰
- **Hyperband**: Random + Early Stopping

---

## âœ… ì •ë¦¬ í¬ì¸íŠ¸

- ëª¨ë¸ ì„ íƒì€ **ì¼ë°˜í™” ì˜¤ë¥˜ ìµœì†Œí™”**ê°€ í•µì‹¬
- Validation set ë˜ëŠ” Cross-Validationìœ¼ë¡œ **true risk ì¶”ì •**
- Grid Search ëŒ€ì‹  **Random / Bayesian / Hyperband** ì¶”ì²œ
- ìµœì¢… ëª¨ë¸ì€ **ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ**

---

# Part 3. k-Means Clustering

## âœ… ì£¼ìš” ê°œë… íë¦„
k-MeansëŠ” ëŒ€í‘œì ì¸ **ë¹„ì§€ë„ í•™ìŠµ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜**ìœ¼ë¡œ, ê° ë°ì´í„°ë¥¼ kê°œì˜ êµ°ì§‘ìœ¼ë¡œ ë¶„í• í•˜ëŠ” ê²ƒì´ ëª©í‘œë‹¤.  
"êµ°ì§‘ ë‚´ ê±°ë¦¬ ì œê³±í•© ìµœì†Œí™”"ë¥¼ í†µí•´ ìœ ì‚¬í•œ ë°ì´í„°ë¼ë¦¬ ë¬¶ëŠ”ë‹¤.  
ê°„ë‹¨í•˜ì§€ë§Œ ë¹ ë¥´ê³  ì§ê´€ì ì´ë©°, ë²¡í„° ì–‘ìí™”, ì „ì²˜ë¦¬ ë“±ì— ë„ë¦¬ í™œìš©ëœë‹¤.

---

## âœ… ê°œë…ê³¼ ìˆ˜ì‹ ì •ë¦¬

### ğŸ¯ Clusteringì´ë€?

- ì…ë ¥: $$ \mathbf{x}_1, \dots, \mathbf{x}_n \in \mathbb{R}^d $$
- ì¶œë ¥: êµ°ì§‘ í•¨ìˆ˜ $$ C: \{1, \dots, n\} \to \{1, \dots, k\} $$

ëª©í‘œ: ê°™ì€ í´ëŸ¬ìŠ¤í„° ë‚´ì—ì„œëŠ” ìœ ì‚¬ë„ â†‘, ì„œë¡œ ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ëŠ” ìœ ì‚¬ë„ â†“

---

### ğŸ¯ k-Means Objective (Within-Cluster Sum of Squares)

$$
W(C) = \sum_{\ell=1}^k \sum_{i: C(i)=\ell} \left\| \mathbf{x}_i - \bar{\mathbf{x}}_\ell \right\|^2
$$

- $$ \bar{\mathbf{x}}_\ell $$: í´ëŸ¬ìŠ¤í„° $$ \ell $$ì˜ ì¤‘ì‹¬ (centroid)  
$$ \bar{\mathbf{x}}_\ell = \frac{1}{n_\ell} \sum_{j: C(j)=\ell} \mathbf{x}_j $$

---

### ğŸ¯ k-Means ì•Œê³ ë¦¬ì¦˜ (Lloydâ€™s Algorithm)

1. ì´ˆê¸° í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ $$ \{m_1, \dots, m_k\} $$ ì„¤ì •
2. **Assignment step**: ê° $$ \mathbf{x}_i $$ë¥¼ ê°€ì¥ ê°€ê¹Œìš´ ì¤‘ì‹¬ì— í• ë‹¹
   $$ C(i) = \arg\min_\ell \| \mathbf{x}_i - m_\ell \| $$
3. **Update step**: ì¤‘ì‹¬ ì¬ê³„ì‚°
   $$ m_\ell = \frac{1}{n_\ell} \sum_{i: C(i)=\ell} \mathbf{x}_i $$
4. ìˆ˜ë ´í•  ë•Œê¹Œì§€ ë°˜ë³µ

---

### ğŸ¯ ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜ë ´ ë³´ì¥

- ë§¤ ë°˜ë³µë§ˆë‹¤ $$ W(C) $$ëŠ” ê°ì†Œ or ìœ ì§€
- êµ­ì†Œ ìµœì†Ÿê°’(local minimum)ì— ìˆ˜ë ´
- ì „ì²´ ìµœì  í•´ ë³´ì¥ì€ ì—†ìŒ (ì´ˆê¸°í™”ì— ë”°ë¼ ë‹¬ë¼ì§)

---

### ğŸ¯ Assignment Stepì˜ ìˆ˜í•™ì  ì •ë‹¹ì„±

- ê° ë°ì´í„° $$ \mathbf{x}_i $$ëŠ” ê°€ì¥ ê°€ê¹Œìš´ ì¤‘ì‹¬ì— í• ë‹¹ë¨:
$$
\left\| \mathbf{x}_i - m_{C(i)} \right\|^2 \leq \left\| \mathbf{x}_i - m_\ell \right\|^2 \quad \forall \ell
$$

â†’ ê° ìŠ¤í…ì—ì„œ $$ W(C) $$ëŠ” ê°ì†Œ

---

### ğŸ¯ Update Stepì˜ ìˆ˜í•™ì  ì •ë‹¹ì„±

- ê³ ì •ëœ í• ë‹¹ $$ C(i) $$ í•˜ì—ì„œ ìµœì  ì¤‘ì‹¬ $$ m_\ell $$ëŠ” í‰ê· ê°’:
$$
m_\ell = \arg\min_m \sum_{i: C(i)=\ell} \| \mathbf{x}_i - m \|^2 = \bar{\mathbf{x}}_\ell
$$

---

### ğŸ¯ k-Means++ ì´ˆê¸°í™”

- ì²« ì¤‘ì‹¬ì€ ëœë¤ìœ¼ë¡œ ì„ íƒ
- ì´í›„ ê° ë°ì´í„° $$ \mathbf{x} $$ëŠ” ê¸°ì¡´ ì¤‘ì‹¬ê³¼ì˜ ê±°ë¦¬ $$ D(\mathbf{x})^2 $$ì— ë¹„ë¡€í•˜ì—¬ ì„ íƒ

â†’ ì˜ í¼ì§„ ì´ˆê¸° ì¤‘ì‹¬ í™•ë³´ â†’ ì„±ëŠ¥ í–¥ìƒ

---

### ğŸ¯ í´ëŸ¬ìŠ¤í„° ê¸°í•˜í•™ì  ì„±ì§ˆ

- í´ëŸ¬ìŠ¤í„°ëŠ” ì„ í˜• ë¶„ë¦¬ (Voronoi Partition)
- ëª¨ë“  í´ëŸ¬ìŠ¤í„°ëŠ” **ë³¼ë¡ì§‘í•©(convex set)**

---

### ğŸ¯ í´ëŸ¬ìŠ¤í„° ìˆ˜ k ì„ íƒ

- ì •í•´ì§„ kì— ë”°ë¼ ê²°ê³¼ ë‹¬ë¼ì§
- **Elbow Method**:  
  - $$ W(C) $$ vs. k ê·¸ë˜í”„ ê·¸ë ¤ì„œ êº¾ì´ëŠ” ì (elbow) ì°¾ê¸°

---

### ğŸ¯ k-Meansì˜ í•œê³„ì 

- í´ëŸ¬ìŠ¤í„°ê°€ êµ¬í˜•(spherical), ë“± í¬ê¸°ë¼ê³  ê°€ì •
- ì´ˆê¸°í™” ë¯¼ê°ë„ ë†’ìŒ
- k í•„ìš”
- ì´ìƒì¹˜(outlier)ì— ë¯¼ê°

---

## âœ… ì •ë¦¬ í¬ì¸íŠ¸

- k-MeansëŠ” **Within-cluster variance** ìµœì†Œí™”
- Lloyd ì•Œê³ ë¦¬ì¦˜ì€ Assignment + Update ë°˜ë³µ
- k-Means++ë¡œ ì´ˆê¸°í™” ì„±ëŠ¥ í–¥ìƒ
- í´ëŸ¬ìŠ¤í„° ìˆ˜ ì„ íƒì€ Elbow Method ë“± í™œìš©
- GMMì„ í†µí•´ soft assignmentì™€ ë¹„ì •ê·œ í˜•íƒœ í™•ì¥ ê°€ëŠ¥

---

# Part 4. Gaussian Mixture Models (GMMs)

## âœ… ì£¼ìš” ê°œë… íë¦„
k-MeansëŠ” ë‹¨ìˆœí•˜ê³  íš¨ìœ¨ì ì´ì§€ë§Œ, í´ëŸ¬ìŠ¤í„°ê°€ **êµ¬í˜•ì´ë©° ê°™ì€ í¬ê¸°**ë¼ëŠ” ì œí•œì„ ê°€ì§.  
GMMì€ ë°ì´í„°ë¥¼ **ì—¬ëŸ¬ ê°œì˜ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¡œ ëª¨ë¸ë§**í•˜ì—¬, ë¹„ì •í˜•/ë¹„ëŒ€ì¹­/ì¤‘ì²©ëœ êµ°ì§‘ë„ ì²˜ë¦¬ ê°€ëŠ¥.  
GMMì€ soft assignment(í™•ë¥  ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§)ë¥¼ ì œê³µí•˜ë©°, **EM ì•Œê³ ë¦¬ì¦˜**ìœ¼ë¡œ í•™ìŠµëœë‹¤.

---

## âœ… ê°œë…ê³¼ ìˆ˜ì‹ ì •ë¦¬

### ğŸ¯ Multivariate Gaussian Distribution

$$
\phi(\mathbf{x}; \mu, \Sigma) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2} (\mathbf{x} - \mu)^\top \Sigma^{-1} (\mathbf{x} - \mu) \right)
$$

- $$ \mu \in \mathbb{R}^d $$: í‰ê·  ë²¡í„°
- $$ \Sigma \in \mathbb{R}^{d \times d} $$: ê³µë¶„ì‚° í–‰ë ¬

---

### ğŸ¯ Gaussian Mixture Model (GMM)

$$
f(\mathbf{x}) = \sum_{k=1}^K w_k \cdot \phi(\mathbf{x}; \mu_k, \Sigma_k)
$$

- $$ w_k \geq 0,\ \sum w_k = 1 $$
- $$ \mu_k, \Sigma_k $$: ê° componentì˜ í‰ê· ê³¼ ê³µë¶„ì‚°

---

### ğŸ¯ Soft Assignment (Responsibilities)

ë°ì´í„° $$ \mathbf{x}_i $$ê°€ í´ëŸ¬ìŠ¤í„° $$ k $$ì— ì†í•  í™•ë¥ :

$$
\gamma_{i,k} = \frac{w_k \cdot \phi(\mathbf{x}_i; \mu_k, \Sigma_k)}{\sum_{l=1}^K w_l \cdot \phi(\mathbf{x}_i; \mu_l, \Sigma_l)}
$$

---

### ğŸ¯ Log-Likelihood

$$
\ell(\theta;\mathcal{X}) = \sum_{i=1}^n \log \left( \sum_{k=1}^K w_k \cdot \phi(\mathbf{x}_i; \mu_k, \Sigma_k) \right)
$$

â†’ ì§ì ‘ ìµœì í™” ë¶ˆê°€ â†’ EM ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©

---

### ğŸ¯ EM ì•Œê³ ë¦¬ì¦˜ ê°œìš” (GMM í•™ìŠµ)

1. **E-step**: Soft assignment ê³„ì‚°
   $$
   \gamma_{i,k}^{(t)} = \frac{w_k^{(t)} \cdot \phi(\mathbf{x}_i; \mu_k^{(t)}, \Sigma_k^{(t)})}{\sum_{\ell=1}^K w_\ell^{(t)} \cdot \phi(\mathbf{x}_i; \mu_\ell^{(t)}, \Sigma_\ell^{(t)})}
   $$

2. **M-step**: íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
   - Mixing coefficient:
     $$
     w_k^{(t+1)} = \frac{1}{n} \sum_{i=1}^n \gamma_{i,k}^{(t)}
     $$
   - Mean:
     $$
     \mu_k^{(t+1)} = \frac{\sum_{i=1}^n \gamma_{i,k}^{(t)} \mathbf{x}_i}{\sum_{i=1}^n \gamma_{i,k}^{(t)}}
     $$
   - Covariance:
     $$
     \Sigma_k^{(t+1)} = \frac{\sum_{i=1}^n \gamma_{i,k}^{(t)} (\mathbf{x}_i - \mu_k^{(t+1)})(\mathbf{x}_i - \mu_k^{(t+1)})^\top}{\sum_{i=1}^n \gamma_{i,k}^{(t)}}
     $$

3. ë°˜ë³µ until ìˆ˜ë ´ (log-likelihood ì¦ê°€ëŸ‰ < Îµ)

---

### ğŸ¯ GMM vs. k-Means

| í•­ëª© | k-Means | GMM |
|------|---------|-----|
| í• ë‹¹ ë°©ì‹ | Hard (1 or 0) | Soft (í™•ë¥ ) |
| í´ëŸ¬ìŠ¤í„° í˜•íƒœ | êµ¬í˜•(spherical) | íƒ€ì›í˜•(general Gaussian) |
| ëª¨ë¸ë§ | ê±°ë¦¬ ê¸°ë°˜ | í™•ë¥  ëª¨ë¸ ê¸°ë°˜ |
| ëª©ì í•¨ìˆ˜ | Within-cluster SSE | Log-likelihood ìµœëŒ€í™” |
| ì•Œê³ ë¦¬ì¦˜ | Lloydâ€™s Algorithm | EM Algorithm |

---

### ğŸ¯ GMMì´ k-Meansë¥¼ ì¼ë°˜í™”í•˜ëŠ” ê²½ìš°

- $$ \Sigma_k = \sigma^2 I,\ \sigma^2 \to 0 $$ ì´ë©´,
$$
\gamma_{i,k} \to \begin{cases}
1, & k = \arg\min_\ell \| \mathbf{x}_i - \mu_\ell \| \\
0, & \text{otherwise}
\end{cases}
$$

â†’ EM ì•Œê³ ë¦¬ì¦˜ì´ k-Means ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìˆ˜ë ´

---

## âœ… ì •ë¦¬ í¬ì¸íŠ¸

- GMMì€ ë‹¤ì–‘í•œ ëª¨ì–‘/í¬ê¸°ì˜ êµ°ì§‘ì„ ëª¨ë¸ë§í•  ìˆ˜ ìˆëŠ” **í™•ë¥ ì  í´ëŸ¬ìŠ¤í„°ë§ ëª¨ë¸**
- EM ì•Œê³ ë¦¬ì¦˜ì€ **E-step (soft assignment)**ê³¼ **M-step (parameter update)**ë¥¼ ë°˜ë³µ
- GMMì€ k-Meansë³´ë‹¤ ìœ ì—°í•˜ë©°, ì‹¤ì œ ë°ì´í„° ë¶„í¬ì— ëŒ€í•œ í™•ë¥  ëª¨ë¸ë§ ê°€ëŠ¥
- k-MeansëŠ” GMMì˜ íŠ¹ìˆ˜í•œ ê²½ìš°ë¡œ í•´ì„ ê°€ëŠ¥

---

# Part 5. Expectation-Maximization (EM) Algorithm

## âœ… ì£¼ìš” ê°œë… íë¦„
EM ì•Œê³ ë¦¬ì¦˜ì€ **ì ì¬ë³€ìˆ˜(latent variable)**ê°€ ì¡´ì¬í•˜ëŠ” í™•ë¥  ëª¨ë¸ì—ì„œ  
**ìµœëŒ€ìš°ë„ì¶”ì •(Maximum Likelihood Estimation)**ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ë°˜ë³µì  ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.  
GMMì„ í¬í•¨í•œ ë‹¤ì–‘í•œ ëª¨ë¸ì— ì ìš©ë˜ë©°, ë§¤ ë°˜ë³µë§ˆë‹¤ **log-likelihoodê°€ ì¦ê°€**í•˜ëŠ” ì„±ì§ˆì„ ê°€ì§„ë‹¤.

---

## âœ… ê°œë…ê³¼ ìˆ˜ì‹ ì •ë¦¬

### ğŸ¯ ë¬¸ì œ ì„¸íŒ…

- ê´€ì¸¡ ë°ì´í„°: $$ \{x_i\}_{i=1}^n $$
- ì ì¬ ë³€ìˆ˜ (ìˆ¨ê²¨ì§„ ë³€ìˆ˜): $$ z $$
- ëª¨ë¸ íŒŒë¼ë¯¸í„°: $$ \theta $$
- ëª©í‘œ: ìµœëŒ€í™”  
$$ \ell(\theta; x) = \log p(x; \theta) = \log \int p(x, z; \theta) dz $$

â†’ ì§ì ‘ ê³„ì‚° ì–´ë ¤ì›€ (ì ë¶„ ì•ˆ ë¨)

---

### ğŸ¯ Evidence Lower Bound (ELBO)

ë¶„í•´:
$$
\log p(x;\theta) = \log \int q(z) \frac{p(x, z; \theta)}{q(z)} dz
= \log \mathbb{E}_{q(z)} \left[ \frac{p(x, z; \theta)}{q(z)} \right]
$$

â†’ Jensen's Inequality ì ìš©:
$$
\log p(x;\theta) \geq \mathbb{E}_{q(z)} \left[ \log \frac{p(x, z; \theta)}{q(z)} \right] =: \text{ELBO}(q, \theta)
$$

---

### ğŸ¯ KL Divergenceë¡œ í‘œí˜„

$$
\log p(x;\theta) = \text{ELBO}(q, \theta) + \text{KL}(q(z) \| p(z \mid x; \theta))
$$

â†’ ELBOê°€ í´ìˆ˜ë¡ log-likelihoodì— ê°€ê¹Œì›Œì§  
â†’ E-stepì—ì„œ KLì„ ìµœì†Œí™”, M-stepì—ì„œ ELBOë¥¼ ìµœëŒ€í™”

---

### ğŸ¯ EM Algorithm ê³¼ì •

1. **E-step (ì ì¬ë³€ìˆ˜ ë¶„í¬ ì¶”ì •)**  
   í˜„ì¬ íŒŒë¼ë¯¸í„° $$ \theta^{(t)} $$ì—ì„œ posterior ê³„ì‚°:  
   $$
   q^{(t)}(z) = p(z \mid x; \theta^{(t)})
   $$

2. **M-step (íŒŒë¼ë¯¸í„° ìµœì í™”)**  
   E-stepì—ì„œ ì–»ì€ $$ q^{(t)}(z) $$ ê³ ì • í›„, íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸:  
   $$
   \theta^{(t+1)} = \arg\max_\theta \mathbb{E}_{q^{(t)}(z)}[\log p(x, z; \theta)]
   $$

---

### ğŸ¯ EM ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜ë ´ ë³´ì¥ (Ascent Property)

- ê° ë‹¨ê³„ì—ì„œ log-likelihoodëŠ” ê°ì†Œí•˜ì§€ ì•ŠìŒ:
$$
\log p(x; \theta^{(t+1)}) \geq \log p(x; \theta^{(t)})
$$

â†’ êµ­ì†Œ ìµœëŒ“ê°’ìœ¼ë¡œ ìˆ˜ë ´ ë³´ì¥

---

### ğŸ¯ ì—¬ëŸ¬ ë°ì´í„°ì˜ ê²½ìš° (iid ë°ì´í„°)

- ê´€ì¸¡: $$ \{x_i\}_{i=1}^n $$
- ì ì¬ë³€ìˆ˜: $$ \{z_i\}_{i=1}^n $$
- ì „ì²´ log-likelihood:
$$
\ell(\theta) = \sum_{i=1}^n \log p(x_i; \theta)
$$

- ELBO:
$$
\sum_{i=1}^n \mathbb{E}_{q_i(z_i)} \left[ \log p(x_i, z_i; \theta) - \log q_i(z_i) \right]
$$

---

### ğŸ¯ EMê³¼ KL Divergence ê´€ê³„

- ì•„ë˜ í•­ë“±ì‹ í•­ìƒ ì„±ë¦½:
$$
\log p(x; \theta) = \text{ELBO}(q, \theta) + \text{KL}(q(z) \| p(z \mid x; \theta))
$$

â†’ E-stepì€ KLì„ 0ìœ¼ë¡œ ë§Œë“¤ê³ ,  
â†’ M-stepì€ ELBOë¥¼ ìµœì í™”í•¨

---

## âœ… ì •ë¦¬ í¬ì¸íŠ¸

- EMì€ ì ì¬ë³€ìˆ˜ë¥¼ í¬í•¨í•œ í™•ë¥ ëª¨í˜•ì—ì„œ ìµœëŒ€ìš°ë„ ì¶”ì •ì„ ìœ„í•œ ìµœì í™” ê¸°ë²•
- E-step: posterior ê³„ì‚° â†’ q ì—…ë°ì´íŠ¸
- M-step: q ê³ ì • í›„ Î¸ ì—…ë°ì´íŠ¸
- ë§¤ ë°˜ë³µë§ˆë‹¤ log-likelihood ì¦ê°€ â†’ ìˆ˜ë ´ ë³´ì¥
- GMM, HMM, ê²°ì¸¡ê°’ ë³´ì • ë“± ë‹¤ì–‘í•œ ëª¨ë¸ì— í™œìš© ê°€ëŠ¥

---

# Part 5. Expectation-Maximization (EM) Algorithm

## âœ… ì£¼ìš” ê°œë… íë¦„
EM ì•Œê³ ë¦¬ì¦˜ì€ **ì ì¬ë³€ìˆ˜(latent variable)**ê°€ ì¡´ì¬í•˜ëŠ” í™•ë¥  ëª¨ë¸ì—ì„œ  
**ìµœëŒ€ìš°ë„ì¶”ì •(Maximum Likelihood Estimation)**ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ë°˜ë³µì  ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.  
GMMì„ í¬í•¨í•œ ë‹¤ì–‘í•œ ëª¨ë¸ì— ì ìš©ë˜ë©°, ë§¤ ë°˜ë³µë§ˆë‹¤ **log-likelihoodê°€ ì¦ê°€**í•˜ëŠ” ì„±ì§ˆì„ ê°€ì§„ë‹¤.

---

## âœ… ê°œë…ê³¼ ìˆ˜ì‹ ì •ë¦¬

### ğŸ¯ ë¬¸ì œ ì„¸íŒ…

- ê´€ì¸¡ ë°ì´í„°: $$ \{x_i\}_{i=1}^n $$
- ì ì¬ ë³€ìˆ˜ (ìˆ¨ê²¨ì§„ ë³€ìˆ˜): $$ z $$
- ëª¨ë¸ íŒŒë¼ë¯¸í„°: $$ \theta $$
- ëª©í‘œ: ìµœëŒ€í™”  
$$ \ell(\theta; x) = \log p(x; \theta) = \log \int p(x, z; \theta) dz $$

â†’ ì§ì ‘ ê³„ì‚° ì–´ë ¤ì›€ (ì ë¶„ ì•ˆ ë¨)

---

### ğŸ¯ Evidence Lower Bound (ELBO)

ë¶„í•´:
$$
\log p(x;\theta) = \log \int q(z) \frac{p(x, z; \theta)}{q(z)} dz
= \log \mathbb{E}_{q(z)} \left[ \frac{p(x, z; \theta)}{q(z)} \right]
$$

â†’ Jensen's Inequality ì ìš©:
$$
\log p(x;\theta) \geq \mathbb{E}_{q(z)} \left[ \log \frac{p(x, z; \theta)}{q(z)} \right] =: \text{ELBO}(q, \theta)
$$

---

### ğŸ¯ KL Divergenceë¡œ í‘œí˜„

$$
\log p(x;\theta) = \text{ELBO}(q, \theta) + \text{KL}(q(z) \| p(z \mid x; \theta))
$$

â†’ ELBOê°€ í´ìˆ˜ë¡ log-likelihoodì— ê°€ê¹Œì›Œì§  
â†’ E-stepì—ì„œ KLì„ ìµœì†Œí™”, M-stepì—ì„œ ELBOë¥¼ ìµœëŒ€í™”

---

### ğŸ¯ EM Algorithm ê³¼ì •

1. **E-step (ì ì¬ë³€ìˆ˜ ë¶„í¬ ì¶”ì •)**  
   í˜„ì¬ íŒŒë¼ë¯¸í„° $$ \theta^{(t)} $$ì—ì„œ posterior ê³„ì‚°:  
   $$
   q^{(t)}(z) = p(z \mid x; \theta^{(t)})
   $$

2. **M-step (íŒŒë¼ë¯¸í„° ìµœì í™”)**  
   E-stepì—ì„œ ì–»ì€ $$ q^{(t)}(z) $$ ê³ ì • í›„, íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸:  
   $$
   \theta^{(t+1)} = \arg\max_\theta \mathbb{E}_{q^{(t)}(z)}[\log p(x, z; \theta)]
   $$

---

### ğŸ¯ EM ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜ë ´ ë³´ì¥ (Ascent Property)

- ê° ë‹¨ê³„ì—ì„œ log-likelihoodëŠ” ê°ì†Œí•˜ì§€ ì•ŠìŒ:
$$
\log p(x; \theta^{(t+1)}) \geq \log p(x; \theta^{(t)})
$$

â†’ êµ­ì†Œ ìµœëŒ“ê°’ìœ¼ë¡œ ìˆ˜ë ´ ë³´ì¥

---

### ğŸ¯ ì—¬ëŸ¬ ë°ì´í„°ì˜ ê²½ìš° (iid ë°ì´í„°)

- ê´€ì¸¡: $$ \{x_i\}_{i=1}^n $$
- ì ì¬ë³€ìˆ˜: $$ \{z_i\}_{i=1}^n $$
- ì „ì²´ log-likelihood:
$$
\ell(\theta) = \sum_{i=1}^n \log p(x_i; \theta)
$$

- ELBO:
$$
\sum_{i=1}^n \mathbb{E}_{q_i(z_i)} \left[ \log p(x_i, z_i; \theta) - \log q_i(z_i) \right]
$$

---

### ğŸ¯ EMê³¼ KL Divergence ê´€ê³„

- ì•„ë˜ í•­ë“±ì‹ í•­ìƒ ì„±ë¦½:
$$
\log p(x; \theta) = \text{ELBO}(q, \theta) + \text{KL}(q(z) \| p(z \mid x; \theta))
$$

â†’ E-stepì€ KLì„ 0ìœ¼ë¡œ ë§Œë“¤ê³ ,  
â†’ M-stepì€ ELBOë¥¼ ìµœì í™”í•¨

---

## âœ… ì •ë¦¬ í¬ì¸íŠ¸

- EMì€ ì ì¬ë³€ìˆ˜ë¥¼ í¬í•¨í•œ í™•ë¥ ëª¨í˜•ì—ì„œ ìµœëŒ€ìš°ë„ ì¶”ì •ì„ ìœ„í•œ ìµœì í™” ê¸°ë²•
- E-step: posterior ê³„ì‚° â†’ q ì—…ë°ì´íŠ¸
- M-step: q ê³ ì • í›„ Î¸ ì—…ë°ì´íŠ¸
- ë§¤ ë°˜ë³µë§ˆë‹¤ log-likelihood ì¦ê°€ â†’ ìˆ˜ë ´ ë³´ì¥
- GMM, HMM, ê²°ì¸¡ê°’ ë³´ì • ë“± ë‹¤ì–‘í•œ ëª¨ë¸ì— í™œìš© ê°€ëŠ¥