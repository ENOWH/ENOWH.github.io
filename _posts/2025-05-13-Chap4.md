---
title: "Chap04"
excerpt: "ë¨¸ì‹ ëŸ¬ë‹"

categories:
  - MachineLearning
tags:
  - [Machine Learning, ë¨¸ì‹ ëŸ¬ë‹]

permalink: /categories/MachineLearning/Chapter04_MachineLearning

toc: true
toc_sticky: true

date: 2025-05-13
last_modified_at: 2025-05-13
---


# ğŸ“˜ Chapter 4: Unsupervised Learning (slide_ch4_part1.pdf)

---

## ğŸŸ¦ 4.1 Introduction to Dimensionality Reduction

- ê³ ì°¨ì› ë°ì´í„° $x_1, ..., x_n \in \mathbb{R}^d$ â†’ ì €ì°¨ì› $\theta \in \mathbb{R}^k$ë¡œ ë³€í™˜
- ëª©í‘œ: ì •ë³´ ì†ì‹¤ ìµœì†Œí™”

### âœ… Dimensionality Reduction ëª©ì 

1. Visualization: 1D, 2D, 3Dë¡œ ì°¨ì› ì¶•ì†Œ â†’ ì‹œê°í™”
2. Noise Removal: ë¶ˆí•„ìš” or noisy feature ì œê±°
3. Algorithm Performance: ê³„ì‚°ëŸ‰ & ì„±ëŠ¥ ê°œì„ 
4. Computational Efficiency: ë©”ëª¨ë¦¬, ì €ì¥ ê³µê°„ ì ˆì•½

---

## ğŸŸ¦ 4.2 Feature Selection vs Feature Extraction

| ë°©ë²• | ì •ì˜ |
|------|------|
| Feature Selection | ê¸°ì¡´ feature ì¤‘ ì¼ë¶€ë§Œ ì„ íƒ |
| Feature Extraction | ê¸°ì¡´ featureë¥¼ ì¡°í•©í•´ ìƒˆë¡œìš´ feature ìƒì„± |

ğŸ“Œ PCAëŠ” Feature Extractionì— í•´ë‹¹

---

## ğŸŸ¦ 4.3 Principal Component Analysis (PCA)

### âœ… PCA í•µì‹¬ ê°œë…

| í•­ëª© | ë‚´ìš© |
|------|------|
| Information Loss | Sum of Squared Errors |
| Supervision | Unsupervised |
| Feature Strategy | Feature Extraction |
| Linearity | Linear |

PCA ëª©í‘œ: ìµœëŒ€ ë¶„ì‚° ë°©í–¥(Principal Components)ìœ¼ë¡œ ë°ì´í„° íˆ¬ì˜

---

### âœ… ì„ í˜• ê·¼ì‚¬ Formulation

ë°ì´í„° í‘œí˜„:
$$
x_i \approx \mu + A \theta_i
$$
- $\mu$: í‰ê·  ë²¡í„°
- $A$: orthonormal columns (subspace ì •ì˜)
- $\theta_i$: low-dimensional representation

ìµœì í™” ë¬¸ì œ:
$$
\min_{\mu, A, \theta} \sum_{i=1}^n \|x_i - \mu - A \theta_i\|^2
$$

í•´:
$$
\mu = \bar{x}, \quad A = [u_1, ..., u_k], \quad \theta_i = A^\top (x_i - \bar{x})
$$
($u_j$: Covariance matrixì˜ top-k eigenvectors)

---

### âœ… Maximum Variance Formulation (ë™ì¼ ê²°ê³¼)

ëª©í‘œ:
$$
\max_{A: A^\top A = I} \operatorname{tr}(A^\top S A)
$$

$S$: Sample Covariance Matrix
$$
S = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(x_i - \bar{x})^\top
$$

PCA ê²°ê³¼:
- A = top-k eigenvectors of $S$
- captured variance = top-k eigenvaluesì˜ í•©

---

### âœ… Data SVDì™€ Covariance EVD ê´€ê³„

$$
X = U \Sigma V^\top, \quad S = \frac{1}{n} X X^\top = U \Lambda U^\top
$$
- $U$: Left singular vectors = Covariance matrix eigenvectors
- $\Lambda = \frac{1}{n} \Sigma \Sigma^\top$

---

### âœ… PCA í”„ë¡œì„¸ìŠ¤

1. Data centering: í‰ê· ê°’ ë¹¼ê¸°
2. Covariance matrix ê³„ì‚°
3. Eigenvalue decomposition
4. Top-k principal components ì„ íƒ
5. ë°ì´í„° íˆ¬ì˜:
$$
y_i = A^\top (x_i - \bar{x})
$$

---

### âœ… PCA í•œê³„

- Linear method â†’ Nonlinear structure (e.g. circular data)ì— ì•½í•¨
- Kernel PCAë¡œ í™•ì¥ ê°€ëŠ¥

---

## ğŸŸ¦ 4.4 Model Selection

- Model Complexity (underfitting â†” overfitting) ì¡°ì ˆ

### âœ… Tuning Parameters ì˜ˆì‹œ

| ì•Œê³ ë¦¬ì¦˜ | Parameter |
|----------|-----------|
| Ridge | $\lambda$ (regularization) |
| Kernel SVM | kernel parameter $\sigma$ |
| k-NN | k (ì´ì›ƒ ìˆ˜) |

---

### âœ… Risk Estimation

- True Risk:
$$
R(f) = \mathbb{E}_{X,Y}[L(Y, f(X))]
$$

- Empirical Risk (Training Error):
$$
\hat{R}_{TR} = \frac{1}{n} \sum_{i=1}^n L(y_i, f(x_i))
$$

ë¬¸ì œ: training errorë§Œ ìµœì†Œí™” â†’ overfitting ë°œìƒ ê°€ëŠ¥

---

### âœ… Holdout Method

Train / Validation split â†’ Validation errorë¡œ ì„±ëŠ¥ í‰ê°€:
$$
\hat{R}_{HO} = \frac{1}{n-m} \sum_{i=m+1}^n L(y_i, f(x_i))
$$

---

### âœ… Cross Validation (K-Fold)

$$
\hat{R}_{CV} = \frac{1}{K} \sum_{j=1}^K \hat{R}^{(j)}(f^{(j)})
$$
- K=5, K=10ì´ ì¼ë°˜ì ìœ¼ë¡œ ì¶”ì²œ

---

### âœ… Bootstrap Method

- ë°ì´í„° ìƒ˜í”Œë§ + OOB(out-of-bag) errorë¡œ í‰ê°€
- ì¥ì : Variance estimation ê°€ëŠ¥
- ë‹¨ì : Pessimistic bias, ê³„ì‚°ëŸ‰ í¼

---

## ğŸŸ¦ 4.5 Clustering: k-means

### âœ… k-means ëª©í‘œ

Within-cluster sum of squares ìµœì†Œí™”:
$$
W(C) = \sum_{k=1}^K \sum_{i: C(i)=k} \|x_i - \bar{x}_k\|^2
$$

ì•Œê³ ë¦¬ì¦˜:
1. ì´ˆê¸° center ì„¤ì •
2. Assignment Step:
$$
C(i) = \arg\min_k \|x_i - m_k\|
$$
3. Update Step:
$$
m_k = \frac{1}{n_k} \sum_{i: C(i)=k} x_i
$$
4. ìˆ˜ë ´í•  ë•Œê¹Œì§€ ë°˜ë³µ

---

### âœ… k-means í•œê³„

- Convex, spherical, ë™ì¼ í¬ê¸° clusterì—ë§Œ ì í•©
- Non-convex data or ë‹¤ì–‘í•œ í¬ê¸° clusterì—ëŠ” ë¶€ì í•©
- ì´ˆê¸°í™”ì— ë¯¼ê° â†’ k-means++ ê°œì„ ë²• ì¡´ì¬

---

## ğŸŸ¦ 4.6 Gaussian Mixture Models (GMM)

### âœ… GMM ì •ì˜

Data density:
$$
f(x) = \sum_{k=1}^K w_k \phi(x; \mu_k, \Sigma_k)
$$

$\phi$: Multivariate Gaussian density
$$
\phi(x; \mu, \Sigma) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2} (x - \mu)^\top \Sigma^{-1}(x - \mu)\right)
$$

---

### âœ… EM Algorithm for GMM

#### E-Step:
Calculate responsibilities:
$$
\gamma_{i,k} = \frac{w_k \phi(x_i; \mu_k, \Sigma_k)}{\sum_{l=1}^K w_l \phi(x_i; \mu_l, \Sigma_l)}
$$

#### M-Step:
Update parameters:
$$
w_k = \frac{1}{n} \sum_{i=1}^n \gamma_{i,k}
$$
$$
\mu_k = \frac{\sum_{i=1}^n \gamma_{i,k} x_i}{\sum_{i=1}^n \gamma_{i,k}}
$$
$$
\Sigma_k = \frac{\sum_{i=1}^n \gamma_{i,k} (x_i - \mu_k)(x_i - \mu_k)^\top}{\sum_{i=1}^n \gamma_{i,k}}
$$

---

### âœ… k-means vs. GMM

| k-means | GMM |
|---------|-----|
| Hard assignment | Soft assignment (probabilistic) |
| Spherical clusters | Arbitrary shape, size ê°€ëŠ¥ |
| Fast, simple | Flexible but ê³„ì‚°ëŸ‰ ë§ìŒ |
| Sensitive to initialization | ëœ ë¯¼ê° (soft responsibilities) |

---

## âœ… Chapter 4 í•µì‹¬ ìš”ì•½

- **PCA**: ê°€ì¥ ë§ì´ ì“°ì´ëŠ” ì°¨ì› ì¶•ì†Œ ê¸°ë²• (ì„ í˜• ê¸°ë°˜)
- **Model Selection**: Hyperparameter tuning â†’ Cross-validation, bootstrap ë“±
- **k-means**: ê°„ë‹¨í•˜ê³  ë¹ ë¥¸ clustering â†’ Convex/Spherical ë°ì´í„°ì— ì í•©
- **GMM**: í™•ë¥  ê¸°ë°˜ soft clustering â†’ arbitrary shaped clusters ëª¨ë¸ë§ ê°€ëŠ¥
- **EM Algorithm**: Latent variable model í•™ìŠµì„ ìœ„í•œ iterative ìµœì í™” ë°©ë²•