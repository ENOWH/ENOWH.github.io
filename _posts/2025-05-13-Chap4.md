---
title: "Chap04"
excerpt: "머신러닝"

categories:
  - MachineLearning
tags:
  - [Machine Learning, 머신러닝]

permalink: /categories/MachineLearning/Chapter04_MachineLearning

toc: true
toc_sticky: true

date: 2025-05-13
last_modified_at: 2025-05-13
---


# 📘 Chapter 4: Unsupervised Learning (slide_ch4_part1.pdf)

---

## 🟦 4.1 Introduction to Dimensionality Reduction

- 고차원 데이터 $x_1, ..., x_n \in \mathbb{R}^d$ → 저차원 $\theta \in \mathbb{R}^k$로 변환
- 목표: 정보 손실 최소화

### ✅ Dimensionality Reduction 목적

1. Visualization: 1D, 2D, 3D로 차원 축소 → 시각화
2. Noise Removal: 불필요 or noisy feature 제거
3. Algorithm Performance: 계산량 & 성능 개선
4. Computational Efficiency: 메모리, 저장 공간 절약

---

## 🟦 4.2 Feature Selection vs Feature Extraction

| 방법 | 정의 |
|------|------|
| Feature Selection | 기존 feature 중 일부만 선택 |
| Feature Extraction | 기존 feature를 조합해 새로운 feature 생성 |

📌 PCA는 Feature Extraction에 해당

---

## 🟦 4.3 Principal Component Analysis (PCA)

### ✅ PCA 핵심 개념

| 항목 | 내용 |
|------|------|
| Information Loss | Sum of Squared Errors |
| Supervision | Unsupervised |
| Feature Strategy | Feature Extraction |
| Linearity | Linear |

PCA 목표: 최대 분산 방향(Principal Components)으로 데이터 투영

---

### ✅ 선형 근사 Formulation

데이터 표현:
$$
x_i \approx \mu + A \theta_i
$$
- $\mu$: 평균 벡터
- $A$: orthonormal columns (subspace 정의)
- $\theta_i$: low-dimensional representation

최적화 문제:
$$
\min_{\mu, A, \theta} \sum_{i=1}^n \|x_i - \mu - A \theta_i\|^2
$$

해:
$$
\mu = \bar{x}, \quad A = [u_1, ..., u_k], \quad \theta_i = A^\top (x_i - \bar{x})
$$
($u_j$: Covariance matrix의 top-k eigenvectors)

---

### ✅ Maximum Variance Formulation (동일 결과)

목표:
$$
\max_{A: A^\top A = I} \operatorname{tr}(A^\top S A)
$$

$S$: Sample Covariance Matrix
$$
S = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(x_i - \bar{x})^\top
$$

PCA 결과:
- A = top-k eigenvectors of $S$
- captured variance = top-k eigenvalues의 합

---

### ✅ Data SVD와 Covariance EVD 관계

$$
X = U \Sigma V^\top, \quad S = \frac{1}{n} X X^\top = U \Lambda U^\top
$$
- $U$: Left singular vectors = Covariance matrix eigenvectors
- $\Lambda = \frac{1}{n} \Sigma \Sigma^\top$

---

### ✅ PCA 프로세스

1. Data centering: 평균값 빼기
2. Covariance matrix 계산
3. Eigenvalue decomposition
4. Top-k principal components 선택
5. 데이터 투영:
$$
y_i = A^\top (x_i - \bar{x})
$$

---

### ✅ PCA 한계

- Linear method → Nonlinear structure (e.g. circular data)에 약함
- Kernel PCA로 확장 가능

---

## 🟦 4.4 Model Selection

- Model Complexity (underfitting ↔ overfitting) 조절

### ✅ Tuning Parameters 예시

| 알고리즘 | Parameter |
|----------|-----------|
| Ridge | $\lambda$ (regularization) |
| Kernel SVM | kernel parameter $\sigma$ |
| k-NN | k (이웃 수) |

---

### ✅ Risk Estimation

- True Risk:
$$
R(f) = \mathbb{E}_{X,Y}[L(Y, f(X))]
$$

- Empirical Risk (Training Error):
$$
\hat{R}_{TR} = \frac{1}{n} \sum_{i=1}^n L(y_i, f(x_i))
$$

문제: training error만 최소화 → overfitting 발생 가능

---

### ✅ Holdout Method

Train / Validation split → Validation error로 성능 평가:
$$
\hat{R}_{HO} = \frac{1}{n-m} \sum_{i=m+1}^n L(y_i, f(x_i))
$$

---

### ✅ Cross Validation (K-Fold)

$$
\hat{R}_{CV} = \frac{1}{K} \sum_{j=1}^K \hat{R}^{(j)}(f^{(j)})
$$
- K=5, K=10이 일반적으로 추천

---

### ✅ Bootstrap Method

- 데이터 샘플링 + OOB(out-of-bag) error로 평가
- 장점: Variance estimation 가능
- 단점: Pessimistic bias, 계산량 큼

---

## 🟦 4.5 Clustering: k-means

### ✅ k-means 목표

Within-cluster sum of squares 최소화:
$$
W(C) = \sum_{k=1}^K \sum_{i: C(i)=k} \|x_i - \bar{x}_k\|^2
$$

알고리즘:
1. 초기 center 설정
2. Assignment Step:
$$
C(i) = \arg\min_k \|x_i - m_k\|
$$
3. Update Step:
$$
m_k = \frac{1}{n_k} \sum_{i: C(i)=k} x_i
$$
4. 수렴할 때까지 반복

---

### ✅ k-means 한계

- Convex, spherical, 동일 크기 cluster에만 적합
- Non-convex data or 다양한 크기 cluster에는 부적합
- 초기화에 민감 → k-means++ 개선법 존재

---

## 🟦 4.6 Gaussian Mixture Models (GMM)

### ✅ GMM 정의

Data density:
$$
f(x) = \sum_{k=1}^K w_k \phi(x; \mu_k, \Sigma_k)
$$

$\phi$: Multivariate Gaussian density
$$
\phi(x; \mu, \Sigma) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2} (x - \mu)^\top \Sigma^{-1}(x - \mu)\right)
$$

---

### ✅ EM Algorithm for GMM

#### E-Step:
Calculate responsibilities:
$$
\gamma_{i,k} = \frac{w_k \phi(x_i; \mu_k, \Sigma_k)}{\sum_{l=1}^K w_l \phi(x_i; \mu_l, \Sigma_l)}
$$

#### M-Step:
Update parameters:
$$
w_k = \frac{1}{n} \sum_{i=1}^n \gamma_{i,k}
$$
$$
\mu_k = \frac{\sum_{i=1}^n \gamma_{i,k} x_i}{\sum_{i=1}^n \gamma_{i,k}}
$$
$$
\Sigma_k = \frac{\sum_{i=1}^n \gamma_{i,k} (x_i - \mu_k)(x_i - \mu_k)^\top}{\sum_{i=1}^n \gamma_{i,k}}
$$

---

### ✅ k-means vs. GMM

| k-means | GMM |
|---------|-----|
| Hard assignment | Soft assignment (probabilistic) |
| Spherical clusters | Arbitrary shape, size 가능 |
| Fast, simple | Flexible but 계산량 많음 |
| Sensitive to initialization | 덜 민감 (soft responsibilities) |

---

## ✅ Chapter 4 핵심 요약

- **PCA**: 가장 많이 쓰이는 차원 축소 기법 (선형 기반)
- **Model Selection**: Hyperparameter tuning → Cross-validation, bootstrap 등
- **k-means**: 간단하고 빠른 clustering → Convex/Spherical 데이터에 적합
- **GMM**: 확률 기반 soft clustering → arbitrary shaped clusters 모델링 가능
- **EM Algorithm**: Latent variable model 학습을 위한 iterative 최적화 방법