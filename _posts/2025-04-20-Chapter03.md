---
title: "Chapter03"
excerpt: "ë¨¸ì‹ ëŸ¬ë‹ Chap03 ì „ì²´ íë¦„"

categories:
  - MachineLearning
tags:
  - [Machine Learning, ë¨¸ì‹ ëŸ¬ë‹]

permalink: /categories/MachineLearning/Chapter03_MachineLearning

toc: true
toc_sticky: true
use_math: true

date: 2025-04-20
last_modified_at: 2025-04-20
---



# ðŸ“˜ Chapter 3: Kernel Methods (`slide_ch3_0409.pdf`)

## ðŸ“‘ ì„¸ì…˜ë³„ êµ¬ì„± ë° ì£¼ìš” ì£¼ì œ ì •ë¦¬

---

### ðŸŸ¦ Session 1: Kernel Methods - Foundations

- **Nonparametric ëª¨ë¸** ê°œìš”: íŒŒë¼ë¯¸í„°ë¥¼ ì •í•˜ì§€ ì•Šê³  í•¨ìˆ˜ ìžì²´ë¥¼ ì¶”ì •
- **Feature Map $\Phi(\mathbf{x})$**: ì›ëž˜ ê³µê°„ì—ì„œ ê³ ì°¨ì› ê³µê°„ìœ¼ë¡œì˜ ë§¤í•‘
- **Kernel Trick**: 
  $$
  k(\mathbf{x}, \mathbf{x}') = \langle \Phi(\mathbf{x}), \Phi(\mathbf{x}') \rangle
  $$
- ê³„ì‚° íš¨ìœ¨ì„±ì„ ìœ„í•´ ì§ì ‘ feature map ê³„ì‚° ì—†ì´ inner product ê³„ì‚°ë§Œ ìˆ˜í–‰
- **ëŒ€í‘œ ì»¤ë„**:
  - Polynomial: $(\mathbf{x}^\top \mathbf{x}' + c)^p$
  - Gaussian (RBF): $\exp\left( - \frac{\|\mathbf{x} - \mathbf{x}'\|^2}{2 \ell^2} \right)$

---

### ðŸŸ¦ Session 2: Kernel Ridge Regression (KRR)

- **Ridge Regression** + **Kernel Trick** = KRR
- ëª©ì  í•¨ìˆ˜:
  $$
  \min_{\mathbf{w}} \frac{1}{n} \sum_{i=1}^n (y_i - \mathbf{w}^\top \Phi(\mathbf{x}_i))^2 + \lambda \|\mathbf{w}\|^2
  $$
- ì»¤ë„í™”ëœ ìµœì¢… ì˜ˆì¸¡ í•¨ìˆ˜:
  $$
  \hat{f}(\mathbf{x}) = \mathbf{y}^\top (K + n\lambda I)^{-1} \mathbf{k}(\mathbf{x})
  $$
- Offset í¬í•¨ ì‹œ ì¤‘ì‹¬í™”(centering) í•„ìš” â†’ centered kernel matrix ì‚¬ìš©

---

### ðŸŸ¦ Session 3: Constrained Optimization

- **ì œì•½ì¡°ê±´ ìžˆëŠ” ìµœì í™”**:
  $$
  \min_{\mathbf{x}} f(\mathbf{x}) \quad \text{s.t. } g_i(\mathbf{x}) \leq 0,\; h_j(\mathbf{x}) = 0
  $$
- **Lagrangian**:
  $$
  \mathcal{L}(\mathbf{x}, \boldsymbol{\alpha}, \boldsymbol{\beta}) = f(\mathbf{x}) + \sum \alpha_i g_i(\mathbf{x}) + \sum \beta_j h_j(\mathbf{x})
  $$
- **Dual problem**: 
  $$
  \max_{\alpha \geq 0, \beta} \min_{\mathbf{x}} \mathcal{L}(\mathbf{x}, \alpha, \beta)
  $$
- **KKT ì¡°ê±´**: ìµœì í•´ê°€ ë§Œì¡±í•´ì•¼ í•˜ëŠ” 4ê°€ì§€ ì¡°ê±´
  - Primal feasibility
  - Dual feasibility
  - Complementary slackness
  - Stationarity

---

### ðŸŸ¦ Session 4: Kernelized Support Vector Machines (SVM)

- Soft-margin SVMì˜ dual ë¬¸ì œë¥¼ ìœ ë„í•˜ê³  kernelization ì ìš©
- ìµœì¢… SVM ë¶„ë¥˜ê¸°:
  $$
  f(\mathbf{x}) = \text{sign} \left( \sum_{i=1}^n \alpha_i y_i k(\mathbf{x}_i, \mathbf{x}) + b \right)
  $$
- **Support Vectors**: $\alpha_i > 0$ì¸ ìƒ˜í”Œë“¤ë§Œ ê²°ì • ê²½ê³„ì— ê¸°ì—¬
- **SMO ì•Œê³ ë¦¬ì¦˜**: Dual ë¬¸ì œë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í‘¸ëŠ” ë°©ë²• (2ê°œ ë³€ìˆ˜ë§Œ ì„ íƒí•´ì„œ QP ë¬¸ì œ ë°˜ë³µ í•´ê²°)

---

### ðŸŸ¦ Session 5: Gaussian Processes (GP)

- ì™„ì „í•œ **Bayesian regression** ì ‘ê·¼ ë°©ì‹
- í•¨ìˆ˜ ìžì²´ë¥¼ ëžœë¤ ë³€ìˆ˜ë¡œ ë³´ê³  **í™•ë¥  ë¶„í¬**ë¥¼ ì •ì˜
- $f \sim \mathcal{GP}(m(\cdot), k(\cdot, \cdot))$
- ì˜ˆì¸¡ ë¶„í¬ (posterior predictive):
  $$
  y^* \mid \mathbf{y}, X, X^* \sim \mathcal{N}(\mu_*, \Sigma_*)
  $$
  - í‰ê· : $\mu_* = K(X^*, X)(K(X, X) + \sigma^2 I)^{-1} \mathbf{y}$
  - ë¶„ì‚°: $\Sigma_* = K(X^*, X^*) + \sigma^2 I - K(X^*, X)(K(X, X) + \sigma^2 I)^{-1} K(X, X^*)$
- í™•ë¥ ì  ì˜ˆì¸¡ê³¼ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”ê°€ ê°€ëŠ¥

---

## ðŸ§­ ì „ì²´ íë¦„ ìš”ì•½

1. **Kernel Methods**ë¡œ ê³ ì°¨ì›ì—ì„œì˜ ì„ í˜•í™” & ë¹„ì„ í˜• ë¶„ë¥˜ ê°€ëŠ¥ì„± í™•ë³´
2. **KRR**ì€ kernel trickì„ í†µí•´ ë¹„ì„ í˜• íšŒê·€ ë¬¸ì œ í•´ê²°
3. **Constrained Optimization**ì€ SVMê³¼ ê°™ì€ ëª¨ë¸ í•™ìŠµì— ìˆ˜í•™ì  ê¸°ë°˜ ì œê³µ
4. **Kernelized SVM**ì€ margin ê¸°ë°˜ì˜ ê°•ë ¥í•œ ë¶„ë¥˜ê¸°
5. **Gaussian Processes**ëŠ” ë¹„ëª¨ìˆ˜ Bayesian ì¶”ë¡ ìœ¼ë¡œ ë¶ˆí™•ì‹¤ì„±ê¹Œì§€ í¬í•¨í•œ ì˜ˆì¸¡ ì œê³µ



# ðŸ§  Session 1: Kernel Methods - Foundations

## ðŸŒ± ê°œë… íë¦„ ìš”ì•½

- Nonparametric ë°©ë²•ì€ **ëª¨ë¸ íŒŒë¼ë¯¸í„°**ë¥¼ ë¯¸ë¦¬ ì •í•˜ì§€ ì•Šê³ , **ë°ì´í„° ì „ì²´ë¡œë¶€í„° í•¨ìˆ˜ ìžì²´ë¥¼ ì¶”ì •**í•˜ëŠ” ë°©ì‹
- ë°ì´í„°ë¥¼ **ë¹„ì„ í˜•ì ìœ¼ë¡œ ë³€í™˜ (nonlinear feature map)** í›„ **ì„ í˜• ëª¨ë¸ì„ ì ìš©**
- ê³ ì°¨ì› feature spaceì—ì„œì˜ ê³„ì‚°ì„ ì»¤ë„ í•¨ìˆ˜ë¡œ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰ â†’ **Kernel Trick**

---

## ðŸ”‘ ê°œë…ë³„ ì •ë¦¬ (ê°œë… â†” ìˆ˜ì‹ ì—°ê²° ì¤‘ì‹¬)

### âœ… Nonparametric Learning

- ê´€ì¸¡ ë°ì´í„°: $D = \{(\mathbf{x}_i, y_i)\}_{i=1}^N$
- ëª©ì : íŠ¹ì •í•œ íŒŒë¼ë¯¸í„° ëª¨ë¸ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ , í•¨ìˆ˜ $f$ ìžì²´ë¥¼ ì¶”ì •
- ì˜ˆì¸¡:
  $$
  \hat{f}(\mathbf{x}^*) \approx \sum_{i=1}^N \text{similarity}(\mathbf{x}_i, \mathbf{x}^*) \cdot y_i
  $$
- ì „ì²´ ë°ì´í„°ë¥¼ ê¸°ì–µí•´ì•¼ í•˜ë¯€ë¡œ parametric ë°©ë²•ë³´ë‹¤ ë©”ëª¨ë¦¬ ë¹„ìš©ì´ í¼

---

### âœ… Feature Map $\Phi(\mathbf{x})$

- ì›ëž˜ ìž…ë ¥ ê³µê°„ $\mathbb{R}^d$ì—ì„œ ê³ ì°¨ì› ê³µê°„ $\mathbb{R}^m$ìœ¼ë¡œ ë³€í™˜:
  $$
  \Phi: \mathbb{R}^d \rightarrow \mathbb{R}^m
  $$

- ì˜ˆ: ë‹¤í•­ì‹ ë³€í™˜
  $$
  \Phi(x) = \begin{bmatrix} x \\ x^2 \\ x^3 \end{bmatrix}, \quad f(x) = \mathbf{w}^\top \Phi(x) + b
  $$

---

### âœ… Curse of Dimensionality ë¬¸ì œ

- $\Phi(x)$ì˜ ì°¨ì› $m$ì€ ë§¤ìš° ì»¤ì§ˆ ìˆ˜ ìžˆìŒ (ì˜ˆ: degree-$p$ ë‹¤í•­ì‹ì˜ ê²½ìš° $m = \binom{p+d}{d}$)
- ì§ì ‘ ê³„ì‚°í•˜ë©´ ë„ˆë¬´ ë§Žì€ feature ì—°ì‚°ì´ í•„ìš” â†’ **Kernel Trick í•„ìš”**

---

### âœ… Kernel Function (ì»¤ë„ í•¨ìˆ˜)

- **ì •ì˜**: ìž…ë ¥ $\mathbf{x}, \mathbf{x}'$ ê°„ì˜ ë‚´ì ì„ ê³ ì°¨ì› feature spaceì—ì„œ ê³„ì‚°í•œ ê²ƒì²˜ëŸ¼ ë§Œë“œëŠ” í•¨ìˆ˜
  $$
  k(\mathbf{x}, \mathbf{x}') = \langle \Phi(\mathbf{x}), \Phi(\mathbf{x}') \rangle
  $$

- **Kernel Trick**:
  - ë‚´ì  ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì—ì„œ $\Phi$ë¥¼ ì§ì ‘ ê³„ì‚°í•˜ì§€ ì•Šê³  $k(\mathbf{x}, \mathbf{x}')$ë§Œìœ¼ë¡œ ì²˜ë¦¬

---

### âœ… ëŒ€í‘œì ì¸ ì»¤ë„ í•¨ìˆ˜ë“¤

| ì´ë¦„ | ìˆ˜ì‹ | íŠ¹ì§• |
|------|------|------|
| **Linear** | $k(\mathbf{x}, \mathbf{x}') = \mathbf{x}^\top \mathbf{x}'$ | $\Phi(x) = x$ |
| **Polynomial** | $k(\mathbf{x}, \mathbf{x}') = (\mathbf{x}^\top \mathbf{x}' + c)^p$ | ê³ ì°¨ ë‹¤í•­ì‹ ëª¨ë¸ |
| **RBF (Gaussian)** | $k(\mathbf{x}, \mathbf{x}') = \exp\left( -\frac{\|\mathbf{x} - \mathbf{x}'\|^2}{2 \ell^2} \right)$ | ë¬´í•œ ì°¨ì› ë§¤í•‘ |
| **Sigmoid** | $k(\mathbf{x}, \mathbf{x}') = \tanh(\alpha \mathbf{x}^\top \mathbf{x}' + \beta)$ | ì‹ ê²½ë§ê³¼ ìœ ì‚¬ |

---

### âœ… Inner Product Kernels & Mercer Condition

- **Inner Product Kernel**:
  $$
  k(\mathbf{x}, \mathbf{x}') = \langle \Phi(\mathbf{x}), \Phi(\mathbf{x}') \rangle
  $$
- **Mercer Kernel**:
  - $k$ì´ **symmetric**ì´ê³ , **positive semi-definite (PSD)**ì´ë©´ kernelë¡œ ì‚¬ìš©í•  ìˆ˜ ìžˆìŒ
  - ì¦‰, kernel matrix $K = [k(x_i, x_j)]$ê°€ ëª¨ë“  ìž…ë ¥ì— ëŒ€í•´ PSDë©´ OK

---

### âœ… ê³„ì‚° ë¹„êµ ì˜ˆì‹œ

#### âœï¸ Explicit Feature Mapping (ì˜ˆ: $\Phi(x) = (x_1^2, \sqrt{2}x_1x_2, x_2^2)$)
- Feature ê³„ì‚° + ë‚´ì : ê³±ì…ˆ 11íšŒ, ë§ì…ˆ 2íšŒ

#### âš¡ Kernel ê³„ì‚° (Polynomial kernel of degree 2)
- $k(\mathbf{x}, \mathbf{x}') = (\mathbf{x}^\top \mathbf{x}')^2$
- ê³±ì…ˆ 3íšŒ, ë§ì…ˆ 1íšŒ â†’ **í›¨ì”¬ íš¨ìœ¨ì !**

---

## âœ… í•µì‹¬ ìš”ì•½

- Kernel ë°©ë²•ì€ **ê³ ì°¨ì›ì—ì„œì˜ ë‚´ì ì„ ì €ì°¨ì›ì—ì„œ ì»¤ë„ í•¨ìˆ˜ë¡œ ìš°íšŒ ê³„ì‚°**
- ì´ë¥¼ í†µí•´ ì„ í˜• ëª¨ë¸ì„ **ë¹„ì„ í˜• ë¬¸ì œì— ì ìš©**í•  ìˆ˜ ìžˆìŒ
- **RBF, Polynomial, Sigmoid** ë“± ë‹¤ì–‘í•œ ì»¤ë„ì„ í†µí•´ ë¬¸ì œ íŠ¹ì„±ì— ë§žê²Œ ì„ íƒ ê°€ëŠ¥
- SVM, Ridge, PCA ë“± ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ì´ **kernelized** ê°€ëŠ¥



# ðŸ“ Session 2: Kernel Ridge Regression (KRR)

## ðŸŒ± ê°œë… íë¦„ ìš”ì•½

- Ridge Regressionì€ ì„ í˜• íšŒê·€ì— ì •ê·œí™”ë¥¼ ë”í•œ ê²ƒ
- KRRì€ ì—¬ê¸°ì— **ì»¤ë„ íŠ¸ë¦­**ì„ ê²°í•©í•˜ì—¬ **ë¹„ì„ í˜• íšŒê·€**ê°€ ê°€ëŠ¥í•˜ë„ë¡ í™•ìž¥
- í•µì‹¬ì€ ì˜ˆì¸¡ í•¨ìˆ˜ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•˜ëŠ” ê²ƒ:
  $$
  \hat{f}(\mathbf{x}) = \sum_{i=1}^n \alpha_i k(\mathbf{x}_i, \mathbf{x})
  $$
- $\alpha_i$ëŠ” í•™ìŠµ ë°ì´í„°ì™€ ì»¤ë„ì— ê¸°ë°˜í•´ ê³„ì‚°ë¨

---

## ðŸ”‘ ê°œë…ë³„ ì •ë¦¬ (ê°œë… â†” ìˆ˜ì‹ ì—°ê²° ì¤‘ì‹¬)

### âœ… Ridge Regression (without offset)

- ìž…ë ¥: $X \in \mathbb{R}^{n \times d}$, $y \in \mathbb{R}^n$
- ëª©ì  í•¨ìˆ˜:
  $$
  \min_{\mathbf{w}} \frac{1}{n} \|y - X\mathbf{w}\|^2 + \lambda \|\mathbf{w}\|^2
  $$

- í•´(solution):
  $$
  \hat{\mathbf{w}} = (X^\top X + n\lambda I)^{-1} X^\top y
  $$

---

### âœ… íšŒê·€ í•¨ìˆ˜ í‘œí˜„

- ì˜ˆì¸¡ í•¨ìˆ˜:
  $$
  \hat{f}(\mathbf{x}) = \hat{\mathbf{w}}^\top \mathbf{x}
  $$
- ë‚´ì  ê¸°ë°˜ìœ¼ë¡œ í‘œí˜„:
  $$
  \hat{f}(\mathbf{x}) = y^\top X (X^\top X + n\lambda I)^{-1} \mathbf{x}
  $$

---

### âœ… Kernelization ëª©í‘œ

- ë‚´ì  $\langle \mathbf{x}_i, \mathbf{x}_j \rangle$ í˜•íƒœë§Œìœ¼ë¡œ í‘œí˜„ë˜ë¯€ë¡œ, **kernel trick ê°€ëŠ¥**
- Gram matrix $G \in \mathbb{R}^{n \times n}$:
  $$
  G_{ij} = \langle \mathbf{x}_i, \mathbf{x}_j \rangle
  $$

---

### âœ… Kernel Ridge Regression

- ì»¤ë„ì„ ì ìš©í•œ ìµœì¢… ì˜ˆì¸¡ í•¨ìˆ˜:
  $$
  \hat{f}(\mathbf{x}) = \mathbf{y}^\top (K + n\lambda I)^{-1} \mathbf{k}(\mathbf{x})
  $$

- ì—¬ê¸°ì„œ:
  - $K \in \mathbb{R}^{n \times n}$: kernel matrix, $K_{ij} = k(\mathbf{x}_i, \mathbf{x}_j)$
  - $\mathbf{k}(\mathbf{x}) = [k(\mathbf{x}_1, \mathbf{x}), \ldots, k(\mathbf{x}_n, \mathbf{x})]^\top$

---

### âœ… Bias (offset) í¬í•¨í•œ KRR

- ëª¨ë¸: $\hat{f}(\mathbf{x}) = \mathbf{w}^\top \mathbf{x} + b$
- ì¤‘ì‹¬í™”(centered)ëœ ë°ì´í„° ì‚¬ìš©: $\tilde{x}_i = x_i - \bar{x}$, $\tilde{y}_i = y_i - \bar{y}$
- ìµœì¢… ì˜ˆì¸¡:
  $$
  \hat{f}(\mathbf{x}) = \bar{y} + \tilde{\mathbf{y}}^\top (KÌƒ + n\lambda I)^{-1} \tilde{\mathbf{k}}(\mathbf{x})
  $$

- **centered kernel matrix** $KÌƒ$ì™€ ë²¡í„° $\tilde{k}(\mathbf{x})$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë¨:
  $$
  \tilde{k}(x_i, x_j) = k(x_i, x_j) - \frac{1}{n} \sum_r k(x_i, x_r) - \frac{1}{n} \sum_s k(x_s, x_j) + \frac{1}{n^2} \sum_{r,s} k(x_r, x_s)
  $$

---

### âœ… ì˜ˆì‹œ: Gaussian Kernel + KRR

- Gaussian (RBF) Kernel:
  $$
  k(x, x') = \exp\left( - \frac{\|x - x'\|^2}{2 \ell^2} \right)
  $$

- ìµœì¢… ì˜ˆì¸¡ í•¨ìˆ˜:
  $$
  \hat{f}(x) = \sum_{i=1}^n \alpha_i k(x, x_i)
  $$
- $\alpha = (K + n\lambda I)^{-1} y$

---

### âœ… KRRì˜ íŠ¹ì§• ìš”ì•½

| í•­ëª© | ë‚´ìš© |
|------|------|
| ëª¨ë¸ ìœ í˜• | ë¹„ì„ í˜• íšŒê·€ (ì»¤ë„ ê¸°ë°˜) |
| ì •ê·œí™” | Ridge ë°©ì‹: $\lambda \|\mathbf{w}\|^2$ |
| ì»¤ë„ ì‚¬ìš© | ë‚´ì  ëŒ€ì‹  ì»¤ë„ í•¨ìˆ˜ $k(x,x')$ ì‚¬ìš© |
| í•™ìŠµ ë°ì´í„° ì˜ì¡´ì„± | ëª¨ë“  í•™ìŠµ ë°ì´í„°ë¥¼ ì €ìž¥í•´ì•¼ í•¨ (nonparametric) |
| ê³„ì‚° ë³µìž¡ë„ | $O(n^3)$ (matrix inversion), ìž‘ì€ ë°ì´í„°ì…‹ì— ì í•© |

---

## âœ… í•µì‹¬ ìš”ì•½

- **KRRì€ ë¹„ì„ í˜• íšŒê·€ ë¬¸ì œë¥¼ ì„ í˜•ì²˜ëŸ¼ í‘¸ëŠ” í•µì‹¬ ê¸°ë²•**
- ë‚´ì  ê¸°ë°˜ Ridge Regressionì„ ì»¤ë„í™” â†’ ê³ ì°¨ì›ì—ì„œë„ ê³„ì‚° ê°€ëŠ¥
- ì»¤ë„ ì„ íƒê³¼ regularizationì´ ëª¨ë¸ ì„±ëŠ¥ì— í° ì˜í–¥



# ðŸ”’ Session 3: Constrained Optimization

## ðŸŒ± ê°œë… íë¦„ ìš”ì•½

- ë¨¸ì‹ ëŸ¬ë‹ì˜ ë§Žì€ ë¬¸ì œëŠ” **ì œì•½ì¡°ê±´ì´ ìžˆëŠ” ìµœì í™” ë¬¸ì œ**ë¡œ êµ¬ì„±ë¨
- ì¼ë°˜ì ì¸ unconstrained ìµœì í™”ì—ì„œëŠ” $\nabla f(x^*) = 0$ ì´ë©´ ì¶©ë¶„í–ˆì§€ë§Œ,
  **ì œì•½ì´ ìžˆì„ ê²½ìš° í•´ê°€ ê²½ê³„(boundary)ì— ì¡´ìž¬í•  ìˆ˜ ìžˆìŒ**
- ë”°ë¼ì„œ **Lagrangianì„ ì´ìš©í•œ dual formulation**ì´ í•„ìš”í•˜ë©°,
  **KKT ì¡°ê±´**ì´ ìµœì ì„± íŒë‹¨ì˜ í•µì‹¬ ê¸°ì¤€ì´ ë¨

---

## ðŸ”‘ ê°œë…ë³„ ì •ë¦¬ (ê°œë… â†” ìˆ˜ì‹ ì—°ê²° ì¤‘ì‹¬)

### âœ… ì œì•½ ìµœì í™” ë¬¸ì œ ì •ì˜

- ì¼ë°˜ì ì¸ í˜•ì‹:
  $$
  \min_{x \in \mathbb{R}^n} f(x) \quad \text{s.t. } g_i(x) \leq 0,\; h_j(x) = 0
  $$
  - $f(x)$: ëª©ì  í•¨ìˆ˜ (convex)
  - $g_i(x)$: ë¶€ë“±ì‹ ì œì•½ ì¡°ê±´
  - $h_j(x)$: ë“±ì‹ ì œì•½ ì¡°ê±´ (affine)

---

### âœ… Lagrangian ì •ì˜

- ì œì•½ì¡°ê±´ì„ ëª©ì í•¨ìˆ˜ì— í†µí•©í•œ ì‹:
  $$
  \mathcal{L}(x, \boldsymbol{\alpha}, \boldsymbol{\beta}) = f(x) + \sum_{i=1}^m \alpha_i g_i(x) + \sum_{j=1}^p \beta_j h_j(x)
  $$
  - $\alpha_i \geq 0$: ë¶€ë“±ì‹ ì œì•½ì˜ ë¼ê·¸ëž‘ì£¼ ìŠ¹ìˆ˜
  - $\beta_j$: ë“±ì‹ ì œì•½ì˜ ë¼ê·¸ëž‘ì£¼ ìŠ¹ìˆ˜

---

### âœ… Primal & Dual ë¬¸ì œ

#### ðŸ”¹ Primal ë¬¸ì œ:
- ì›ëž˜ì˜ ë¬¸ì œ:
  $$
  \min_x \theta_P(x) = \max_{\alpha \geq 0, \beta} \mathcal{L}(x, \alpha, \beta)
  $$

#### ðŸ”¹ Dual ë¬¸ì œ:
- ì œì•½ ì¡°ê±´ì„ ë‚´ìž¬í™”í•œ dual ëª©ì í•¨ìˆ˜:
  $$
  \theta_D(\alpha, \beta) = \min_x \mathcal{L}(x, \alpha, \beta)
  $$
- Dual ë¬¸ì œ:
  $$
  \max_{\alpha \geq 0, \beta} \theta_D(\alpha, \beta)
  $$

---

### âœ… Duality Gap & Lemmas

- **Lemma 1 (Dual bound)**: $\theta_D(\alpha, \beta) \leq p^*$ (primal optimum)
- **Weak Duality**:
  $$
  d^* = \theta_D(\alpha^*, \beta^*) \leq p^* = \theta_P(x^*)
  $$
- **Strong Duality** (Slater ì¡°ê±´ ë§Œì¡± ì‹œ):
  $$
  d^* = p^*
  $$

---

### âœ… Complementary Slackness

- **ì¡°ê±´**:
  $$
  \alpha_i^* \cdot g_i(x^*) = 0 \quad \forall i
  $$
- ì˜ë¯¸: ì–´ë–¤ ì œì•½ ì¡°ê±´ì´ **í™œì„±(active)** ë˜ì–´ ìžˆë‹¤ë©´, í•´ë‹¹ $\alpha_i^*$ëŠ” ì–‘ìˆ˜ì´ê³   
  ì œì•½ì´ **ë¹„í™œì„±**ì´ë©´ $\alpha_i^* = 0$

---

### âœ… KKT (Karush-Kuhn-Tucker) ì¡°ê±´

**Convex ìµœì í™” ë¬¸ì œì—ì„œ ìµœì í•´ê°€ ë§Œì¡±í•´ì•¼ í•˜ëŠ” 4ê°€ì§€ ì¡°ê±´**:

1. **Primal Feasibility**: $g_i(x^*) \leq 0$, $h_j(x^*) = 0$
2. **Dual Feasibility**: $\alpha_i^* \geq 0$
3. **Complementary Slackness**: $\alpha_i^* g_i(x^*) = 0$
4. **Stationarity**:  
   $$
   \nabla f(x^*) + \sum_i \alpha_i^* \nabla g_i(x^*) + \sum_j \beta_j^* \nabla h_j(x^*) = 0
   $$

âœ… ì´ ë„¤ ê°€ì§€ê°€ ëª¨ë‘ ë§Œì¡±ë˜ë©´ $(x^*, \alpha^*, \beta^*)$ëŠ” ìµœì í•´!

---

### âœ… ì˜ˆì œ ë¬¸ì œ (ì§ê´€ ì—°ìŠµ)

**ë¬¸ì œ**:
$$
\min_{x \in \mathbb{R}^2} x_1^2 + x_2 \\
\text{s.t. } 2x_1 + x_2 \geq 4, \quad x_2 \geq 1
$$

â†’ í‘œì¤€í˜•ìœ¼ë¡œ ë³€í™˜:
$$
g_1(x) = -2x_1 - x_2 + 4 \leq 0, \quad g_2(x) = -x_2 + 1 \leq 0
$$

**í•´ë²•**:
- Lagrangian êµ¬ì„±
- Stationarity ì¡°ê±´ìœ¼ë¡œ $x_1, x_2$ ë„ì¶œ
- Complementary Slackness í™•ì¸
- Primal, Dual Feasibility í™•ì¸
- KKT ì¡°ê±´ì´ ëª¨ë‘ ë§Œì¡±ë˜ëŠ” ìµœì ì  ë„ì¶œ

ðŸ“Œ ìµœì í•´: $x^* = (1, 2)$, $f(x^*) = 3$

---

## âœ… í•µì‹¬ ìš”ì•½

- ì œì•½ì¡°ê±´ì´ ìžˆëŠ” ìµœì í™”ëŠ” **Lagrangian + Duality + KKT ì¡°ê±´**ìœ¼ë¡œ ë¶„ì„ ê°€ëŠ¥
- **Primal/dual ë¬¸ì œë¥¼ ëª¨ë‘ ì´í•´**í•´ì•¼ í•´ì„ ê°€ëŠ¥
- **KKT ì¡°ê±´**ì€ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ (ex. SVM)ì—ì„œ ìµœì ì„±ì„ íŒë³„í•˜ëŠ” í•µì‹¬ ë„êµ¬



# ðŸ§± Session 4: Kernelized Support Vector Machines (SVM)

## ðŸŒ± ê°œë… íë¦„ ìš”ì•½

- SVMì€ **marginì´ ìµœëŒ€ê°€ ë˜ëŠ” ì„ í˜• ë¶„ë¥˜ê¸°**ë¥¼ ì°¾ëŠ” ë¬¸ì œ
- ì„ í˜• ë¶„ë¦¬ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ë¥¼ ìœ„í•´ slack ë³€ìˆ˜ ë„ìž… (soft-margin)
- ìµœì í™” ë¬¸ì œë¥¼ dual formìœ¼ë¡œ ë°”ê¾¸ë©´, **ì»¤ë„ íŠ¸ë¦­ì„ ì ìš©í•´ ë¹„ì„ í˜• ë¶„ë¥˜ë„ ê°€ëŠ¥**
- ì´ë¡œì¨ SVMì€ **ê°•ë ¥í•˜ê³  ì¼ë°˜í™” ì„±ëŠ¥ì´ ë†’ì€ ì»¤ë„ ê¸°ë°˜ ë¶„ë¥˜ê¸°**ë¡œ í™•ìž¥ë¨

---

## ðŸ”‘ ê°œë…ë³„ ì •ë¦¬ (ê°œë… â†” ìˆ˜ì‹ ì—°ê²° ì¤‘ì‹¬)

### âœ… Soft-Margin SVM: Primal Formulation

- ìž…ë ¥: $\{(\mathbf{x}_i, y_i)\}_{i=1}^n$, $y_i \in \{-1, +1\}$
- ìµœì í™” ë¬¸ì œ:
  $$
  \min_{\mathbf{w}, b, \boldsymbol{\xi}} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^n \xi_i
  $$
  $$
  \text{s.t. } y_i (\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 - \xi_i,\quad \xi_i \geq 0
  $$

- ëª©ì : margin ìµœëŒ€í™” + ì˜¤ë¶„ë¥˜ íŽ˜ë„í‹° ìµœì†Œí™”

---

### âœ… Lagrangian êµ¬ì„±

- Lagrangian í•¨ìˆ˜:
  $$
  \mathcal{L}(\mathbf{w}, b, \boldsymbol{\xi}, \boldsymbol{\alpha}, \boldsymbol{\beta}) = \frac{1}{2} \|\mathbf{w}\|^2 + C \sum \xi_i - \sum \alpha_i [y_i(\mathbf{w}^\top \mathbf{x}_i + b) - 1 + \xi_i] - \sum \beta_i \xi_i
  $$

---

### âœ… Dual Problem ìœ ë„

- Stationarity ì¡°ê±´ì„ ì´ìš©í•´ $\mathbf{w}$, $b$, $\xi_i$ ì œê±° í›„ dual ë¬¸ì œë¡œ ì •ë¦¬

ðŸ“ ìµœì¢… Dual ë¬¸ì œ:
$$
\max_{\boldsymbol{\alpha}} -\frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j \langle \mathbf{x}_i, \mathbf{x}_j \rangle + \sum_i \alpha_i
$$
$$
\text{s.t. } \sum_i \alpha_i y_i = 0,\quad 0 \leq \alpha_i \leq \frac{C}{n}
$$

- ê²°ì • í•¨ìˆ˜:
  $$
  f(\mathbf{x}) = \operatorname{sign} \left( \sum_{i=1}^n \alpha_i^* y_i \langle \mathbf{x}_i, \mathbf{x} \rangle + b^* \right)
  $$

---

### âœ… Kernelization (ì»¤ë„í™”)

- ì»¤ë„ íŠ¸ë¦­ ì ìš©:
  $$
  k(\mathbf{x}_i, \mathbf{x}_j) = \langle \Phi(\mathbf{x}_i), \Phi(\mathbf{x}_j) \rangle
  $$

ðŸ“ ìµœì¢… Kernel SVM ë¶„ë¥˜ê¸°:
$$
f(\mathbf{x}) = \operatorname{sign} \left( \sum_{i=1}^n \alpha_i^* y_i k(\mathbf{x}_i, \mathbf{x}) + b^* \right)
$$

- $\alpha_i^* > 0$ì¸ ìƒ˜í”Œë§Œ ê²°ì • ê²½ê³„ì— ê¸°ì—¬ â†’ **Support Vectors**

---

### âœ… b* ê³„ì‚° ë°©ë²•

- $\alpha_j^* \in (0, \frac{C}{n})$ ì¸ ìƒ˜í”Œì— ëŒ€í•´:
  $$
  b^* = y_j - \sum_{i=1}^n \alpha_i^* y_i k(\mathbf{x}_i, \mathbf{x}_j)
  $$

- ì—¬ëŸ¬ support vectorê°€ ìžˆì„ ê²½ìš° í‰ê·  ì‚¬ìš© ê°€ëŠ¥

---

### âœ… Support Vector ë¶„ë¥˜ ê¸°ì¤€

| ìœ„ì¹˜ | ì¡°ê±´ | $\alpha_i^*$ ê°’ |
|------|------|-----------------|
| Margin ë°”ê¹¥ | $y_i f(x_i) > 1$ | 0 |
| Margin ìœ„ | $y_i f(x_i) = 1$ | $0 < \alpha_i^* < \frac{C}{n}$ |
| Margin ì•ˆ | $0 < y_i f(x_i) < 1$ | $\alpha_i^* = \frac{C}{n}$ |
| Misclassified | $y_i f(x_i) < 0$ | $\alpha_i^* = \frac{C}{n}$ |

---

### âœ… SMO (Sequential Minimal Optimization)

- Dual problemì˜ ì œí•œ ì¡°ê±´ ë•Œë¬¸ì— ì¢Œí‘œìƒìŠ¹ë²•ì„ ì§ì ‘ ì“°ê¸° ì–´ë ¤ì›€
- **ë‘ ë³€ìˆ˜ì”© ì„ íƒí•´ì„œ ìµœì í™”í•˜ëŠ” ë°©ì‹**ìœ¼ë¡œ dual ë¬¸ì œë¥¼ í‘¸ëŠ” ì•Œê³ ë¦¬ì¦˜

ðŸ’¡ SMOëŠ” ì‹¤ì œ SVM êµ¬í˜„ì— í•„ìˆ˜ì ì¸ ìµœì í™” ë°©ë²•

---

## âœ… í•µì‹¬ ìš”ì•½

- SVMì€ margin ê¸°ë°˜ì˜ ê°•ë ¥í•œ ë¶„ë¥˜ê¸°
- Dual formìœ¼ë¡œ ë³€í™˜ í›„, **kernel trick**ì„ ì ìš©í•´ ë¹„ì„ í˜• ë¶„ë¥˜ê¸° êµ¬í˜„
- ìµœì¢… ì˜ˆì¸¡ í•¨ìˆ˜ëŠ” support vectorì˜ kernel ê°€ì¤‘í•©
- ì»¤ë„ ì„ íƒê³¼ ì •ê·œí™” íŒŒë¼ë¯¸í„° $C$ê°€ ì„±ëŠ¥ì— í° ì˜í–¥
- **SMO ì•Œê³ ë¦¬ì¦˜**ì€ íš¨ìœ¨ì ì¸ SVM í•™ìŠµì˜ í•µì‹¬ ë„êµ¬



# ðŸŒŒ Session 5: Gaussian Processes (GP)

## ðŸŒ± ê°œë… íë¦„ ìš”ì•½

- ê¸°ì¡´ì˜ íšŒê·€ ëª¨ë¸ì€ **ë‹¨ í•˜ë‚˜ì˜ ìµœì  í•¨ìˆ˜ $f$ë¥¼ ì°¾ëŠ” ê²ƒ**ì´ì—ˆìŒ
- GPëŠ” **ëª¨ë“  ê°€ëŠ¥í•œ í•¨ìˆ˜ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ì •ì˜**í•˜ê³ ,  
  í•™ìŠµ ë°ì´í„°ë¥¼ í†µí•´ posterior ë¶„í¬ë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ ì˜ˆì¸¡í•¨
- í•µì‹¬ ê°œë…ì€ **multivariate Gaussian distribution over functions**
- ì»¤ë„ í•¨ìˆ˜ $k(x,x')$ëŠ” ìž…ë ¥ ê°„ì˜ **í•¨ìˆ˜ ê°’ ìœ ì‚¬ë„**ë¥¼ ì •ì˜

---

## ðŸ”‘ ê°œë…ë³„ ì •ë¦¬ (ê°œë… â†” ìˆ˜ì‹ ì—°ê²° ì¤‘ì‹¬)

### âœ… Gaussian Process ì •ì˜

- **GPëŠ” í•¨ìˆ˜ë“¤ì˜ ë¶„í¬**:
  $$
  f(\cdot) \sim \mathcal{GP}(m(\cdot), k(\cdot, \cdot))
  $$
- $m(x)$: í‰ê·  í•¨ìˆ˜ (ë³´í†µ 0ìœ¼ë¡œ ê°€ì •)
- $k(x,x')$: ì»¤ë„ í•¨ìˆ˜ = ê³µë¶„ì‚° í•¨ìˆ˜

---

### âœ… GP Prior

- ìž„ì˜ì˜ ìž…ë ¥ ì§‘í•© $X = \{x_1, \ldots, x_n\}$ì— ëŒ€í•´:
  $$
  \mathbf{f} = \begin{bmatrix} f(x_1) \\ \vdots \\ f(x_n) \end{bmatrix} \sim \mathcal{N}(0, K)
  $$
- $K_{ij} = k(x_i, x_j)$

---

### âœ… Observation ëª¨ë¸ (Noise í¬í•¨)

- ê´€ì¸¡ê°’: $y_i = f(x_i) + \epsilon_i$, $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$
- ê´€ì¸¡ ë°ì´í„° ë²¡í„° $\mathbf{y}$ì˜ ë¶„í¬:
  $$
  \mathbf{y} \sim \mathcal{N}(0, K + \sigma^2 I)
  $$

---

### âœ… GP Posterior (ì˜ˆì¸¡ ë¶„í¬)

- í…ŒìŠ¤íŠ¸ ìž…ë ¥: $X^* = \{x_1^*, \ldots, x_m^*\}$
- ì—°ê´€ëœ joint ë¶„í¬:
  $$
  \begin{bmatrix} \mathbf{y} \\ \mathbf{f}^* \end{bmatrix} \sim \mathcal{N}\left(0, \begin{bmatrix} K(X,X) + \sigma^2 I & K(X,X^*) \\ K(X^*,X) & K(X^*,X^*) \end{bmatrix} \right)
  $$

- ì¡°ê±´ë¶€ ë¶„í¬ (posterior predictive distribution):
  $$
  \mathbf{f}^* \mid \mathbf{y}, X, X^* \sim \mathcal{N}(\boldsymbol{\mu}_*, \Sigma_*)
  $$

- í‰ê·  (ì˜ˆì¸¡ê°’):
  $$
  \boldsymbol{\mu}_* = K(X^*, X)\left(K(X,X) + \sigma^2 I\right)^{-1} \mathbf{y}
  $$

- ë¶„ì‚° (ë¶ˆí™•ì‹¤ì„±):
  $$
  \Sigma_* = K(X^*,X^*) + \sigma^2 I - K(X^*, X)\left(K(X,X) + \sigma^2 I\right)^{-1} K(X, X^*)
  $$

---

### âœ… ì»¤ë„ í•¨ìˆ˜ ì˜ˆì‹œ

| ì´ë¦„ | ìˆ˜ì‹ | íŠ¹ì§• |
|------|------|------|
| **Squared Exponential (RBF)** | $\exp\left( -\frac{\|x - x'\|^2}{2 \tau^2} \right)$ | ë§¤ë„ëŸ¬ìš´ í•¨ìˆ˜ |
| **Polynomial** | $(x^\top x' + c)^p$ | ë‹¤í•­ íšŒê·€ ëª¨ë¸ |
| **Linear** | $x^\top x'$ | ì„ í˜• íšŒê·€ì™€ ë™ì¼ |

---

### âœ… GPR ì˜ˆì¸¡ ì•Œê³ ë¦¬ì¦˜ ìš”ì•½

1. Compute kernel matrix $K = k(X, X)$  
2. Compute kernel vector $k_* = k(X, x^*)$  
3. Solve $\alpha = (K + \sigma^2 I)^{-1} y$  
4. ì˜ˆì¸¡ í‰ê· :
   $$
   \hat{y}^* = k_*^\top \alpha
   $$
5. ì˜ˆì¸¡ ë¶„ì‚°:
   $$
   \text{Var}(y^*) = k(x^*, x^*) + \sigma^2 - k_*^\top (K + \sigma^2 I)^{-1} k_*
   $$

---

### âœ… GPRì˜ íŠ¹ì§•

| í•­ëª© | ë‚´ìš© |
|------|------|
| ëª¨ë¸ í˜•íƒœ | í•¨ìˆ˜ ì „ì²´ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ |
| í•™ìŠµ | í™•ë¥ ì  ì¶”ë¡  (posterior ê³„ì‚°) |
| ì˜ˆì¸¡ ê²°ê³¼ | í‰ê·  + ë¶„ì‚° (ë¶ˆí™•ì‹¤ì„± í¬í•¨) |
| ìž¥ì  | ë§¤ë„ëŸ½ê³  ìœ ì—°í•¨, ë¶ˆí™•ì‹¤ì„± ì¶”ì • ê°€ëŠ¥ |
| ë‹¨ì  | ê³„ì‚°ëŸ‰ $O(n^3)$ (inverse), í° ë°ì´í„°ì…‹ì— ë¹„íš¨ìœ¨ |

---

## âœ… í•µì‹¬ ìš”ì•½

- Gaussian ProcessëŠ” **í•¨ìˆ˜ ìžì²´ë¥¼ í™•ë¥  ë³€ìˆ˜ë¡œ ëª¨ë¸ë§**
- ì˜ˆì¸¡ì€ **í™•ë¥  ë¶„í¬**ë¡œ ë‚˜ì™€ì„œ **ë¶ˆí™•ì‹¤ì„±ê¹Œì§€ í¬í•¨ëœ íšŒê·€ ê²°ê³¼** ì œê³µ
- ì»¤ë„ í•¨ìˆ˜ë¡œ **ë¹„ì„ í˜• íŒ¨í„´ì„ ìœ ì—°í•˜ê²Œ ëª¨ë¸ë§** ê°€ëŠ¥
- ê³ ê¸‰ íšŒê·€, Bayesian ëª¨ë¸ë§, Active Learning ë“±ì— í•„ìˆ˜ ê°œë…!