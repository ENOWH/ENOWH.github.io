---
title: "Chapter03"
excerpt: "머신러닝 Chap03 전체 흐름"

categories:
  - MachineLearning
tags:
  - [Machine Learning, 머신러닝]

permalink: /categories/MachineLearning/Chapter03_MachineLearning

toc: true
toc_sticky: true
use_math: true

date: 2025-04-20
last_modified_at: 2025-04-20
---



# 📘 Chapter 3: Kernel Methods (`slide_ch3_0409.pdf`)

## 📑 세션별 구성 및 주요 주제 정리

---

### 🟦 Session 1: Kernel Methods - Foundations

- **Nonparametric 모델** 개요: 파라미터를 정하지 않고 함수 자체를 추정
- **Feature Map $\Phi(\mathbf{x})$**: 원래 공간에서 고차원 공간으로의 매핑
- **Kernel Trick**: 
  $$
  k(\mathbf{x}, \mathbf{x}') = \langle \Phi(\mathbf{x}), \Phi(\mathbf{x}') \rangle
  $$
- 계산 효율성을 위해 직접 feature map 계산 없이 inner product 계산만 수행
- **대표 커널**:
  - Polynomial: $(\mathbf{x}^\top \mathbf{x}' + c)^p$
  - Gaussian (RBF): $\exp\left( - \frac{\|\mathbf{x} - \mathbf{x}'\|^2}{2 \ell^2} \right)$

---

### 🟦 Session 2: Kernel Ridge Regression (KRR)

- **Ridge Regression** + **Kernel Trick** = KRR
- 목적 함수:
  $$
  \min_{\mathbf{w}} \frac{1}{n} \sum_{i=1}^n (y_i - \mathbf{w}^\top \Phi(\mathbf{x}_i))^2 + \lambda \|\mathbf{w}\|^2
  $$
- 커널화된 최종 예측 함수:
  $$
  \hat{f}(\mathbf{x}) = \mathbf{y}^\top (K + n\lambda I)^{-1} \mathbf{k}(\mathbf{x})
  $$
- Offset 포함 시 중심화(centering) 필요 → centered kernel matrix 사용

---

### 🟦 Session 3: Constrained Optimization

- **제약조건 있는 최적화**:
  $$
  \min_{\mathbf{x}} f(\mathbf{x}) \quad \text{s.t. } g_i(\mathbf{x}) \leq 0,\; h_j(\mathbf{x}) = 0
  $$
- **Lagrangian**:
  $$
  \mathcal{L}(\mathbf{x}, \boldsymbol{\alpha}, \boldsymbol{\beta}) = f(\mathbf{x}) + \sum \alpha_i g_i(\mathbf{x}) + \sum \beta_j h_j(\mathbf{x})
  $$
- **Dual problem**: 
  $$
  \max_{\alpha \geq 0, \beta} \min_{\mathbf{x}} \mathcal{L}(\mathbf{x}, \alpha, \beta)
  $$
- **KKT 조건**: 최적해가 만족해야 하는 4가지 조건
  - Primal feasibility
  - Dual feasibility
  - Complementary slackness
  - Stationarity

---

### 🟦 Session 4: Kernelized Support Vector Machines (SVM)

- Soft-margin SVM의 dual 문제를 유도하고 kernelization 적용
- 최종 SVM 분류기:
  $$
  f(\mathbf{x}) = \text{sign} \left( \sum_{i=1}^n \alpha_i y_i k(\mathbf{x}_i, \mathbf{x}) + b \right)
  $$
- **Support Vectors**: $\alpha_i > 0$인 샘플들만 결정 경계에 기여
- **SMO 알고리즘**: Dual 문제를 효율적으로 푸는 방법 (2개 변수만 선택해서 QP 문제 반복 해결)

---

### 🟦 Session 5: Gaussian Processes (GP)

- 완전한 **Bayesian regression** 접근 방식
- 함수 자체를 랜덤 변수로 보고 **확률 분포**를 정의
- $f \sim \mathcal{GP}(m(\cdot), k(\cdot, \cdot))$
- 예측 분포 (posterior predictive):
  $$
  y^* \mid \mathbf{y}, X, X^* \sim \mathcal{N}(\mu_*, \Sigma_*)
  $$
  - 평균: $\mu_* = K(X^*, X)(K(X, X) + \sigma^2 I)^{-1} \mathbf{y}$
  - 분산: $\Sigma_* = K(X^*, X^*) + \sigma^2 I - K(X^*, X)(K(X, X) + \sigma^2 I)^{-1} K(X, X^*)$
- 확률적 예측과 불확실성 정량화가 가능

---

## 🧭 전체 흐름 요약

1. **Kernel Methods**로 고차원에서의 선형화 & 비선형 분류 가능성 확보
2. **KRR**은 kernel trick을 통해 비선형 회귀 문제 해결
3. **Constrained Optimization**은 SVM과 같은 모델 학습에 수학적 기반 제공
4. **Kernelized SVM**은 margin 기반의 강력한 분류기
5. **Gaussian Processes**는 비모수 Bayesian 추론으로 불확실성까지 포함한 예측 제공



# 🧠 Session 1: Kernel Methods - Foundations

## 🌱 개념 흐름 요약

- Nonparametric 방법은 **모델 파라미터**를 미리 정하지 않고, **데이터 전체로부터 함수 자체를 추정**하는 방식
- 데이터를 **비선형적으로 변환 (nonlinear feature map)** 후 **선형 모델을 적용**
- 고차원 feature space에서의 계산을 커널 함수로 효율적으로 수행 → **Kernel Trick**

---

## 🔑 개념별 정리 (개념 ↔ 수식 연결 중심)

### ✅ Nonparametric Learning

- 관측 데이터: $D = \{(\mathbf{x}_i, y_i)\}_{i=1}^N$
- 목적: 특정한 파라미터 모델을 사용하지 않고, 함수 $f$ 자체를 추정
- 예측:
  $$
  \hat{f}(\mathbf{x}^*) \approx \sum_{i=1}^N \text{similarity}(\mathbf{x}_i, \mathbf{x}^*) \cdot y_i
  $$
- 전체 데이터를 기억해야 하므로 parametric 방법보다 메모리 비용이 큼

---

### ✅ Feature Map $\Phi(\mathbf{x})$

- 원래 입력 공간 $\mathbb{R}^d$에서 고차원 공간 $\mathbb{R}^m$으로 변환:
  $$
  \Phi: \mathbb{R}^d \rightarrow \mathbb{R}^m
  $$

- 예: 다항식 변환
  $$
  \Phi(x) = \begin{bmatrix} x \\ x^2 \\ x^3 \end{bmatrix}, \quad f(x) = \mathbf{w}^\top \Phi(x) + b
  $$

---

### ✅ Curse of Dimensionality 문제

- $\Phi(x)$의 차원 $m$은 매우 커질 수 있음 (예: degree-$p$ 다항식의 경우 $m = \binom{p+d}{d}$)
- 직접 계산하면 너무 많은 feature 연산이 필요 → **Kernel Trick 필요**

---

### ✅ Kernel Function (커널 함수)

- **정의**: 입력 $\mathbf{x}, \mathbf{x}'$ 간의 내적을 고차원 feature space에서 계산한 것처럼 만드는 함수
  $$
  k(\mathbf{x}, \mathbf{x}') = \langle \Phi(\mathbf{x}), \Phi(\mathbf{x}') \rangle
  $$

- **Kernel Trick**:
  - 내적 기반 알고리즘에서 $\Phi$를 직접 계산하지 않고 $k(\mathbf{x}, \mathbf{x}')$만으로 처리

---

### ✅ 대표적인 커널 함수들

| 이름 | 수식 | 특징 |
|------|------|------|
| **Linear** | $k(\mathbf{x}, \mathbf{x}') = \mathbf{x}^\top \mathbf{x}'$ | $\Phi(x) = x$ |
| **Polynomial** | $k(\mathbf{x}, \mathbf{x}') = (\mathbf{x}^\top \mathbf{x}' + c)^p$ | 고차 다항식 모델 |
| **RBF (Gaussian)** | $k(\mathbf{x}, \mathbf{x}') = \exp\left( -\frac{\|\mathbf{x} - \mathbf{x}'\|^2}{2 \ell^2} \right)$ | 무한 차원 매핑 |
| **Sigmoid** | $k(\mathbf{x}, \mathbf{x}') = \tanh(\alpha \mathbf{x}^\top \mathbf{x}' + \beta)$ | 신경망과 유사 |

---

### ✅ Inner Product Kernels & Mercer Condition

- **Inner Product Kernel**:
  $$
  k(\mathbf{x}, \mathbf{x}') = \langle \Phi(\mathbf{x}), \Phi(\mathbf{x}') \rangle
  $$
- **Mercer Kernel**:
  - $k$이 **symmetric**이고, **positive semi-definite (PSD)**이면 kernel로 사용할 수 있음
  - 즉, kernel matrix $K = [k(x_i, x_j)]$가 모든 입력에 대해 PSD면 OK

---

### ✅ 계산 비교 예시

#### ✏️ Explicit Feature Mapping (예: $\Phi(x) = (x_1^2, \sqrt{2}x_1x_2, x_2^2)$)
- Feature 계산 + 내적: 곱셈 11회, 덧셈 2회

#### ⚡ Kernel 계산 (Polynomial kernel of degree 2)
- $k(\mathbf{x}, \mathbf{x}') = (\mathbf{x}^\top \mathbf{x}')^2$
- 곱셈 3회, 덧셈 1회 → **훨씬 효율적!**

---

## ✅ 핵심 요약

- Kernel 방법은 **고차원에서의 내적을 저차원에서 커널 함수로 우회 계산**
- 이를 통해 선형 모델을 **비선형 문제에 적용**할 수 있음
- **RBF, Polynomial, Sigmoid** 등 다양한 커널을 통해 문제 특성에 맞게 선택 가능
- SVM, Ridge, PCA 등 다양한 알고리즘이 **kernelized** 가능



# 📐 Session 2: Kernel Ridge Regression (KRR)

## 🌱 개념 흐름 요약

- Ridge Regression은 선형 회귀에 정규화를 더한 것
- KRR은 여기에 **커널 트릭**을 결합하여 **비선형 회귀**가 가능하도록 확장
- 핵심은 예측 함수를 다음과 같이 표현하는 것:
  $$
  \hat{f}(\mathbf{x}) = \sum_{i=1}^n \alpha_i k(\mathbf{x}_i, \mathbf{x})
  $$
- $\alpha_i$는 학습 데이터와 커널에 기반해 계산됨

---

## 🔑 개념별 정리 (개념 ↔ 수식 연결 중심)

### ✅ Ridge Regression (without offset)

- 입력: $X \in \mathbb{R}^{n \times d}$, $y \in \mathbb{R}^n$
- 목적 함수:
  $$
  \min_{\mathbf{w}} \frac{1}{n} \|y - X\mathbf{w}\|^2 + \lambda \|\mathbf{w}\|^2
  $$

- 해(solution):
  $$
  \hat{\mathbf{w}} = (X^\top X + n\lambda I)^{-1} X^\top y
  $$

---

### ✅ 회귀 함수 표현

- 예측 함수:
  $$
  \hat{f}(\mathbf{x}) = \hat{\mathbf{w}}^\top \mathbf{x}
  $$
- 내적 기반으로 표현:
  $$
  \hat{f}(\mathbf{x}) = y^\top X (X^\top X + n\lambda I)^{-1} \mathbf{x}
  $$

---

### ✅ Kernelization 목표

- 내적 $\langle \mathbf{x}_i, \mathbf{x}_j \rangle$ 형태만으로 표현되므로, **kernel trick 가능**
- Gram matrix $G \in \mathbb{R}^{n \times n}$:
  $$
  G_{ij} = \langle \mathbf{x}_i, \mathbf{x}_j \rangle
  $$

---

### ✅ Kernel Ridge Regression

- 커널을 적용한 최종 예측 함수:
  $$
  \hat{f}(\mathbf{x}) = \mathbf{y}^\top (K + n\lambda I)^{-1} \mathbf{k}(\mathbf{x})
  $$

- 여기서:
  - $K \in \mathbb{R}^{n \times n}$: kernel matrix, $K_{ij} = k(\mathbf{x}_i, \mathbf{x}_j)$
  - $\mathbf{k}(\mathbf{x}) = [k(\mathbf{x}_1, \mathbf{x}), \ldots, k(\mathbf{x}_n, \mathbf{x})]^\top$

---

### ✅ Bias (offset) 포함한 KRR

- 모델: $\hat{f}(\mathbf{x}) = \mathbf{w}^\top \mathbf{x} + b$
- 중심화(centered)된 데이터 사용: $\tilde{x}_i = x_i - \bar{x}$, $\tilde{y}_i = y_i - \bar{y}$
- 최종 예측:
  $$
  \hat{f}(\mathbf{x}) = \bar{y} + \tilde{\mathbf{y}}^\top (K̃ + n\lambda I)^{-1} \tilde{\mathbf{k}}(\mathbf{x})
  $$

- **centered kernel matrix** $K̃$와 벡터 $\tilde{k}(\mathbf{x})$는 다음과 같이 계산됨:
  $$
  \tilde{k}(x_i, x_j) = k(x_i, x_j) - \frac{1}{n} \sum_r k(x_i, x_r) - \frac{1}{n} \sum_s k(x_s, x_j) + \frac{1}{n^2} \sum_{r,s} k(x_r, x_s)
  $$

---

### ✅ 예시: Gaussian Kernel + KRR

- Gaussian (RBF) Kernel:
  $$
  k(x, x') = \exp\left( - \frac{\|x - x'\|^2}{2 \ell^2} \right)
  $$

- 최종 예측 함수:
  $$
  \hat{f}(x) = \sum_{i=1}^n \alpha_i k(x, x_i)
  $$
- $\alpha = (K + n\lambda I)^{-1} y$

---

### ✅ KRR의 특징 요약

| 항목 | 내용 |
|------|------|
| 모델 유형 | 비선형 회귀 (커널 기반) |
| 정규화 | Ridge 방식: $\lambda \|\mathbf{w}\|^2$ |
| 커널 사용 | 내적 대신 커널 함수 $k(x,x')$ 사용 |
| 학습 데이터 의존성 | 모든 학습 데이터를 저장해야 함 (nonparametric) |
| 계산 복잡도 | $O(n^3)$ (matrix inversion), 작은 데이터셋에 적합 |

---

## ✅ 핵심 요약

- **KRR은 비선형 회귀 문제를 선형처럼 푸는 핵심 기법**
- 내적 기반 Ridge Regression을 커널화 → 고차원에서도 계산 가능
- 커널 선택과 regularization이 모델 성능에 큰 영향



# 🔒 Session 3: Constrained Optimization

## 🌱 개념 흐름 요약

- 머신러닝의 많은 문제는 **제약조건이 있는 최적화 문제**로 구성됨
- 일반적인 unconstrained 최적화에서는 $\nabla f(x^*) = 0$ 이면 충분했지만,
  **제약이 있을 경우 해가 경계(boundary)에 존재할 수 있음**
- 따라서 **Lagrangian을 이용한 dual formulation**이 필요하며,
  **KKT 조건**이 최적성 판단의 핵심 기준이 됨

---

## 🔑 개념별 정리 (개념 ↔ 수식 연결 중심)

### ✅ 제약 최적화 문제 정의

- 일반적인 형식:
  $$
  \min_{x \in \mathbb{R}^n} f(x) \quad \text{s.t. } g_i(x) \leq 0,\; h_j(x) = 0
  $$
  - $f(x)$: 목적 함수 (convex)
  - $g_i(x)$: 부등식 제약 조건
  - $h_j(x)$: 등식 제약 조건 (affine)

---

### ✅ Lagrangian 정의

- 제약조건을 목적함수에 통합한 식:
  $$
  \mathcal{L}(x, \boldsymbol{\alpha}, \boldsymbol{\beta}) = f(x) + \sum_{i=1}^m \alpha_i g_i(x) + \sum_{j=1}^p \beta_j h_j(x)
  $$
  - $\alpha_i \geq 0$: 부등식 제약의 라그랑주 승수
  - $\beta_j$: 등식 제약의 라그랑주 승수

---

### ✅ Primal & Dual 문제

#### 🔹 Primal 문제:
- 원래의 문제:
  $$
  \min_x \theta_P(x) = \max_{\alpha \geq 0, \beta} \mathcal{L}(x, \alpha, \beta)
  $$

#### 🔹 Dual 문제:
- 제약 조건을 내재화한 dual 목적함수:
  $$
  \theta_D(\alpha, \beta) = \min_x \mathcal{L}(x, \alpha, \beta)
  $$
- Dual 문제:
  $$
  \max_{\alpha \geq 0, \beta} \theta_D(\alpha, \beta)
  $$

---

### ✅ Duality Gap & Lemmas

- **Lemma 1 (Dual bound)**: $\theta_D(\alpha, \beta) \leq p^*$ (primal optimum)
- **Weak Duality**:
  $$
  d^* = \theta_D(\alpha^*, \beta^*) \leq p^* = \theta_P(x^*)
  $$
- **Strong Duality** (Slater 조건 만족 시):
  $$
  d^* = p^*
  $$

---

### ✅ Complementary Slackness

- **조건**:
  $$
  \alpha_i^* \cdot g_i(x^*) = 0 \quad \forall i
  $$
- 의미: 어떤 제약 조건이 **활성(active)** 되어 있다면, 해당 $\alpha_i^*$는 양수이고  
  제약이 **비활성**이면 $\alpha_i^* = 0$

---

### ✅ KKT (Karush-Kuhn-Tucker) 조건

**Convex 최적화 문제에서 최적해가 만족해야 하는 4가지 조건**:

1. **Primal Feasibility**: $g_i(x^*) \leq 0$, $h_j(x^*) = 0$
2. **Dual Feasibility**: $\alpha_i^* \geq 0$
3. **Complementary Slackness**: $\alpha_i^* g_i(x^*) = 0$
4. **Stationarity**:  
   $$
   \nabla f(x^*) + \sum_i \alpha_i^* \nabla g_i(x^*) + \sum_j \beta_j^* \nabla h_j(x^*) = 0
   $$

✅ 이 네 가지가 모두 만족되면 $(x^*, \alpha^*, \beta^*)$는 최적해!

---

### ✅ 예제 문제 (직관 연습)

**문제**:
$$
\min_{x \in \mathbb{R}^2} x_1^2 + x_2 \\
\text{s.t. } 2x_1 + x_2 \geq 4, \quad x_2 \geq 1
$$

→ 표준형으로 변환:
$$
g_1(x) = -2x_1 - x_2 + 4 \leq 0, \quad g_2(x) = -x_2 + 1 \leq 0
$$

**해법**:
- Lagrangian 구성
- Stationarity 조건으로 $x_1, x_2$ 도출
- Complementary Slackness 확인
- Primal, Dual Feasibility 확인
- KKT 조건이 모두 만족되는 최적점 도출

📌 최적해: $x^* = (1, 2)$, $f(x^*) = 3$

---

## ✅ 핵심 요약

- 제약조건이 있는 최적화는 **Lagrangian + Duality + KKT 조건**으로 분석 가능
- **Primal/dual 문제를 모두 이해**해야 해석 가능
- **KKT 조건**은 머신러닝 모델 (ex. SVM)에서 최적성을 판별하는 핵심 도구



# 🧱 Session 4: Kernelized Support Vector Machines (SVM)

## 🌱 개념 흐름 요약

- SVM은 **margin이 최대가 되는 선형 분류기**를 찾는 문제
- 선형 분리 불가능한 경우를 위해 slack 변수 도입 (soft-margin)
- 최적화 문제를 dual form으로 바꾸면, **커널 트릭을 적용해 비선형 분류도 가능**
- 이로써 SVM은 **강력하고 일반화 성능이 높은 커널 기반 분류기**로 확장됨

---

## 🔑 개념별 정리 (개념 ↔ 수식 연결 중심)

### ✅ Soft-Margin SVM: Primal Formulation

- 입력: $\{(\mathbf{x}_i, y_i)\}_{i=1}^n$, $y_i \in \{-1, +1\}$
- 최적화 문제:
  $$
  \min_{\mathbf{w}, b, \boldsymbol{\xi}} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^n \xi_i
  $$
  $$
  \text{s.t. } y_i (\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 - \xi_i,\quad \xi_i \geq 0
  $$

- 목적: margin 최대화 + 오분류 페널티 최소화

---

### ✅ Lagrangian 구성

- Lagrangian 함수:
  $$
  \mathcal{L}(\mathbf{w}, b, \boldsymbol{\xi}, \boldsymbol{\alpha}, \boldsymbol{\beta}) = \frac{1}{2} \|\mathbf{w}\|^2 + C \sum \xi_i - \sum \alpha_i [y_i(\mathbf{w}^\top \mathbf{x}_i + b) - 1 + \xi_i] - \sum \beta_i \xi_i
  $$

---

### ✅ Dual Problem 유도

- Stationarity 조건을 이용해 $\mathbf{w}$, $b$, $\xi_i$ 제거 후 dual 문제로 정리

📐 최종 Dual 문제:
$$
\max_{\boldsymbol{\alpha}} -\frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j \langle \mathbf{x}_i, \mathbf{x}_j \rangle + \sum_i \alpha_i
$$
$$
\text{s.t. } \sum_i \alpha_i y_i = 0,\quad 0 \leq \alpha_i \leq \frac{C}{n}
$$

- 결정 함수:
  $$
  f(\mathbf{x}) = \operatorname{sign} \left( \sum_{i=1}^n \alpha_i^* y_i \langle \mathbf{x}_i, \mathbf{x} \rangle + b^* \right)
  $$

---

### ✅ Kernelization (커널화)

- 커널 트릭 적용:
  $$
  k(\mathbf{x}_i, \mathbf{x}_j) = \langle \Phi(\mathbf{x}_i), \Phi(\mathbf{x}_j) \rangle
  $$

📐 최종 Kernel SVM 분류기:
$$
f(\mathbf{x}) = \operatorname{sign} \left( \sum_{i=1}^n \alpha_i^* y_i k(\mathbf{x}_i, \mathbf{x}) + b^* \right)
$$

- $\alpha_i^* > 0$인 샘플만 결정 경계에 기여 → **Support Vectors**

---

### ✅ b* 계산 방법

- $\alpha_j^* \in (0, \frac{C}{n})$ 인 샘플에 대해:
  $$
  b^* = y_j - \sum_{i=1}^n \alpha_i^* y_i k(\mathbf{x}_i, \mathbf{x}_j)
  $$

- 여러 support vector가 있을 경우 평균 사용 가능

---

### ✅ Support Vector 분류 기준

| 위치 | 조건 | $\alpha_i^*$ 값 |
|------|------|-----------------|
| Margin 바깥 | $y_i f(x_i) > 1$ | 0 |
| Margin 위 | $y_i f(x_i) = 1$ | $0 < \alpha_i^* < \frac{C}{n}$ |
| Margin 안 | $0 < y_i f(x_i) < 1$ | $\alpha_i^* = \frac{C}{n}$ |
| Misclassified | $y_i f(x_i) < 0$ | $\alpha_i^* = \frac{C}{n}$ |

---

### ✅ SMO (Sequential Minimal Optimization)

- Dual problem의 제한 조건 때문에 좌표상승법을 직접 쓰기 어려움
- **두 변수씩 선택해서 최적화하는 방식**으로 dual 문제를 푸는 알고리즘

💡 SMO는 실제 SVM 구현에 필수적인 최적화 방법

---

## ✅ 핵심 요약

- SVM은 margin 기반의 강력한 분류기
- Dual form으로 변환 후, **kernel trick**을 적용해 비선형 분류기 구현
- 최종 예측 함수는 support vector의 kernel 가중합
- 커널 선택과 정규화 파라미터 $C$가 성능에 큰 영향
- **SMO 알고리즘**은 효율적인 SVM 학습의 핵심 도구



# 🌌 Session 5: Gaussian Processes (GP)

## 🌱 개념 흐름 요약

- 기존의 회귀 모델은 **단 하나의 최적 함수 $f$를 찾는 것**이었음
- GP는 **모든 가능한 함수에 대한 확률 분포를 정의**하고,  
  학습 데이터를 통해 posterior 분포를 업데이트하여 예측함
- 핵심 개념은 **multivariate Gaussian distribution over functions**
- 커널 함수 $k(x,x')$는 입력 간의 **함수 값 유사도**를 정의

---

## 🔑 개념별 정리 (개념 ↔ 수식 연결 중심)

### ✅ Gaussian Process 정의

- **GP는 함수들의 분포**:
  $$
  f(\cdot) \sim \mathcal{GP}(m(\cdot), k(\cdot, \cdot))
  $$
- $m(x)$: 평균 함수 (보통 0으로 가정)
- $k(x,x')$: 커널 함수 = 공분산 함수

---

### ✅ GP Prior

- 임의의 입력 집합 $X = \{x_1, \ldots, x_n\}$에 대해:
  $$
  \mathbf{f} = \begin{bmatrix} f(x_1) \\ \vdots \\ f(x_n) \end{bmatrix} \sim \mathcal{N}(0, K)
  $$
- $K_{ij} = k(x_i, x_j)$

---

### ✅ Observation 모델 (Noise 포함)

- 관측값: $y_i = f(x_i) + \epsilon_i$, $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$
- 관측 데이터 벡터 $\mathbf{y}$의 분포:
  $$
  \mathbf{y} \sim \mathcal{N}(0, K + \sigma^2 I)
  $$

---

### ✅ GP Posterior (예측 분포)

- 테스트 입력: $X^* = \{x_1^*, \ldots, x_m^*\}$
- 연관된 joint 분포:
  $$
  \begin{bmatrix} \mathbf{y} \\ \mathbf{f}^* \end{bmatrix} \sim \mathcal{N}\left(0, \begin{bmatrix} K(X,X) + \sigma^2 I & K(X,X^*) \\ K(X^*,X) & K(X^*,X^*) \end{bmatrix} \right)
  $$

- 조건부 분포 (posterior predictive distribution):
  $$
  \mathbf{f}^* \mid \mathbf{y}, X, X^* \sim \mathcal{N}(\boldsymbol{\mu}_*, \Sigma_*)
  $$

- 평균 (예측값):
  $$
  \boldsymbol{\mu}_* = K(X^*, X)\left(K(X,X) + \sigma^2 I\right)^{-1} \mathbf{y}
  $$

- 분산 (불확실성):
  $$
  \Sigma_* = K(X^*,X^*) + \sigma^2 I - K(X^*, X)\left(K(X,X) + \sigma^2 I\right)^{-1} K(X, X^*)
  $$

---

### ✅ 커널 함수 예시

| 이름 | 수식 | 특징 |
|------|------|------|
| **Squared Exponential (RBF)** | $\exp\left( -\frac{\|x - x'\|^2}{2 \tau^2} \right)$ | 매끄러운 함수 |
| **Polynomial** | $(x^\top x' + c)^p$ | 다항 회귀 모델 |
| **Linear** | $x^\top x'$ | 선형 회귀와 동일 |

---

### ✅ GPR 예측 알고리즘 요약

1. Compute kernel matrix $K = k(X, X)$  
2. Compute kernel vector $k_* = k(X, x^*)$  
3. Solve $\alpha = (K + \sigma^2 I)^{-1} y$  
4. 예측 평균:
   $$
   \hat{y}^* = k_*^\top \alpha
   $$
5. 예측 분산:
   $$
   \text{Var}(y^*) = k(x^*, x^*) + \sigma^2 - k_*^\top (K + \sigma^2 I)^{-1} k_*
   $$

---

### ✅ GPR의 특징

| 항목 | 내용 |
|------|------|
| 모델 형태 | 함수 전체에 대한 확률 분포 |
| 학습 | 확률적 추론 (posterior 계산) |
| 예측 결과 | 평균 + 분산 (불확실성 포함) |
| 장점 | 매끄럽고 유연함, 불확실성 추정 가능 |
| 단점 | 계산량 $O(n^3)$ (inverse), 큰 데이터셋에 비효율 |

---

## ✅ 핵심 요약

- Gaussian Process는 **함수 자체를 확률 변수로 모델링**
- 예측은 **확률 분포**로 나와서 **불확실성까지 포함된 회귀 결과** 제공
- 커널 함수로 **비선형 패턴을 유연하게 모델링** 가능
- 고급 회귀, Bayesian 모델링, Active Learning 등에 필수 개념!