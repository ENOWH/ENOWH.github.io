---
title: "Chapter05"
excerpt: "MachineLearning_Chap05"

categories:
  - MachineLearning
tags:
  - [Machine Learning, ë¨¸ì‹ ëŸ¬ë‹]

permalink: /categories/MachineLearning/Chapter05_MachineLearning

toc: true
toc_sticky: true

date: 2025-05-27
last_modified_at: 2025-05-27
---

# ğŸ“˜ Chapter 5: Multilayer Perceptrons & Reinforcement Learning

## Part 1. Multilayer Perceptrons (MLPs)
1. Feedforward Neural Networks
2. Structure of MLP
3. Forward Pass and Algorithm
4. Activation Functions (Sigmoid, tanh, ReLU)
5. Mathematical Formulation
6. MLPs for Regression and Classification
7. Softmax and Prediction
8. Empirical Risk Minimization
9. Backpropagation Algorithm
10. Practical Considerations
11. Summary

---

## Part 2. Reinforcement Learning (RL)
1. Introduction and Framework
2. Policies (Stochastic vs Deterministic)
3. Returns (Episodic vs Continuing)
4. Value Functions: VÏ€ and QÏ€
5. Multi-Armed Bandit Problem
6. Action Selection (Îµ-Greedy, Exploration vs Exploitation)
7. MDPs and Bellman Equations
8. Solving Bellman Equations
9. Policy Iteration vs Value Iteration
10. Learning via Interaction (Model-Free RL)
    - Monte Carlo Methods
    - Temporal Difference (TD) Learning
    - SARSA
    - Q-Learning
11. Policy Gradient Methods
    - REINFORCE
    - Baseline and Advantage Function
    - Gradient Derivation

---

# Part 1. Multilayer Perceptrons (MLPs)

## âœ… ì£¼ìš” ê°œë… íë¦„
Multilayer Perceptron (MLP)ì€ Feedforward Neural Network(FNN)ì˜ ëŒ€í‘œì ì¸ í˜•íƒœë¡œ,  
ì…ë ¥ â†’ ì€ë‹‰ì¸µ â†’ ì¶œë ¥ì¸µ êµ¬ì¡°ë¥¼ ê°€ì§€ë©°, ê° ì¸µì€ **ì„ í˜•ë³€í™˜ + ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜**ë¡œ êµ¬ì„±ë¨.  
í•™ìŠµì€ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ë„ë¡ **ì—­ì „íŒŒ(backpropagation)**ë¥¼ í†µí•´ ìˆ˜í–‰ëœë‹¤.

---

## âœ… ê°œë…ê³¼ ìˆ˜ì‹ ì •ë¦¬

### ğŸ¯ Forward Pass

ì…ë ¥ $$ \mathbf{x} $$ì— ëŒ€í•´ ì¶œë ¥ $$ f(\mathbf{x}) $$ë¥¼ ê³„ì‚°í•˜ëŠ” ê³¼ì •:

- ì´ˆê¸°ê°’:
$$ z^{(0)} = \mathbf{x} $$

- ê° ì¸µ â„“ (1 â‰¤ â„“ â‰¤ L):
$$
a^{(\ell)} = W^{(\ell)} z^{(\ell-1)} \\
z^{(\ell)} = \sigma(a^{(\ell)})
$$

- ì¶œë ¥ì¸µ (í™œì„±í™” X):
$$ f(\mathbf{x}) = a^{(L)} $$

---

### ğŸ¯ í™œì„±í™” í•¨ìˆ˜ (Activation Functions)

- **Sigmoid**:
$$ \sigma(t) = \frac{1}{1 + e^{-t}} $$
â†’ ë²”ìœ„: (0, 1), gradient vanishing ë¬¸ì œ ì¡´ì¬

- **Tanh**:
$$ \sigma(t) = \tanh(t) = \frac{e^t - e^{-t}}{e^t + e^{-t}} $$
â†’ ë²”ìœ„: (âˆ’1, 1), zero-centered

- **ReLU**:
$$ \sigma(t) = \max(0, t) $$
â†’ ë²”ìœ„: [0, âˆ), ì—°ì‚° ë¹ ë¥´ê³  gradient vanishing ë¬¸ì œ ì™„í™”

---

### ğŸ¯ MLPì˜ ëª©ì  í•¨ìˆ˜

#### â–ªï¸ Regression (Mean Squared Error):
- Scalar output:
$$ R(\theta) = \sum_{n=1}^N (y_n - f(x_n))^2 $$
- Vector output:
$$ R(\theta) = \sum_{n=1}^N \| y_n - f(x_n) \|^2 $$

#### â–ªï¸ Classification (Cross Entropy):
$$ R(\theta) = - \sum_{n=1}^N \sum_{k=1}^K y_{nk} \log \psi_k(f(x_n)) $$

- Softmax:
$$ \psi_k(\mathbf{v}) = \frac{e^{v_k}}{\sum_{j=1}^K e^{v_j}} $$

---

### ğŸ¯ Backpropagation í•µì‹¬ ìˆ˜ì‹

- ì—ëŸ¬ ì‹œê·¸ë„:
$$ \delta_{ni}^{(\ell)} = \frac{\partial R_n}{\partial a_{ni}^{(\ell)}} $$

- weight gradient:
$$ \frac{\partial R_n}{\partial w_{ij}^{(\ell)}} = \delta_{ni}^{(\ell)} z_{nj}^{(\ell-1)} $$

- ì—­ì „íŒŒ (ì€ë‹‰ì¸µ):
$$ \delta_{ni}^{(\ell)} = \sigma'(a_{ni}^{(\ell)}) \sum_k w_{ki}^{(\ell+1)} \delta_{nk}^{(\ell+1)} $$

---

### ğŸ¯ ì¶œë ¥ì¸µ ì—ëŸ¬ (ì˜ˆ: Regression)
$$ \delta_{n1}^{(L)} = \frac{\partial R_n}{\partial a_{n1}^{(L)}} = -2(y_n - a_{n1}^{(L)}) $$

---

## âœ… Practical Tips

1. **ReLU ì‚¬ìš©** â†’ ë¹ ë¥´ê³  gradient ë¬¸ì œ ì™„í™”  
2. **ë°ì´í„° í‘œì¤€í™”** (sigmoid ì‚¬ìš© ì‹œ íŠ¹íˆ ì¤‘ìš”)  
3. **ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”**: ì‘ì€ ëœë¤ê°’ (0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ë©´ í•™ìŠµ ë¶ˆê°€)

---

## âœ… ì •ë¦¬ í¬ì¸íŠ¸

- MLPëŠ” ì¸µë³„ë¡œ $$ a^{(\ell)} = W^{(\ell)} z^{(\ell-1)} $$ ê³„ì‚° í›„ ë¹„ì„ í˜• $$ \sigma $$ ì ìš©
- ì¶œë ¥ì¸µì€ í™œì„±í™” ì—†ì´ $$ f(x) = a^{(L)} $$
- ëª©ì  í•¨ìˆ˜ëŠ” Regression â†’ MSE, Classification â†’ Cross Entropy
- Backpropagationì„ í†µí•´ ê° weightì˜ gradient ê³„ì‚°
- ì‹¤ì „ì—ì„œëŠ” **ReLU + ì •ê·œí™” + ì´ˆê¸°í™”**ê°€ ì¤‘ìš”

---

# Part 2. Reinforcement Learning (RL)

## âœ… ì£¼ìš” ê°œë… íë¦„
Reinforcement Learning(RL)ì€ ì—ì´ì „íŠ¸ê°€ **í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©**í•˜ë©° **ë³´ìƒ ëˆ„ì  ìµœëŒ€í™”**ë¥¼ í•™ìŠµí•˜ëŠ” í”„ë ˆì„ì›Œí¬ë‹¤.  
í™˜ê²½ì€ ë³´ìƒì„ í†µí•´ ëª©í‘œë¥¼ ì „ë‹¬í•˜ê³ , ì—ì´ì „íŠ¸ëŠ” ìƒíƒœì— ë”°ë¼ í–‰ë™ì„ ì„ íƒí•˜ëŠ” **ì •ì±…(policy)**ì„ í•™ìŠµí•œë‹¤.  
MDP ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ, ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ìµœì  ì •ì±…ì„ ì°¾ëŠ”ë‹¤ (Value-based, Policy-based ë“±).

---

## âœ… ê°œë…ê³¼ ìˆ˜ì‹ ì •ë¦¬

### ğŸ¯ MDP (Markov Decision Process)

- ìƒíƒœ: $$ s \in \mathcal{S} $$
- í–‰ë™: $$ a \in \mathcal{A} $$
- ë³´ìƒ: $$ r \in \mathbb{R} $$
- ì „ì´í™•ë¥ : $$ P(s' \mid s, a) $$
- ê°ê°€ìœ¨: $$ \gamma \in [0, 1) $$

---

### ğŸ¯ ì •ì±… (Policy)

- í™•ë¥ ì  ì •ì±…:
$$ \pi(a \mid s) = P(a_t = a \mid s_t = s) $$

- ê²°ì •ì  ì •ì±…:
$$ a = \pi(s) $$

---

### ğŸ¯ Return (ëˆ„ì  ë³´ìƒ)

- **Episodic**:
$$ R_t = r_{t+1} + r_{t+2} + \dots + r_T $$

- **Continuing**:
$$ R_t = \sum_{k=0}^\infty \gamma^k r_{t+k+1} $$

---

### ğŸ¯ Value Functions

- **State Value**:
$$ V^\pi(s) = \mathbb{E}_\pi[R_t \mid s_t = s] $$

- **Action Value**:
$$ Q^\pi(s, a) = \mathbb{E}_\pi[R_t \mid s_t = s, a_t = a] $$

- **Advantage Function**:
$$ A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s) $$

---

### ğŸ¯ Bellman Equations

- **Expectation Equation (V)**:
$$
V^\pi(s) = \sum_a \pi(a \mid s) \sum_{s'} P(s' \mid s, a) [R(s, a, s') + \gamma V^\pi(s')]
$$

- **Expectation Equation (Q)**:
$$
Q^\pi(s, a) = \sum_{s'} P(s' \mid s, a) [R(s, a, s') + \gamma \sum_{a'} \pi(a' \mid s') Q^\pi(s', a')]
$$

---

### ğŸ¯ Optimality Equations

- **Optimal V\* (nonlinear)**:
$$
V^*(s) = \max_a \sum_{s'} P(s' \mid s, a)[R(s, a, s') + \gamma V^*(s')]
$$

- **Optimal Q\* (nonlinear)**:
$$
Q^*(s, a) = \sum_{s'} P(s' \mid s, a)[R(s, a, s') + \gamma \max_{a'} Q^*(s', a')]
$$

---

### ğŸ¯ Policy Iteration

1. Policy Evaluation: $$ V^\pi $$ ê³„ì‚°  
2. Policy Improvement: $$ \pi(s) \leftarrow \arg\max_a Q^\pi(s, a) $$  
â†’ ë°˜ë³µí•˜ì—¬ $$ \pi \to \pi^* $$

---

### ğŸ¯ Value Iteration

$$
V_{k+1}(s) = \max_a \sum_{s'} P(s' \mid s, a)[R(s, a, s') + \gamma V_k(s')]
$$

â†’ ë°˜ë³µ í›„ $$ V^* $$ ìˆ˜ë ´

---

### ğŸ¯ Model-Free Learning (í™˜ê²½ ëª¨ë¸ ì—†ì´ í•™ìŠµ)

#### â–ªï¸ Monte Carlo (MC):
$$ V(s) \leftarrow \text{average of returns } R_t $$  
â†’ ì—í”¼ì†Œë“œ ì¢…ë£Œ í›„ ì—…ë°ì´íŠ¸

#### â–ªï¸ TD(0):
$$ V(s_t) \leftarrow V(s_t) + \eta [r_{t+1} + \gamma V(s_{t+1}) - V(s_t)] $$  
â†’ ì˜¨ë¼ì¸/ë¶€íŠ¸ìŠ¤íŠ¸ë© ë°©ì‹

---

### ğŸ¯ Control ì•Œê³ ë¦¬ì¦˜

#### â–ªï¸ SARSA (On-policy):
$$ Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \eta [r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)] $$

#### â–ªï¸ Q-Learning (Off-policy):
$$ Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \eta [r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)] $$

---

### ğŸ¯ Policy Gradient

- ëª©í‘œ í•¨ìˆ˜:
$$ J(\theta) = \mathbb{E}_{\pi_\theta} [R(\tau)] $$

- ê¸°ë³¸ gradient:
$$
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \sum_t \nabla_\theta \log \pi_\theta(a_t \mid s_t) R_t \right]
$$

- Baseline ì ìš©:
$$
\nabla_\theta J(\theta) = \mathbb{E} \left[ \sum_t \nabla_\theta \log \pi_\theta(a_t \mid s_t)(R_t - b(s_t)) \right]
$$

- Advantage ê¸°ë°˜:
$$
\nabla_\theta J(\theta) \approx \mathbb{E} [\nabla_\theta \log \pi_\theta(a_t \mid s_t) A^\pi(s_t, a_t)]
$$

---

## âœ… ì •ë¦¬ í¬ì¸íŠ¸

- RLì€ ìƒíƒœ-í–‰ë™-ë³´ìƒ ìˆœí™˜ êµ¬ì¡°ë¡œ, ëˆ„ì  ë³´ìƒì„ ìµœëŒ€í™”í•˜ëŠ” ì •ì±…ì„ í•™ìŠµí•¨
- Bellman ì‹ì€ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ì •ì˜
- Value Iteration / Policy Iteration â†’ ëª¨ë¸ ê¸°ë°˜ ë°©ì‹
- MC, TD, SARSA, Q-Learning â†’ ëª¨ë¸ ì—†ì´ í•™ìŠµí•˜ëŠ” ë°©ì‹
- Policy GradientëŠ” íŒŒë¼ë¯¸í„°í™”ëœ ì •ì±…ì„ ì§ì ‘ ìµœì í™”