---
title: "Taylor"
excerpt: "Chatgpt 참조"

categories:
  - MachineLearning
tags:
  - [Machine Learning, 머신러닝, Taylor]

permalink: /categories/MachineLearning/Taylor

toc: true
toc_sticky: true

date: 2025-03-24
last_modified_at: 2025-03-24
---

테일러 시리즈 완전 처음부터 배우기

⸻

✅ 1. 왜 테일러 시리즈가 필요할까?

어려운 함수를 간단한 함수(다항식)으로 근사해서 계산하기 위해서야!

예를 들어:
	•	\sin(x), e^x, \log(x) 같은 함수는
→ 복잡해서 계산하기 어려워
	•	하지만 “직선”, “이차 함수” 같은 건 계산이 쉽지?
	•	그래서:
“어떤 점 근처에서 복잡한 함수를 간단한 다항식으로 바꿔서 쓰자!”
→ 그 방법이 바로 테일러 시리즈(Taylor Series) 야!

⸻

✅ 2. 기본 아이디어 요약

함수 f(x) 가 어떤 점 a 근처에서
선형(linear), 2차, 3차… 다항식으로 근사될 수 있다는 뜻이야.

그게 바로:

f(x) \approx f(a) + f{\prime}(a)(x - a) + \frac{f{\prime}{\prime}(a)}{2!}(x - a)^2 + \cdots

이게 바로 테일러 급수(Taylor Series) 의 핵심 형태야.

⸻

✅ 3. 단계별로 쌓아가기

⸻

🔹 Step 1. 0차 근사 (상수 함수)

f(x) \approx f(a)

→ “함수값이 그냥 a에서의 값과 같다”는 가장 단순한 근사

⸻

🔹 Step 2. 1차 근사 (직선)

f(x) \approx f(a) + f{\prime}(a)(x - a)

→ x = a 근처에서는 함수가 직선처럼 보일 수 있어
→ 이게 바로 접선(tangent line) 이고,
→ 우리가 흔히 아는 “선형 근사(linear approximation)”

⸻

🔹 Step 3. 2차 근사 (곡선 포함)

f(x) \approx f(a) + f{\prime}(a)(x - a) + \frac{f{\prime}{\prime}(a)}{2!}(x - a)^2

→ 이건 이제 함수의 곡률(휘어진 정도) 까지도 반영한 거야
→ 더 정밀한 근사

⸻

🔹 Step 4. 일반화 (n차까지 근사)

f(x) \approx \sum_{k=0}^{n} \frac{f^{(k)}(a)}{k!} (x - a)^k
	•	이걸 n차 테일러 다항식이라고 해
	•	f^{(k)}(a): k번째 도함수
	•	k!: 팩토리얼

⸻

🔹 Step 5. 무한히 더하면?

f(x) = \sum_{k=0}^{\infty} \frac{f^{(k)}(a)}{k!} (x - a)^k

→ 이게 바로 테일러 시리즈 (Taylor Series)
→ 무한한 다항식으로 함수 전체를 표현하려는 시도야

⸻

🧪 예제 1: e^x 의 테일러 시리즈 (중심 a = 0)

e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots

→ 이건 맥클로린 급수(Maclaurin Series) 라고도 불러
(※ a = 0일 때의 테일러 시리즈)

⸻