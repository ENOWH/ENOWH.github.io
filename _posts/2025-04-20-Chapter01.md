---
title: "Chapter01"
excerpt: "머신러닝 Chap01 전체 흐름"

categories:
  - MachineLearning
tags:
  - [Machine Learning, 머신러닝]

permalink: /categories/MachineLearning/Chapter01_MachineLearning

toc: true
toc_sticky: true

date: 2025-04-20
last_modified_at: 2025-04-20
---

## 📘 Session 1: Course Introduction (슬라이드 1–15)

### ✅ 1. 주요 개념 정리

| 개념 이름 | 설명 |
|-----------|------|
| **Machine Learning** | 데이터를 통해 정량적인 추론 및 예측을 수행하는 학문 |
| **Feature (특징)** | 관찰된 현상의 수치적 속성 |
| **Feature Vector** | 여러 특징을 담은 벡터. 일반적으로 \( \mathbf{x} \in \mathbb{R}^d \) |
| **Supervised Learning** | 입력과 정답(label)이 있는 데이터로부터 모델을 학습 |
| **Unsupervised Learning** | 레이블 없이 데이터의 구조를 파악 |
| **Weakly Supervised Learning** | 일부 레이블만 있거나, 노이즈가 있는 경우 등 |
| **Generative vs Discriminative Models** | 데이터의 전체 분포를 모델링(G), 결정 경계를 모델링(D) |
| **Parametric vs Nonparametric Models** | 모델의 복잡도와 유연성에 따른 분류 |
| **Linear vs Nonlinear Models** | 출력이 데이터에 대해 선형인지 여부 |
| **Constraints** | 실제 문제에서 고려해야 할 제약들 (예: Privacy, Fairness 등) |

---

### 🧠 2. 개념 ↔ 수식 정리

| 개념 이름 | 수식 (LaTeX) |
|-----------|--------------|
| **Feature Vector** | \( \mathbf{x} = \begin{bmatrix} x_1 \\ \vdots \\ x_d \end{bmatrix} \in \mathbb{R}^d \) |
| **Supervised Learning (분류)** | 학습 데이터: \( (\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n) \),<br>where \( y_i \in \{1, \dots, C\} \) |
| **Supervised Learning (회귀)** | \( y_i \in \mathbb{R} \) |
| **Generative Model** | \( p(\mathbf{x}, y) = p(y)p(\mathbf{x} \mid y) \) |
| **Discriminative Model** | \( p(y \mid \mathbf{x}) \) 또는 결정 함수 \( f(\mathbf{x}) \) 직접 모델링 |

---

### 🔁 3. 개념 흐름 설명

이 세션은 머신러닝의 전반적인 개요를 제공하며, 다음과 같은 흐름으로 전개됩니다:

1. **머신러닝 정의**  
   - 데이터로부터 예측과 추론을 수행하는 학문  
   - 통계, 인공지능, 신호처리 등에서 유래하여 독립적인 분야로 발전

2. **기본 구성요소 도입**  
   - Feature: 데이터의 특성  
   - Feature Vector: 여러 특성의 집합, \( \mathbf{x} \in \mathbb{R}^d \)  
   - 다양한 동의어: examples, instances, data points 등

3. **학습 유형 소개**  
   - **Supervised Learning**: 레이블 존재, 분류 & 회귀  
   - **Unsupervised Learning**: 레이블 없음, 클러스터링/차원축소/밀도추정  
   - **Weakly Supervised Learning**: 반지도학습, 노이즈 있는 레이블 등

4. **모델 분류 기준 제시**  
   - **Generative vs Discriminative**: 전체 분포 모델링 vs 결정 경계 모델링  
   - **Parametric vs Nonparametric**: 고정된 매개변수 vs 데이터 크기에 따라 유연  
   - **Linear vs Nonlinear**: 출력이 데이터의 선형결합인지 여부  
   - **Constraints**: 실제 환경에서 고려해야 할 요소들 (예: 연산 자원, 공정성, 프라이버시 등)


## 📘 Session 2: Unconstrained Optimization (슬라이드 16–45)

### ✅ 1. 주요 개념 정리

| 개념 이름 | 설명 |
|-----------|------|
| **Unconstrained Optimization** | 제약 조건 없이 함수 \( f(x) \) 를 최소화하는 문제 |
| **Objective Function** | 최적화 대상 함수 \( f: \mathbb{R}^d \to \mathbb{R} \) |
| **Local Minimizer** | 어떤 점 \( x^* \) 주변에서 가장 작은 값을 가지는 점 |
| **Global Minimizer** | 전체 영역에서 가장 작은 값을 가지는 점 |
| **Gradient** | 함수의 기울기 벡터 \( \nabla f(x) \) |
| **Hessian** | 함수의 이차 도함수 행렬 \( \nabla^2 f(x) \) |
| **First-Order Necessary Condition** | 최소점에서의 그래디언트가 0이어야 함 |
| **Second-Order Necessary Condition** | 최소점에서의 해시안이 양의 준정정부호 |
| **Convex Function** | 직선이 그래프 아래에 있는 형태. 지역 최소점 = 전역 최소점 |
| **Strictly Convex Function** | 하나의 전역 최소점만 존재 |
| **First-Order Characterization** | 기울기를 통해 볼 때 볼록성 확인 가능 |
| **Second-Order Characterization** | 해시안의 성질로 볼록성 판별 가능 |

---

### 🧠 2. 개념 ↔ 수식 정리

| 개념 이름 | 수식 (LaTeX) |
|-----------|--------------|
| **최적화 문제 정의** | \( \min_{x \in \mathbb{R}^d} f(x) \) |
| **Gradient** | \( \nabla f(x) = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \vdots \\ \frac{\partial f}{\partial x_d} \end{bmatrix} \) |
| **Hessian** | \( \nabla^2 f(x) = \left[ \frac{\partial^2 f}{\partial x_i \partial x_j} \right]_{i,j=1}^d \) |
| **1차 필요조건** | \( \nabla f(x^*) = 0 \) |
| **2차 필요조건** | \( \nabla^2 f(x^*) \succeq 0 \) (양의 준정정부호) |
| **볼록함수 정의** | \( f(tx + (1 - t)y) \leq tf(x) + (1 - t)f(y) \) |
| **Strictly convex** | \( f(tx + (1 - t)y) < tf(x) + (1 - t)f(y) \), for \( x \ne y \) |
| **1차 볼록성 조건** | \( f(y) \geq f(x) + \langle \nabla f(x), y - x \rangle \) |
| **2차 볼록성 조건** | \( f \) convex \( \Leftrightarrow \nabla^2 f(x) \succeq 0 \) |
| **최적성 조건 (Convex)** | \( \nabla f(x^*) = 0 \Rightarrow x^* \) is global minimizer |

---

### 🔁 3. 개념 흐름 설명

이 세션에서는 **제약 없는 최적화 문제**에 대해 다룹니다. 흐름은 다음과 같아요:

1. **문제 정의**  
   - \( \min_{x \in \mathbb{R}^d} f(x) \) 형태의 최적화 문제 설정  
   - 로컬/글로벌 최소점의 정의

2. **미분을 통한 최적성 조건 도출**  
   - **1차 조건**: \( \nabla f(x^*) = 0 \)  
   - **2차 조건**: \( \nabla^2 f(x^*) \succeq 0 \)

3. **Taylor 전개로 증명 제공**  
   - 방향 도함수, 체인룰, 테일러 전개를 이용한 수학적 유도

4. **볼록 함수의 정의와 성질**  
   - 볼록성 ⇒ 로컬 미니멈이 전역 미니멈  
   - Strict convex ⇒ 전역 최소점 유일

5. **볼록성의 1차, 2차 특성**  
   - 1차 특성: 접선은 항상 그래프 아래  
   - 2차 특성: 해시안이 양의 준정정부호

6. **머신러닝에서의 활용 예시**  
   - 선형 회귀, 로지스틱 회귀, 서포트 벡터 머신, Lasso 회귀 등 모두 convex 최적화 문제로 귀결

---

### 🧪 머신러닝에서 자주 등장하는 볼록 최적화 예시

| 문제 | 목적 함수 \( f(w) \) |
|------|----------------------|
| **Linear Regression** | \( \|Xw - y\|^2 \) |
| **Logistic Regression** | \( \sum_{i=1}^n \log(1 + e^{-y_i w^T x_i}) \) |
| **SVM** | \( \frac{1}{2}\|w\|^2 + C \sum_{i=1}^n \max(0, 1 - y_i w^T x_i) \) |
| **Lasso** | \( \|Xw - y\|^2 + \lambda \|w\|_1 \) |