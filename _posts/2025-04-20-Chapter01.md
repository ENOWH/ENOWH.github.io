---
title: "Chapter01"
excerpt: "머신러닝 Chap01 전체 흐름"

categories:
  - MachineLearning
tags:
  - [Machine Learning, 머신러닝]

permalink: /categories/MachineLearning/Chapter01_MachineLearning

toc: true
toc_sticky: true

date: 2025-04-20
last_modified_at: 2025-04-20
---

# 📘 Session 1: Course Introduction

## 🌱 개념 흐름 요약
- 머신러닝은 **데이터로부터 예측이나 추론을 수행**하는 학문이다.
- 문제 유형은 크게 **Supervised / Unsupervised / Weakly Supervised Learning** 으로 나뉜다.
- 머신러닝 모델은 여러 기준에 따라 나뉜다:  
  (1) 데이터 분포를 모델링하는 방식 (Generative vs. Discriminative)  
  (2) 모델의 복잡도 (Parametric vs. Nonparametric)  
  (3) 입력과 출력의 관계 (Linear vs. Nonlinear)
- 실제 문제에서는 **Scalability, Privacy, Fairness, Safety** 같은 제약조건들도 중요하다.

---

## 🔑 개념별 정리 (이름 ↔ 수식 연결 중심)

### ✅ Feature & Feature Vector

- **Feature (특징)**: 관측 대상의 수치적 특성 (예: 키, 몸무게, 점수)
- **Feature Vector (특징 벡터)**: 여러 feature들을 모은 벡터

📐 수식:
$$
\mathbf{x} = \begin{bmatrix} x_1 \\ \vdots \\ x_d \end{bmatrix}, \quad \mathbf{x} \in \mathbb{R}^d
$$

💬 다른 표현들: attributes, predictors, covariates, inputs

---

### ✅ Supervised Learning (지도 학습)

- **정의**: 입력 $\mathbf{x}$에 대해 정답 $y$가 주어진 상태에서 학습
- **목적**: **예측 (Prediction)**

🧪 수식 (학습 데이터):
$$
\{(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)\}
$$

🧩 세부 유형:
- **Classification**: $y$가 이산값 (예: 0~9 숫자 분류)
- **Regression**: $y$가 연속값 (예: 집 값 예측)

---

### ✅ Unsupervised Learning (비지도 학습)

- **정의**: 정답(label)이 없는 데이터에서 패턴을 찾는 학습
- **목적**: **추론 (Inference)**

예시 유형:
- Clustering (군집화)
- Dimensionality Reduction (차원 축소)
- Density Estimation (확률밀도 추정)

❌ 출력 변수 $y$가 없기 때문에 수식은 주로 $\mathbf{x}$의 구조에 초점

---

### ✅ Weakly Supervised Learning (약지도 학습)

- **정의**: 라벨이 부족하거나 부정확한 데이터를 활용하는 학습
- 예시:
  - Semi-supervised Learning: 일부만 라벨 있음
  - Noisy Labels: 라벨에 오류 있음

---

### ✅ Generative vs. Discriminative

| 개념 | 설명 | 관련 수식 |
|------|------|-----------|
| **Generative Model** | 입력과 출력을 **공동 분포**로 모델링 | $$ p(\mathbf{x}, y) = p(y) \cdot p(\mathbf{x} \mid y) $$ |
| **Discriminative Model** | **조건부 분포**만 모델링하여 직접 예측 | $$ p(y \mid \mathbf{x}) $$ |

🧠 Generative는 데이터를 생성할 수 있고, Discriminative는 예측에 최적화됨.

---

### ✅ Parametric vs. Nonparametric

- **Parametric**: 모델 파라미터 수가 고정 (예: 선형 회귀)
- **Nonparametric**: 데이터가 많아질수록 파라미터 수 증가 (예: k-NN)

---

### ✅ Linear vs. Nonlinear

- **Linear Model**: 출력이 입력의 **선형 함수**  
  $$ f(\mathbf{x}) = \mathbf{w}^\top \mathbf{x} + b $$
- **Nonlinear Model**: 위 형태를 따르지 않음 (예: 신경망)

---

### ✅ 현실 제약 조건 (Constraints)

| 제약 조건 | 설명 |
|-----------|------|
| **Scalability** | 큰 데이터셋에서도 빠르게 작동 |
| **Privacy** | 개인 정보 보호 |
| **Fairness** | 특정 집단에 불공정하지 않게 |
| **Safety** | 잘못된 판단을 하지 않도록 안전하게 설계 |


# 🧮 Session 2: Unconstrained Optimization

## 🌱 개념 흐름 요약
- 머신러닝에서 많은 문제는 어떤 **목적 함수 f(x)** 를 최소화하는 형태로 표현된다.
- 이때 제약조건이 없는 경우 → **Unconstrained Optimization**
- 최적화 문제에서 해가 최적인지를 판단하려면 **1차, 2차 조건 (First/Second Order Conditions)** 을 사용한다.
- 특히 **Convexity (함수의 볼록성)** 는 최적화에서 해가 전역해인지 판단하는 데 매우 중요하다.

---

## 🔑 개념별 정리 (이름 ↔ 수식 연결)

### ✅ Unconstrained Optimization Problem

- **정의**: 제약조건 없이 목적함수 $f(x)$ 를 최소화하는 문제

📐 수식:
$$
\min_{\mathbf{x} \in \mathbb{R}^d} f(\mathbf{x})
$$

- **Local Minimum**: 어떤 반경 $r > 0$ 내에서 가장 작은 값
- **Global Minimum**: 전체 영역에서 가장 작은 값

---

### ✅ Gradient & Hessian (미분 도구)

- **Gradient** (1차 미분 벡터):
  $$
  \nabla f(\mathbf{x}) =
  \begin{bmatrix}
    \frac{\partial f}{\partial x_1} \\
    \vdots \\
    \frac{\partial f}{\partial x_d}
  \end{bmatrix}
  $$

- **Hessian** (2차 미분 행렬):
  $$
  \nabla^2 f(\mathbf{x}) =
  \begin{bmatrix}
    \frac{\partial^2 f}{\partial x_1^2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_d} \\
    \vdots & \ddots & \vdots \\
    \frac{\partial^2 f}{\partial x_d \partial x_1} & \cdots & \frac{\partial^2 f}{\partial x_d^2}
  \end{bmatrix}
  $$

---

### ✅ First-order Necessary Condition (1차 최적조건)

- **개념**: 어떤 점 $\mathbf{x}^*$ 이 local minimum 이려면,
  $$
  \nabla f(\mathbf{x}^*) = 0
  $$

- 직관: 기울기(gradient)가 0이면 더 내려갈 방향이 없음

---

### ✅ Second-order Necessary Condition (2차 최적조건)

- **개념**: $\nabla f(\mathbf{x}^*) = 0$ 이고, 그 점에서 Hessian이 **positive semi-definite** 해야 함
  $$
  \nabla^2 f(\mathbf{x}^*) \succeq 0
  $$

- 의미: 함수 곡률이 위로 볼록해야 local minimum일 수 있음

---

### ✅ Convexity (볼록성)

- **Convex Function**: 직선이 항상 함수 위에 있을 때
  $$
  f(tx + (1 - t)y) \leq t f(x) + (1 - t)f(y), \quad \forall x, y, t \in [0,1]
  $$

- **Strictly Convex Function**: 위 식이 **엄격한 부등식**일 때

---

### ✅ Property: Convexity → Local min = Global min

- **정리**: Convex 함수에서는 local min이면 곧 global min이다.

---

### ✅ First-Order Characterization of Convexity

- **정의**: 함수 $f$ 가 미분 가능할 때, convex 하려면
  $$
  f(y) \geq f(x) + \langle \nabla f(x), y - x \rangle
  $$

- 이 식은 직선(접선)이 항상 함수 아래에 있다는 의미

---

### ✅ Second-Order Characterization of Convexity

- **조건**:
  - $f$ 가 $C^2$ 함수 (2차 연속 미분 가능)
  - $\nabla^2 f(x) \succeq 0$ (Hessian이 positive semi-definite)

- **Strictly convex** 하려면:
  $$
  \nabla^2 f(x) \succ 0 \quad \forall x
  $$

---

### ✅ Convexity가 중요한 이유

- **Convex한 문제**는:
  - 전역 최소값이 존재하고,
  - gradient descent 같은 단순한 방법으로도 해를 찾을 수 있음
  - 알고리즘이 빠르고 안정적

---

### ✅ 머신러닝에서 자주 나오는 Convex 함수 예시

| 모델 | 목적 함수 |
|------|-----------|
| **Linear Regression** | $$ f(\mathbf{w}) = \|X\mathbf{w} - y\|^2 $$ |
| **Logistic Regression** | $$ f(\mathbf{w}) = \sum_{i=1}^n \log(1 + e^{-y_i \mathbf{w}^\top \mathbf{x}_i}) $$ |
| **SVM (Hinge Loss)** | $$ f(\mathbf{w}) = \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^n \max(0, 1 - y_i \mathbf{w}^\top \mathbf{x}_i) $$ |
| **Lasso** | $$ f(\mathbf{w}) = \|X\mathbf{w} - y\|^2 + \lambda \|\mathbf{w}\|_1 $$ |

---


# 🔁 Session 3: Iterative Algorithms for Continuous Optimization

## 🌱 개념 흐름 요약
- 앞에서 정의한 최적화 문제  
  $$ \min_{\boldsymbol{\theta} \in \mathbb{R}^k} J(\boldsymbol{\theta}) $$
  를 실제로 어떻게 풀까?

- 목적 함수 $J$가 연속이거나 미분 가능한 경우, **반복적 알고리즘**을 통해 해를 찾는다.
- 대표적인 방법들:
  - Gradient Descent (1차 정보)
  - Subgradient Method (비미분 함수 포함)
  - Stochastic Gradient Descent (데이터가 많을 때)
  - Newton’s Method (2차 정보 포함)
  - Majorize-Minimize (비미분/복잡한 함수에도 가능)

---

## 🔑 개념별 정리 (이름 ↔ 수식 연결 중심)

### ✅ Directional Derivative (방향 도함수)

- **정의**: 어떤 방향 $\mathbf{u}$로 $J(\boldsymbol{\theta})$ 가 얼마나 증가/감소하는지

📐 수식:
$$
D_{\mathbf{u}} J(\boldsymbol{\theta}_0) = \lim_{t \to 0} \frac{J(\boldsymbol{\theta}_0 + t \mathbf{u}) - J(\boldsymbol{\theta}_0)}{t}
= \langle \nabla J(\boldsymbol{\theta}_0), \mathbf{u} \rangle
$$

- **해석**: gradient와 $\mathbf{u}$ 사이의 내적값

---

### ✅ Gradient Descent (경사하강법)

- **핵심 아이디어**: 가장 빠르게 함수값이 **줄어드는 방향**으로 이동
- 방향: $-\nabla J(\boldsymbol{\theta})$
- 반복적 업데이트:

📐 수식:
$$
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \nabla J(\boldsymbol{\theta}_t)
$$

- $\eta$: **학습률 (step size)**

💡 조건: $J$가 미분 가능해야 함

---

### ✅ Subgradient Method (부분 경사법)

- **문제**: $J$가 **비미분**일 수 있다 (예: hinge loss, L1 norm)
- **대안**: gradient 대신 **subgradient** 사용

📐 수식 (업데이트):
$$
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \cdot \mathbf{u}_t \quad \text{where } \mathbf{u}_t \in \partial J(\boldsymbol{\theta}_t)
$$

- $\partial J(\boldsymbol{\theta})$: 해당 지점의 **subdifferential 집합**

💬 subgradient는 일반 gradient처럼 한 방향이 아닌 **여러 가능성의 집합**

---

### ✅ Stochastic Gradient Descent (SGD)

- **문제**: $J(\boldsymbol{\theta}) = \sum_{i=1}^n J_i(\boldsymbol{\theta})$ 와 같이 데이터가 클 경우
- **해결책**: 매 반복마다 전체가 아닌 **일부 데이터로 업데이트**

📐 수식:
$$
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \nabla J_i(\boldsymbol{\theta}_t)
$$

- 또는 mini-batch로:
$$
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \cdot \frac{1}{b} \sum_{i \in \Omega_t} \nabla J_i(\boldsymbol{\theta}_t)
$$

- $\Omega_t$: 크기 $b$의 무작위 샘플링된 데이터 인덱스

---

### ✅ Newton's Method (뉴턴 방법)

- **Gradient Descent**는 1차 미분 정보만 사용 → 느릴 수 있음
- Newton's method는 **Hessian (2차 미분)**을 사용하여 더 빠르게 수렴

📐 수식:
$$
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \left( \nabla^2 J(\boldsymbol{\theta}_t) \right)^{-1} \nabla J(\boldsymbol{\theta}_t)
$$

- 빠르게 수렴하지만 **Hessian 역행렬 계산이 비쌈**
- 고차원에서는 **비효율적**

---

### ✅ Majorize-Minimize (MM) Algorithm

- **아이디어**: 복잡한 함수 $J$를 더 간단한 함수 $J_t$로 **upper bound**해서 매번 최소화

#### 알고리즘 구조:

1. Majorize step:
   - $J(\boldsymbol{\theta}) \leq J_t(\boldsymbol{\theta})$
   - $J(\boldsymbol{\theta}_t) = J_t(\boldsymbol{\theta}_t)$

2. Minimize step:
   - $$
     \boldsymbol{\theta}_{t+1} = \arg\min_{\boldsymbol{\theta}} J_t(\boldsymbol{\theta})
     $$

💬 대표 예시: EM algorithm, IRLS, robust regression 등

---

### ✅ Descent Property (하강 보장)

- Gradient Descent:  
  $$
  J(\boldsymbol{\theta}_{t+1}) \leq J(\boldsymbol{\theta}_t)
  $$
  단, $\eta$ 작고 $\nabla J \neq 0$ 일 때

- MM Algorithm:
  $$
  J(\boldsymbol{\theta}_{t+1}) \leq J_t(\boldsymbol{\theta}_{t+1}) \leq J_t(\boldsymbol{\theta}_t) = J(\boldsymbol{\theta}_t)
  $$

---

## ✅ 각 알고리즘 요약 비교

| 알고리즘 | 조건 | 특징 | 수식 |
|----------|------|------|------|
| Gradient Descent | $J$ 미분 가능 | 단순, 안정적 | $$ \boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \nabla J(\boldsymbol{\theta}_t) $$ |
| Subgradient | $J$ convex | 비미분 함수 대응 | $$ \boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \cdot \mathbf{u}_t $$ |
| SGD | $J = \sum J_i$ | 빠르고 효율적 | $$ \boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \nabla J_i(\boldsymbol{\theta}_t) $$ |
| Newton | $J$ twice diff. | 빠른 수렴, 비용 높음 | $$ \boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - (\nabla^2 J)^{-1} \nabla J $$ |
| MM | 일반 함수 가능 | gradient 없이도 최적화 가능 | $$ \boldsymbol{\theta}_{t+1} = \arg\min J_t(\boldsymbol{\theta}) $$ |



# 👣 Session 4: Nearest Neighbor Classifier

## 🌱 개념 흐름 요약
- Nearest Neighbor는 **학습(training)** 없이, **거리 기반으로 예측**하는 방법이다.
- 핵심 아이디어: **입력 벡터 $\mathbf{x}$와 가장 가까운 훈련 데이터의 레이블을 따라간다.**
- 일반화된 버전인 **k-NN**은 가장 가까운 k개의 이웃의 **다수결 투표**로 결정한다.
- k 값에 따라 **bias-variance trade-off**가 발생한다.
- 성능 측정은 **test error**를 통해 하고, 최적의 k는 **validation set**으로 조정한다.

---

## 🔑 개념별 정리 (이름 ↔ 수식 연결)

### ✅ Nearest Neighbor Classifier

- **문제 세팅**:  
  학습 데이터 $ \{(\mathbf{x}_1, y_1), \dots, (\mathbf{x}_n, y_n)\} $  
  - $\mathbf{x}_i \in \mathbb{R}^d$ (입력 벡터)
  - $y_i \in \{-1, +1\}$ (이진 분류 예시)

- **예측 방법**: 입력 $\mathbf{x}$에 대해 가장 가까운 $\mathbf{x}_i$ 를 찾아, 그에 해당하는 $y_i$ 를 예측으로 사용

📐 수식:
$$
\hat{y} = y_{i^*}, \quad \text{where } i^* = \arg\min_i \|\mathbf{x} - \mathbf{x}_i\|
$$

💡 학습 X, 저장만 한다.

---

### ✅ k-Nearest Neighbors (k-NN)

- **정의**: $\mathbf{x}$와 가까운 **k개의 이웃**의 레이블을 가져와 **다수결**로 결정
- **k는 홀수**가 일반적 (동률 방지)

📐 수식:
$$
\hat{y} = \operatorname{sign} \left( \sum_{j=1}^{k} y_{(j)} \right)
$$

- 여기서 $y_{(j)}$는 $\mathbf{x}$와 가장 가까운 $k$개의 $\mathbf{x}_i$ 중 $j$번째 이웃의 레이블

---

### ✅ Bias-Variance Trade-off in k-NN

| k 값 | Bias | Variance | 특성 |
|------|------|----------|------|
| **작을수록 (예: k=1)** | 낮은 bias | 높은 variance | 복잡한 패턴도 포착, 노이즈 민감 |
| **클수록 (예: k=15)** | 높은 bias | 낮은 variance | 더 안정적, 국소 정보 놓칠 수 있음 |

---

### ✅ Test Error (테스트 오류)

- 실제 테스트 샘플 $m$개에 대해, 예측이 틀린 비율

📐 수식:
$$
\text{Test Error} = \frac{1}{m} \sum_{i=1}^{m} \mathbf{1} \{ f_k(\mathbf{x}_{n+i}) \ne y_{n+i} \}
$$

- $\mathbf{1}\{\cdot\}$: indicator function (참이면 1, 거짓이면 0)

---

### ✅ Training Error vs. Test Error

- **Training Error**:
  - k=1이면 항상 0 (자기 자신이 가장 가까운 이웃이므로)
  - 하지만 **Overfitting** 가능성

- **Test Error**:
  - 일반화 능력 측정
  - **Validation Set**을 이용해 **최적 k** 결정

---

### ✅ 기타 특성 정리

| 항목 | 내용 |
|------|------|
| **학습 과정** | 없음 (Non-parametric) |
| **계산 비용** | 예측 시 거리 계산 필요 → 느릴 수 있음 |
| **모델 저장** | 전체 데이터를 저장해야 함 |
| **장점** | 직관적, 분류 성능 좋음 |
| **단점** | 고차원에서 성능 저하 (curse of dimensionality) |

---