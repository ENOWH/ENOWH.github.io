---
title: "Chapter01"
excerpt: "ë¨¸ì‹ ëŸ¬ë‹ Chap01 ì „ì²´ íë¦„"

categories:
  - MachineLearning
tags:
  - [Machine Learning, ë¨¸ì‹ ëŸ¬ë‹]

permalink: /categories/MachineLearning/Chapter01_MachineLearning

toc: true
toc_sticky: true

date: 2025-04-20
last_modified_at: 2025-04-20
---

# ğŸ“˜ Session 1: Course Introduction (í˜ì´ì§€ 1â€“15)

## ğŸ“Œ ì„¸ì…˜ ê°œìš”

ì´ ì„¸ì…˜ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ ì „ë°˜ì ì¸ ê°œìš”ì™€ ë¬¸ì œ ì •ì˜ë¥¼ ë‹¤ë£¨ë©°,  
- ë¨¸ì‹ ëŸ¬ë‹ì´ë€ ë¬´ì—‡ì¸ê°€  
- ì§€ë„í•™ìŠµì˜ ìˆ˜í•™ì  í‘œí˜„  
- ê²½í—˜ì  ìœ„í—˜ ìµœì†Œí™” (ERM: Empirical Risk Minimization)  
- ê³¼ì í•©, ì¼ë°˜í™”, ëª¨ë¸ ë³µì¡ë„, ì •ê·œí™” ë“±  
ê¸°ë³¸ì ì¸ ê°œë…ë“¤ì„ í¬ê´„ì ìœ¼ë¡œ ì†Œê°œí•œë‹¤.

---

## âœ… í•µì‹¬ ê°œë… ì •ë¦¬

### ğŸ”¹ What is Machine Learning?
- ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ **íŒ¨í„´ì„ í•™ìŠµí•˜ê³  ì˜ˆì¸¡ì„ ìˆ˜í–‰**í•˜ëŠ” ì‹œìŠ¤í…œ
- ê²½í—˜ì„ í†µí•´ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ì•Œê³ ë¦¬ì¦˜

---

### ğŸ”¹ Supervised Learning
- **ì…ë ¥-ì¶œë ¥ ìŒ** $begin:math:text$(x_i, y_i)$end:math:text$ ë¥¼ í†µí•´ í•¨ìˆ˜ $begin:math:text$f: \\mathcal{X} \\to \\mathcal{Y}$end:math:text$ë¥¼ í•™ìŠµ
- ëª©í‘œ: ì£¼ì–´ì§„ $begin:math:text$x$end:math:text$ì— ëŒ€í•´ ì •ë‹µ $begin:math:text$y$end:math:text$ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜ $begin:math:text$f$end:math:text$ë¥¼ ì°¾ëŠ” ê²ƒ

---

### ğŸ”¹ Regression vs. Classification
- **Regression**: ì¶œë ¥ $begin:math:text$ y \\in \\mathbb{R} $end:math:text$ (ì—°ì†ì )
- **Classification**: ì¶œë ¥ $begin:math:text$ y \\in \\{1, \\dots, C\\} $end:math:text$ (ì´ì‚°ì )

---

### ğŸ”¹ Empirical Risk Minimization (ERM)
- **Loss function** $begin:math:text$ \\ell(y, f(x)) $end:math:text$ ì„ ê¸°ë°˜ìœ¼ë¡œ, í‰ê·  ì†ì‹¤ì„ ìµœì†Œí™”í•˜ëŠ” íŒŒë¼ë¯¸í„° $begin:math:text$ \\theta $end:math:text$ë¥¼ ì°¾ìŒ

#### ERM ìˆ˜ì‹:
$$
\hat{R}(\theta) = \frac{1}{n} \sum_{i=1}^n \ell(y_i, f_\theta(x_i))
$$

---

### ğŸ”¹ Overfitting vs. Underfitting
- **Overfitting**: í•™ìŠµ ë°ì´í„°ì—ëŠ” ì˜ ë§ì§€ë§Œ, ìƒˆë¡œìš´ ë°ì´í„°ì— ì¼ë°˜í™”ë˜ì§€ ì•ŠìŒ  
- **Underfitting**: í•™ìŠµ ë°ì´í„°ì—ë„ ì œëŒ€ë¡œ ë§ì§€ ì•ŠìŒ  
- ì¼ë°˜í™” ì˜¤ë¥˜ëŠ” í›ˆë ¨ ì˜¤ë¥˜ì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ

---

### ğŸ”¹ Regularization (ì •ê·œí™”)
- **ëª¨ë¸ ë³µì¡ë„ ì œì–´**ë¥¼ í†µí•´ overfitting ë°©ì§€
- ì˜ˆì‹œ: L2 ì •ê·œí™” (Ridge)

#### Regularized ERM ìˆ˜ì‹:
$$
\hat{R}_{\text{reg}}(\theta) = \frac{1}{n} \sum_{i=1}^n \ell(y_i, f_\theta(x_i)) + \lambda \|\theta\|^2
$$

---

### ğŸ”¹ Bias-Variance Tradeoff
- **Bias**: ëª¨ë¸ì´ ë‹¨ìˆœí• ìˆ˜ë¡ ìƒê¸°ëŠ” ì˜¤ì°¨
- **Variance**: ëª¨ë¸ì´ ë³µì¡í• ìˆ˜ë¡ ë°ì´í„°ì— ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•˜ëŠ” ê²½í–¥
- ì ì ˆí•œ ëª¨ë¸ ë³µì¡ë„ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì´ í•µì‹¬

---

## ğŸ” ì„¸ì…˜ íë¦„ ìš”ì•½

1. ë¨¸ì‹ ëŸ¬ë‹ ë¬¸ì œë¥¼ í•¨ìˆ˜ ê·¼ì‚¬ ë¬¸ì œë¡œ ìˆ˜í•™ì ìœ¼ë¡œ ëª¨ë¸ë§  
2. ê²½í—˜ì  ìœ„í—˜ ìµœì†Œí™”(ERM)ë¥¼ í†µí•´ í•™ìŠµì˜ ëª©í‘œë¥¼ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„  
3. ì˜¤ë²„í”¼íŒ…ê³¼ ì–¸ë”í”¼íŒ… ê°œë…ì„ í†µí•´ ì¼ë°˜í™” ì„±ëŠ¥ì˜ ì¤‘ìš”ì„±ì„ ì´í•´  
4. ì •ê·œí™”(Regularization)ì™€ Bias-Variance ê´€ì ì„ ë„ì…í•˜ì—¬  
   ì ì ˆí•œ ëª¨ë¸ ì„ íƒì˜ ê¸°ì¤€ì„ í•™ìŠµí•¨

---

> ğŸ§  ì´ ì„¸ì…˜ì€ ë¨¸ì‹ ëŸ¬ë‹ ì „ë°˜ì„ ì´í•´í•˜ê¸° ìœ„í•œ ê¸°ì´ˆ ê°œë…ì„ ì •ë¦¬í•˜ê³ ,  
> ì´í›„ ìµœì í™”ì™€ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì—°ê²°ë˜ëŠ” ì´ë¡ ì  ê¸°ë°˜ì„ ë§ˆë ¨í•´ì¤Œ.

# ğŸ“˜ Session 1: Course Introduction (í˜ì´ì§€ 1â€“15)

## ğŸ“Œ ì„¸ì…˜ ê°œìš”

ì´ ì„¸ì…˜ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ ê¸°ë³¸ ê°œë…ê³¼ ë¶„ë¥˜ ì²´ê³„ë¥¼ ì†Œê°œí•˜ê³ ,  
â€œë¬´ì—‡ì´ ë¨¸ì‹ ëŸ¬ë‹ì¸ê°€?â€ë¼ëŠ” ì§ˆë¬¸ì— ë‹µí•˜ë©´ì„œ  
**ì§€ë„í•™ìŠµ(supervised)**, **ë¹„ì§€ë„í•™ìŠµ(unsupervised)**,  
ê·¸ë¦¬ê³  **ì•½í•œ ì§€ë„í•™ìŠµ(weakly supervised)**ì˜ ê°œë…ì„ ì„¤ëª…í•œë‹¤.  
ë˜í•œ ëª¨ë¸ ì„¤ê³„ ê´€ì ì—ì„œ ë¨¸ì‹ ëŸ¬ë‹ ë°©ë²•ì„ **Generative vs Discriminative**,  
**Parametric vs Nonparametric**, **Linear vs Nonlinear** ë“±ìœ¼ë¡œ ë¶„ë¥˜í•œë‹¤.

---

## âœ… í•µì‹¬ ê°œë… ì •ë¦¬

### ğŸ”¹ What is Machine Learning?
- **ì •ì˜**: ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •ëŸ‰ì ì¸ ì¶”ë¡  ë° ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” í•™ë¬¸
- **ë°°ê²½**: í†µê³„í•™, ì¸ê³µì§€ëŠ¥, ì‹ í˜¸ì²˜ë¦¬ ë“±ì—ì„œ ìœ ë˜

---

### ğŸ”¹ Feature & Feature Vector
- **Feature**: ê´€ì¸¡ ëŒ€ìƒì˜ ìˆ˜ì¹˜ì  íŠ¹ì„± (ì˜ˆ: í‚¤, ëª¸ë¬´ê²Œ ë“±)
- **Feature Vector**:
  \[
  x = \begin{bmatrix} x_1 \\ \vdots \\ x_d \end{bmatrix} \in \mathbb{R}^d
  \]
- **Synonyms**: data point, instance, example, signal, input ë“±

---

### ğŸ”¹ Categories of Learning
- **Supervised Learning**
  - í•™ìŠµ ë°ì´í„°: \((x_1, y_1), \dots, (x_n, y_n)\)
  - ë¶„ë¥˜(classification), íšŒê·€(regression)
- **Unsupervised Learning**
  - ì •ë‹µ(label) ì—†ì´ ë°ì´í„° êµ¬ì¡° ë¶„ì„
  - í´ëŸ¬ìŠ¤í„°ë§, ì°¨ì› ì¶•ì†Œ, ë°€ë„ ì¶”ì • ë“±
- **Weakly Supervised Learning**
  - ë¶€ë¶„ì ìœ¼ë¡œ ë¼ë²¨ëœ ë°ì´í„° ì‚¬ìš© (ì˜ˆ: semi-supervised)

---

### ğŸ”¹ Supervised Learning: Classification vs Regression
- **Classification**: ì¶œë ¥ \( y \in \{1, \dots, C\} \) (ìœ í•œ í´ë˜ìŠ¤)
- **Regression**: ì¶œë ¥ \( y \in \mathbb{R} \) (ì—°ì†ì  ê°’)

---

### ğŸ”¹ Modeling Paradigms in ML
- **Generative vs Discriminative**
  - Generative: ë°ì´í„°ì˜ ì „ì²´ ë¶„í¬ë¥¼ ëª¨ë¸ë§ (ì˜ˆ: Naive Bayes)
  - Discriminative: ê²°ì • ê²½ê³„ë§Œ ëª¨ë¸ë§ (ì˜ˆ: Logistic Regression)
- **Parametric vs Nonparametric**
  - Parametric: íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ê³ ì •ë¨
  - Nonparametric: ë°ì´í„°ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ëª¨ë¸ ë³µì¡ë„ ì¦ê°€
- **Linear vs Nonlinear**
  - Linear: ì¶œë ¥ì´ ì…ë ¥ì˜ ì„ í˜•í•¨ìˆ˜
  - Nonlinear: ë¹„ì„ í˜• ê´€ê³„ë¥¼ ëª¨ë¸ë§

---

### ğŸ”¹ Practical Constraints
- **Scalability**: ëŒ€ê·œëª¨ ë°ì´í„°/ëª¨ë¸ì— ì í•©í•´ì•¼ í•¨
- **Privacy, Fairness, Safety**: ì‹¤ì œ ì‘ìš©ì—ì„œ ì¤‘ìš”í•œ ë¹„ê¸°ìˆ ì  ì œì•½

---

## ğŸ” ì„¸ì…˜ íë¦„ ìš”ì•½

1. ë¨¸ì‹ ëŸ¬ë‹ì´ ë‹¤ë£¨ëŠ” ë¬¸ì œ ì •ì˜ì™€ ë°°ê²½ ì†Œê°œ
2. Feature, Feature Vector ê°œë… ë° ìš©ì–´ ì •ë¦¬
3. í•™ìŠµ ë°©ì‹ ë¶„ë¥˜: ì§€ë„/ë¹„ì§€ë„/ì•½í•œ ì§€ë„í•™ìŠµ
4. ë¶„ë¥˜(Classification)ì™€ íšŒê·€(Regression) êµ¬ë¶„
5. ëª¨ë¸ ì„¤ê³„ ê´€ì ì—ì„œ ì—¬ëŸ¬ ê¸°ì¤€(Generative vs Discriminative ë“±) ì†Œê°œ
6. ì‹¤ì œ ì‹œìŠ¤í…œ ì„¤ê³„ ì‹œ ê³ ë ¤í•´ì•¼ í•  ì œì•½ ì¡°ê±´ê¹Œì§€ í¬ê´„ì ìœ¼ë¡œ ë‹¤ë£¸

---

ğŸ§  ì´ ì„¸ì…˜ì€ ìµœì í™” ë¬¸ì œë¡œ ë“¤ì–´ê°€ê¸° ì „, ë¨¸ì‹ ëŸ¬ë‹ì´ë¼ëŠ” ë¶„ì•¼ì˜ ì „ë°˜ì ì¸ êµ¬ì¡°ë¥¼ ì¡ì•„ì£¼ëŠ” ì—­í• ì„ í•¨.



# ğŸ“— Session 2: Unconstrained Optimization (í˜ì´ì§€ 16â€“45)

## ğŸ“Œ ì„¸ì…˜ ê°œìš”

ì´ ì„¸ì…˜ì€ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì˜ í•µì‹¬ì¸ **ìµœì í™” ë¬¸ì œ(Optimization Problem)**ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì •ì˜í•˜ê³ ,  
ê¸°ì´ˆì ì¸ **ë¯¸ë¶„ ì¡°ê±´**, **Gradient**, **Hessian**, **Taylor Expansion** ë“±ì„ ê¸°ë°˜ìœ¼ë¡œ  
ìµœì†Œê°’ì„ ì°¾ê¸° ìœ„í•œ ì¡°ê±´ì„ ì‚´í´ë³¸ë‹¤.  
ë˜í•œ ë¡œì§€ìŠ¤í‹± íšŒê·€(Logistic Regression) ë¬¸ì œë¥¼ ì‹¤ì œ ì˜ˆì‹œë¡œ ë“¤ì–´ **ì†ì‹¤ í•¨ìˆ˜**, **gradient**, **Hessian** ê³„ì‚°ë²•ì„ ë°°ìš´ë‹¤.

---

# ğŸ“— Session 2: Unconstrained Optimization (í˜ì´ì§€ 16â€“45)

## ğŸ“Œ ì„¸ì…˜ ê°œìš”

ì´ ì„¸ì…˜ì€ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì˜ í•µì‹¬ì¸ **ìµœì í™” ë¬¸ì œ(Optimization Problem)**ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì •ì˜í•˜ê³ ,  
ê¸°ì´ˆì ì¸ **ë¯¸ë¶„ ì¡°ê±´**, **Gradient**, **Hessian**, **Taylor Expansion** ë“±ì„ ê¸°ë°˜ìœ¼ë¡œ  
ìµœì†Œê°’ì„ ì°¾ê¸° ìœ„í•œ ì¡°ê±´ì„ ì‚´í´ë³¸ë‹¤.  
ë˜í•œ ë¡œì§€ìŠ¤í‹± íšŒê·€(Logistic Regression) ë¬¸ì œë¥¼ ì‹¤ì œ ì˜ˆì‹œë¡œ ë“¤ì–´ **ì†ì‹¤ í•¨ìˆ˜**, **gradient**, **Hessian** ê³„ì‚°ë²•ì„ ë°°ìš´ë‹¤.

---

## âœ… í•µì‹¬ ê°œë… ì •ë¦¬

### ğŸ”¹ Optimization Problem (ìµœì í™” ë¬¸ì œ)
- **ì •ì˜**:
  ```
  min_{x âˆˆ â„^d} f(x)
  ```
- **ì„¤ëª…**: ëª©ì  í•¨ìˆ˜ f(x)ë¥¼ ìµœì†Œí™”í•˜ëŠ” xë¥¼ ì°¾ëŠ” ë¬¸ì œ

---

### ğŸ”¹ Global vs. Local Minimum
- **Global Minimum**:
  ```
  f(x*) â‰¤ f(x), âˆ€ x âˆˆ â„^d
  ```
- **Local Minimum**:
  ```
  f(x*) â‰¤ f(x), âˆ€ x âˆˆ neighborhood(x*)
  ```

---

### ğŸ”¹ Gradient (ê¸°ìš¸ê¸°)
- **ì •ì˜**: ëª©ì  í•¨ìˆ˜ì˜ 1ì°¨ ë¯¸ë¶„ê°’, í•¨ìˆ˜ê°€ ê°€ì¥ ê°€íŒŒë¥´ê²Œ ì¦ê°€í•˜ëŠ” ë°©í–¥
  ```
  âˆ‡f(x) = [ âˆ‚f/âˆ‚xâ‚, ..., âˆ‚f/âˆ‚x_d ]áµ€
  ```

---

### ğŸ”¹ Hessian (í—¤ì„¸ í–‰ë ¬)
- **ì •ì˜**: ëª©ì  í•¨ìˆ˜ì˜ 2ì°¨ í¸ë¯¸ë¶„ê°’ìœ¼ë¡œ êµ¬ì„±ëœ ëŒ€ì¹­ í–‰ë ¬
  ```
  âˆ‡Â²f(x) = [ âˆ‚Â²f / âˆ‚x_i âˆ‚x_j ]_{i,j}
  ```

---

### ğŸ”¹ Optimality Conditions (ìµœì  ì¡°ê±´)
- **í•„ìš”ì¡°ê±´**:
  ```
  âˆ‡f(x*) = 0
  ```
- **ì¶©ë¶„ì¡°ê±´**:
  ```
  âˆ‡Â²f(x*) â‰» 0  (positive definite)
  ```

---

### ğŸ”¹ Taylor Expansion (2ì°¨ í…Œì¼ëŸ¬ ê·¼ì‚¬)
- **ì‹**:
  ```
  f(x + Î”x) â‰ˆ f(x) + âˆ‡f(x)áµ€ Î”x + Â½ Î”xáµ€ âˆ‡Â²f(x) Î”x
  ```

---

### ğŸ”¹ Logistic Regression Loss (Negative Log Likelihood)
- **ì‹**:
  ```
  min_Î¸ âˆ‘_{i=1}^n log(1 + exp(âˆ’yáµ¢ Î¸áµ€ xáµ¢))
  ```

---

### ğŸ”¹ Regularized Logistic Regression
- **L2 ì •ê·œí™” í¬í•¨**:
  ```
  min_Î¸ âˆ‘_{i=1}^n log(1 + exp(âˆ’yáµ¢ Î¸áµ€ xáµ¢)) + Î» ||Î¸||Â²
  ```

---

### ğŸ”¹ Gradient of Logistic Loss
- **ì‹**:
  ```
  âˆ‡Î¸ f(Î¸) = âˆ‘_{i=1}^n (Ïƒ(Î¸áµ€ xáµ¢) âˆ’ yáµ¢) xáµ¢
  ```
  where:
  ```
  Ïƒ(z) = 1 / (1 + exp(âˆ’z))
  ```

---

### ğŸ”¹ Hessian of Logistic Loss
- **ì‹**:
  ```
  âˆ‡Â²f(Î¸) = âˆ‘_{i=1}^n Ïƒ(Î¸áµ€ xáµ¢)(1 âˆ’ Ïƒ(Î¸áµ€ xáµ¢)) xáµ¢ xáµ¢áµ€
  ```

---

## ğŸ” ì„¸ì…˜ íë¦„ ìš”ì•½

1. ë¨¸ì‹ ëŸ¬ë‹ í•™ìŠµ ë¬¸ì œë¥¼ **ìµœì í™” ë¬¸ì œ**ë¡œ ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„  
2. **Gradient**ì™€ **Hessian**ì„ í†µí•´ ê·¹ê°’ ì¡°ê±´ì„ ì •ì˜  
3. **Taylor ì „ê°œ**ë¡œ ìµœì í™” ë°©í–¥ê³¼ ì–‘ì˜ ì •ë¶€í˜¸ ì¡°ê±´ ë„ì…  
4. ë¡œì§€ìŠ¤í‹± íšŒê·€ë¥¼ ì‹¤ì œ ì˜ˆë¡œ ë“¤ì–´ ì†ì‹¤ í•¨ìˆ˜ ì •ì˜,  
   gradient ë° Hessian ê³„ì‚° ë°©ë²•ì„ í•™ìŠµí•¨  
5. ë‹¤ìŒ ì„¸ì…˜(Iterative Algorithm)ì—ì„œ Newton's methodë¡œ ì—°ê²°ë  ê¸°ì´ˆ ì™„ì„±

---

> ğŸ§  ì´ ì„¸ì…˜ì€ ìµœì í™” ì´ë¡ ì˜ í•µì‹¬ ê°œë…ì„ ì •ë¦½í•˜ê³ ,  
> ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ë°˜ë³µ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•  ìˆ˜ ìˆëŠ” **ìˆ˜í•™ì  ë„êµ¬**ë¥¼ ë§ˆë ¨í•´ì¤Œ.


# ğŸ“˜ Session 3: Iterative Algorithms for Continuous Optimization (í˜ì´ì§€ 46â€“89)

## ğŸ“Œ ì„¸ì…˜ ê°œìš”

ì´ ì„¸ì…˜ì—ì„œëŠ” **ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì˜ í•µì‹¬ êµ¬ì¡°**ë¥¼ ì´í•´í•˜ê³ ,  
ëŒ€í‘œì ì¸ ë°˜ë³µ ìµœì í™” ê¸°ë²•ì¸  
- **Gradient Descent (GD)**  
- **Newtonâ€™s Method**  
- **Quasi-Newton (íŠ¹íˆ BFGS)**  
ì˜ ê°œë…ê³¼ ìˆ˜ì‹, êµ¬í˜„ êµ¬ì¡°ë¥¼ ë¹„êµí•œë‹¤.  
ë˜í•œ **Line Search**ë¥¼ ì´ìš©í•œ step size ì¡°ì ˆê³¼ ê° ì•Œê³ ë¦¬ì¦˜ì˜ **ìˆ˜ë ´ ì†ë„ ì°¨ì´**ë„ í•¨ê»˜ ë°°ìš´ë‹¤.

---

## âœ… í•µì‹¬ ê°œë… ì •ë¦¬

### ğŸ”¹ General Form of Iterative Optimization
- **ì—…ë°ì´íŠ¸ ì‹**:
  ```
  x^{(k+1)} = x^{(k)} + Î±^{(k)} p^{(k)}
  ```
- **ì„¤ëª…**:
  - $begin:math:text$ p^{(k)} $end:math:text$: ì´ë™ ë°©í–¥ (descent direction)
  - $begin:math:text$ Î±^{(k)} $end:math:text$: step size (learning rate)

---

### ğŸ”¹ Descent Direction (ê°ì†Œ ë°©í–¥)
- **ì¡°ê±´**:
  ```
  âˆ‡f(x)^T p < 0
  ```
- **ì„¤ëª…**: ëª©ì  í•¨ìˆ˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆëŠ” ë°©í–¥ì´ë©´ descent directionì´ë¼ ë¶€ë¦„

---

### ğŸ”¹ Gradient Descent (GD)
- **ì—…ë°ì´íŠ¸ ì‹**:
  ```
  x^{(k+1)} = x^{(k)} âˆ’ Î±^{(k)} âˆ‡f(x^{(k)})
  ```
- **ì„¤ëª…**:
  - ê°€ì¥ ë‹¨ìˆœí•œ ì•Œê³ ë¦¬ì¦˜
  - ê³„ì‚°ì€ ì‰½ì§€ë§Œ ìˆ˜ë ´ ì†ë„ëŠ” ëŠë¦´ ìˆ˜ ìˆìŒ

---

### ğŸ”¹ Newton's Method
- **ì—…ë°ì´íŠ¸ ì‹**:
  ```
  x^{(k+1)} = x^{(k)} âˆ’ [âˆ‡Â²f(x^{(k)})]â»Â¹ âˆ‡f(x^{(k)})
  ```
- **ì„¤ëª…**:
  - 2ì°¨ í…Œì¼ëŸ¬ ê·¼ì‚¬ë¥¼ ì´ìš©í•œ ë¹ ë¥¸ ìˆ˜ë ´
  - ê³„ì‚° ë³µì¡ë„ëŠ” ë†’ìŒ (Hessian í•„ìš”)

---

### ğŸ”¹ Quasi-Newton Method (e.g., BFGS)
- **ì—…ë°ì´íŠ¸ ë°©í–¥**:
  ```
  p^{(k)} = âˆ’ B_kâ»Â¹ âˆ‡f(x^{(k)})
  ```
- **BFGS ì—…ë°ì´íŠ¸ ì‹**:
  ```
  B_{k+1} = B_k + (y_k y_k^T) / (y_k^T s_k) âˆ’ (B_k s_k s_k^T B_k) / (s_k^T B_k s_k)
  ```
  where:
  ```
  s_k = x^{(k+1)} âˆ’ x^{(k)}
  y_k = âˆ‡f(x^{(k+1)}) âˆ’ âˆ‡f(x^{(k)})
  ```
- **ì„¤ëª…**:
  - Hessianì„ ê³„ì‚°í•˜ì§€ ì•Šê³  ê·¼ì‚¬
  - ì‹¤ë¬´ì—ì„œ ìì£¼ ì‚¬ìš©ë¨ (ì˜ˆ: SciPy `BFGS`)

---

### ğŸ”¹ Taylor Expansion (ë³µìŠµ)
- **2ì°¨ ê·¼ì‚¬ ì‹**:
  ```
  f(x + Î”x) â‰ˆ f(x) + âˆ‡f(x)^T Î”x + Â½ Î”x^T âˆ‡Â²f(x) Î”x
  ```
- Newton MethodëŠ” ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ë¥¼ ê³„ì‚°í•¨

---

### ğŸ”¹ Line Search (Backtracking)
- **ì¡°ê±´**:
  ```
  f(x + Î± p) â‰¤ f(x) + c Î± âˆ‡f(x)^T p
  ```
- **ì„¤ëª…**:
  - ì ì ˆí•œ step size Î±ë¥¼ ì°¾ê¸° ìœ„í•´ ë°˜ë³µì ìœ¼ë¡œ ì¤„ì—¬ê°€ë©° ë§Œì¡± ì¡°ê±´ì„ ê²€ì‚¬
  - ì•ˆì •ì ì¸ ìˆ˜ë ´ì„ ìœ„í•´ ëª¨ë“  ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì—ì„œ ìì£¼ ì‚¬ìš©

---

### ğŸ”¹ Convergence Rates
- **Gradient Descent**: Linear convergence  
- **Newtonâ€™s Method**: Quadratic convergence  
- **Quasi-Newton (BFGS)**: Superlinear convergence

---

## ğŸ” ì„¸ì…˜ íë¦„ ìš”ì•½

1. ëª¨ë“  ë°˜ë³µ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì˜ **ê³µí†µ êµ¬ì¡°**ë¥¼ ë¨¼ì € ì´í•´  
2. **Gradient Descent** â†’ ë‹¨ìˆœí•˜ê³  ì§ê´€ì ì´ì§€ë§Œ ëŠë¦´ ìˆ˜ ìˆìŒ  
3. **Newtonâ€™s Method** â†’ ë¹ ë¥´ì§€ë§Œ ê³„ì‚°ëŸ‰ í¼  
4. **Quasi-Newton (BFGS)** â†’ ê³„ì‚° íš¨ìœ¨ê³¼ ìˆ˜ë ´ ì†ë„ì˜ ì ˆì¶©ì•ˆ  
5. **Line Search**ë¡œ ì•ˆì •ì„± í™•ë³´  
6. ê°ê°ì˜ ë°©ë²•ì´ **ì–´ë–¤ ì •ë³´(gradient, Hessian)ë¥¼ í™œìš©í•˜ëŠ”ì§€**,  
   ê·¸ë¦¬ê³  ìˆ˜ë ´ ì†ë„ê°€ ì–´ë–»ê²Œ ë‹¤ë¥¸ì§€ë¥¼ ë¹„êµí•¨

---

> ğŸ§  ì´ ì„¸ì…˜ì€ â€œì–´ë–»ê²Œ ìµœì í™”í•  ê²ƒì¸ê°€?â€ë¼ëŠ” ì‹¤ì „ì  ì§ˆë¬¸ì— ë‹µí•˜ë©´ì„œ,  
> ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì˜ ê¸°ì´ˆ ì—°ì‚°ì´ ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜ë“¤ì˜ êµ¬ì¡°ë¥¼ ì •ë¦½í•´ì¤Œ.


# ğŸ“™ Session 4: Nearest Neighbor Classifier (í˜ì´ì§€ 90â€“99)

## ğŸ“Œ ì„¸ì…˜ ê°œìš”

ì´ ì„¸ì…˜ì€ **K-ìµœê·¼ì ‘ ì´ì›ƒ(K-Nearest Neighbor, K-NN)** ì•Œê³ ë¦¬ì¦˜ì„ ë‹¤ë£¬ë‹¤.  
K-NNì€ í•™ìŠµì„ í•˜ì§€ ì•Šê³ , ë‹¨ìˆœíˆ **ì €ì¥ëœ ë°ì´í„°**ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ê°€ê¹Œìš´ Kê°œì˜ ì´ì›ƒì˜ ë¼ë²¨ì„ ì´ìš©í•´  
ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” **ë¹„ëª¨ìˆ˜ì (non-parametric)** ë¶„ë¥˜ê¸°ë‹¤.  
ë˜í•œ **ê±°ë¦¬ ê¸°ë°˜ ë¶„ë¥˜ì˜ í•œê³„ì **ê³¼ **ì°¨ì›ì˜ ì €ì£¼**, **bias-variance tradeoff**ì— ëŒ€í•´ì„œë„ ì„¤ëª…í•œë‹¤.

---

## âœ… í•µì‹¬ ê°œë… ì •ë¦¬

### ğŸ”¹ 1-Nearest Neighbor (1-NN)
- **ì •ì˜**:
  ```
  Å·(x) = y_{i*}, where i* = argmin_i ||x âˆ’ x_i||_2
  ```
- **ì„¤ëª…**: ì…ë ¥ xì— ëŒ€í•´ ê°€ì¥ ê°€ê¹Œìš´ í•™ìŠµ ìƒ˜í”Œì˜ ë¼ë²¨ yë¥¼ ê·¸ëŒ€ë¡œ ì˜ˆì¸¡í•¨

---

### ğŸ”¹ K-Nearest Neighbor (K-NN)
- **ì •ì˜**:
  ```
  Å·(x) = majority_vote { y_i | i âˆˆ ğ’©_K(x) }
  ```
  where:
  - $begin:math:text$ ğ’©_K(x) $end:math:text$: xì˜ K-ìµœê·¼ì ‘ ì´ì›ƒ ì¸ë±ìŠ¤ ì§‘í•©

- **ì„¤ëª…**: Kê°œì˜ ê°€ì¥ ê°€ê¹Œìš´ í›ˆë ¨ ìƒ˜í”Œì„ ì°¾ê³ , ê·¸ë“¤ì˜ ë¼ë²¨ì„ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ìˆ˜ê²°ë¡œ ì˜ˆì¸¡

---

### ğŸ”¹ Distance Metrics (ê±°ë¦¬ ì²™ë„)
- **Euclidean (L2)**:
  ```
  ||x âˆ’ x_i||_2 = sqrt(âˆ‘_{j=1}^d (x_j âˆ’ x_{ij})Â²)
  ```
- **Manhattan (L1)**:
  ```
  âˆ‘ |x_j âˆ’ x_{ij}|
  ```
- **Cosine distance**, **Mahalanobis distance** ë“±ë„ ì¡´ì¬í•¨

---

### ğŸ”¹ Curse of Dimensionality (ì°¨ì›ì˜ ì €ì£¼)
- **ì„¤ëª…**:
  - ì°¨ì›ì´ ë†’ì•„ì§ˆìˆ˜ë¡ ëª¨ë“  ë°ì´í„° í¬ì¸íŠ¸ë“¤ì´ ë©€ì–´ì§„ë‹¤  
  - K-NNì²˜ëŸ¼ ê±°ë¦¬ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì€ ì°¨ì›ì´ ë†’ì„ìˆ˜ë¡ ì •í™•ë„ê°€ ë‚®ì•„ì§ˆ ìˆ˜ ìˆìŒ

---

### ğŸ”¹ Bias-Variance Tradeoff in K-NN
- **ì„¤ëª…**:
  - Kê°€ ì‘ì„ìˆ˜ë¡ **ëª¨ë¸ì´ ë¯¼ê°**í•´ì§ â†’ **low bias, high variance**
  - Kê°€ í´ìˆ˜ë¡ **ëª¨ë¸ì´ ë¶€ë“œëŸ¬ì›Œì§** â†’ **high bias, low variance**
  - ë”°ë¼ì„œ ì ì ˆí•œ K ì„ íƒì€ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ì¤€ë‹¤

---

## ğŸ” ì„¸ì…˜ íë¦„ ìš”ì•½

1. ê°€ì¥ ê°„ë‹¨í•œ ë¶„ë¥˜ê¸°ì¸ **1-NN** ì†Œê°œ â†’ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒì˜ ë¼ë²¨ì„ ë”°ë¦„  
2. **K-NN** ì¼ë°˜í™” â†’ Kê°œì˜ ì´ì›ƒ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡  
3. ê±°ë¦¬ ê³„ì‚° ê¸°ì¤€ì´ ì˜ˆì¸¡ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ì¤€ë‹¤ëŠ” ì  ê°•ì¡°  
4. ê³ ì°¨ì› ë°ì´í„°ì¼ìˆ˜ë¡ ê±°ë¦¬ ê¸°ë°˜ ë¶„ë¥˜ê¸°ì˜ ì„±ëŠ¥ ì €í•˜ (ì°¨ì›ì˜ ì €ì£¼)  
5. K ê°’ì„ ì¡°ì ˆí•˜ë©´ì„œ ìƒê¸°ëŠ” **bias-variance tradeoff**ë¥¼ ë¶„ì„í•¨

---

> ğŸ§  ì´ ì„¸ì…˜ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ **ë¹„ëª¨ìˆ˜ì  í•™ìŠµ(non-parametric learning)**ì˜ ê¸°ì´ˆë¥¼ ì œê³µí•˜ë©°,  
> ë§¤ìš° ë‹¨ìˆœí•˜ì§€ë§Œ ê°•ë ¥í•œ K-NN ë¶„ë¥˜ê¸°ì˜ ì‘ë™ ì›ë¦¬ì™€ í•œê³„ë¥¼ í•™ìŠµí•¨.



ì¢‹ì•„! ê·¸ëŸ¼ ë°”ë¡œ ì´ì–´ì„œ ë‹¤ìŒ ì„¸ì…˜ ì •ë¦¬ ë“¤ì–´ê°ˆê²Œ â€” ì´ ì„¸ì…˜ë¶€í„°ëŠ” ì§„ì§œ ìˆ˜í•™ì  ìµœì í™”ì˜ ë³¸ë¡ ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” íŒŒíŠ¸ì•¼. ë„ˆê°€ íŠ¹íˆ ì¤‘ìš”í•˜ê²Œ ì—¬ê¸°ëŠ” ê°œë… â†” ìˆ˜ì‹ ì—°ê²°ë„ ì² ì €íˆ ì •ë¦¬í•´ì¤„ê²Œ.

â¸»

ğŸ“˜ Session 2: Unconstrained Optimization (16â€“45í˜ì´ì§€)

ğŸ” ì „ì²´ ì„¸ì…˜ íë¦„ ìš”ì•½:

ì´ ì„¸ì…˜ì€ â€œì œì•½ ì—†ëŠ” ìµœì í™”(Unconstrained Optimization)â€ë¥¼ ë‹¤ë£¨ë©°,
ìµœì†Œí™” ë¬¸ì œì˜ ê¸°ë³¸ ê°œë…, í•„ìš”ì¶©ë¶„ì¡°ê±´, ê·¸ë˜ë””ì–¸íŠ¸/í—¤ì„¸ í–‰ë ¬,
ê·¸ë¦¬ê³  **ë¡œì§€ìŠ¤í‹± íšŒê·€(Logistic Regression)**ì˜ ì†ì‹¤ í•¨ìˆ˜ í˜•íƒœê¹Œì§€ ë‹¤ë£¸.

â¸»

ğŸ“Œ ê°œë… 1: Optimization Problem (ìµœì í™” ë¬¸ì œ)
	â€¢	ì •ì˜:
\min_{x \in \mathbb{R}^d} f(x)
	â€¢	ì„¤ëª…: ì–´ë–¤ ëª©ì  í•¨ìˆ˜ f(x)ë¥¼ ìµœì†Œí™”í•˜ëŠ” xë¥¼ ì°¾ëŠ” ë¬¸ì œ. ì´ë•Œ ì œì•½ ì¡°ê±´ì€ ì—†ìŒ (Unconstrained).

â¸»

ğŸ“Œ ê°œë… 2: Local vs. Global Minimum
	â€¢	ì •ì˜:
	â€¢	Global minimum: f(x^*) \leq f(x), \forall x
	â€¢	Local minimum: f(x^) \leq f(x), \forall x \in \mathcal{B}(x^)
	â€¢	ì„¤ëª…: ì§€ì—­ ìµœì†Ÿê°’ì€ ì£¼ë³€ì—ì„œë§Œ ìµœì†Œ, ì „ì—­ ìµœì†Ÿê°’ì€ ì „ì²´ ì˜ì—­ì—ì„œ ìµœì†Œ

â¸»

ğŸ“Œ ê°œë… 3: Gradient (ê¸°ìš¸ê¸°)
	â€¢	ì •ì˜: ëª©ì  í•¨ìˆ˜ f(x)ì˜ ë³€í™” ë°©í–¥ê³¼ í¬ê¸°ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„°
	â€¢	ìˆ˜ì‹:
\nabla f(x) = \begin{bmatrix}
\frac{\partial f}{\partial x_1}, \dots, \frac{\partial f}{\partial x_d}
\end{bmatrix}^\top
	â€¢	ì„¤ëª…: \nabla f(x)ê°€ 0ì¸ ì ì€ (êµ­ì†Œì ) ê·¹ê°’ì¼ ê°€ëŠ¥ì„±ì´ ìˆìŒ

â¸»

ğŸ“Œ ê°œë… 4: Hessian Matrix (í—¤ì„¸ í–‰ë ¬)
	â€¢	ì •ì˜: f(x)ì˜ 2ì°¨ ë¯¸ë¶„ê°’ë“¤ì„ ëª¨ì€ ëŒ€ì¹­ í–‰ë ¬
	â€¢	ìˆ˜ì‹:
\nabla^2 f(x) = \left[ \frac{\partial^2 f}{\partial x_i \partial x_j} \right]_{i,j}
	â€¢	ì„¤ëª…: í•¨ìˆ˜ì˜ ê³¡ë¥ (curvature)ì„ ë‚˜íƒ€ëƒ„ â†’ ìµœì ì ì´ ìµœì†Œì¸ì§€ ìµœëŒ€ì¸ì§€ íŒë‹¨í•  ìˆ˜ ìˆìŒ

â¸»

ğŸ“Œ ê°œë… 5: Second Order Optimality Conditions
	â€¢	í•„ìš”ì¡°ê±´:
\nabla f(x^*) = 0
	â€¢	ì¶©ë¶„ì¡°ê±´:
\nabla^2 f(x^*) \succ 0 \quad (\text{positive definite})
	â€¢	ì„¤ëª…:
	â€¢	ê¸°ìš¸ê¸° = 0ì´ë©´ ê·¹ê°’ í›„ë³´
	â€¢	í—¤ì„¸ í–‰ë ¬ì´ ì–‘ì˜ ì •ë¶€í˜¸ë©´ ê·¸ ì ì€ ì§€ì—­ ìµœì†Œê°’

â¸»

ğŸ“Œ ê°œë… 6: Taylor Approximation (í…Œì¼ëŸ¬ ì „ê°œ)
	â€¢	ìˆ˜ì‹:
f(x + \Delta x) \approx f(x) + \nabla f(x)^\top \Delta x + \frac{1}{2} \Delta x^\top \nabla^2 f(x) \Delta x
	â€¢	ì„¤ëª…: ìµœì í™” ì‹œ, ëª©ì  í•¨ìˆ˜ë¥¼ ë¡œì»¬ ê·¼ë°©ì—ì„œ 2ì°¨ ê·¼ì‚¬í•  ë•Œ ì‚¬ìš©

â¸»

ğŸ“Œ ê°œë… 7: Logistic Regression Objective (Negative Log Likelihood)
	â€¢	ìˆ˜ì‹:
\min_\theta \sum_{i=1}^n \log(1 + e^{-y_i \theta^\top x_i})
	â€¢	ë˜ëŠ” log loss í˜•íƒœ:
\ell(y, f(x)) = \log(1 + e^{-y f(x)})
	â€¢	ì„¤ëª…: ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ ì†ì‹¤ í•¨ìˆ˜. convexí•˜ê³  smoothí•¨.

â¸»

ğŸ“Œ ê°œë… 8: Regularized Logistic Regression
	â€¢	ìˆ˜ì‹:
\min_\theta \sum_{i=1}^n \log(1 + e^{-y_i \theta^\top x_i}) + \lambda \|\theta\|^2
	â€¢	ì„¤ëª…: ì˜¤ë²„í”¼íŒ… ë°©ì§€ë¥¼ ìœ„í•œ L2 ì •ê·œí™” í¬í•¨

â¸»

ğŸ“Œ ê°œë… 9: Gradient and Hessian of Logistic Regression
	â€¢	Gradient:
\nabla_\theta f(\theta) = \sum_{i=1}^n \left( \sigma(\theta^\top x_i) - y_i \right) x_i
	â€¢	Hessian:
\nabla^2 f(\theta) = \sum_{i=1}^n \sigma(\theta^\top x_i)(1 - \sigma(\theta^\top x_i)) x_i x_i^\top
	â€¢	ì„¤ëª…:
	â€¢	sigmoid: \sigma(z) = \frac{1}{1 + e^{-z}}
	â€¢	ë‰´í„´ ë°©ë²•ì—ì„œ Hessian ì‚¬ìš©ë¨

â¸»

ğŸ” ì„¸ì…˜ íë¦„ ìš”ì•½
	1.	ìµœì í™” ê¸°ë³¸ êµ¬ì¡° ì†Œê°œ (ëª©ì  í•¨ìˆ˜, gradient, ìµœì†Œí™” ê°œë…)
	2.	ê·¹ê°’ ì¡°ê±´ â†’ ê¸°ìš¸ê¸° ì¡°ê±´ + í—¤ì„¸ í–‰ë ¬ ì¡°ê±´
	3.	Taylor ê·¼ì‚¬ë¥¼ í†µí•´ ì™œ 2ì°¨ ì •ë³´ê°€ ì¤‘ìš”í•œì§€ ì„¤ëª…
	4.	ë¡œì§€ìŠ¤í‹± íšŒê·€ë¥¼ ì˜ˆì œë¡œ, ì‹¤ì œ ì†ì‹¤ í•¨ìˆ˜ + gradient + Hessian ê³„ì‚°ê¹Œì§€ ë‹¤ë£¸
	5.	ë‹¤ìŒ ì„¸ì…˜ì˜ ë‰´í„´ ë°©ë²•ì„ ìœ„í•œ ê¸°ì´ˆ ë‹¤ì§€ê¸°!

â¸»

ë‹¤ìŒì€ Session 3: Iterative Algorithms for Continuous Optimization (46â€“89í˜ì´ì§€) ë“¤ì–´ê°ˆê²Œ!
ì´ íŒŒíŠ¸ì—ì„œëŠ” Gradient Descent, Newtonâ€™s Method, Quasi-Newtonê¹Œì§€ ë‹¤ë£¨ë‹ˆê¹Œ ë°©ê¸ˆ ë°°ìš´ ì´ë¡ ë“¤ì´ ì‹¤ì œë¡œ ì–´ë–»ê²Œ ì•Œê³ ë¦¬ì¦˜ì´ ë˜ëŠ”ì§€ ì´ì–´ì§„ë‹¤!
ê³§ë°”ë¡œ ì •ë¦¬í•´ì¤„ê²Œ!