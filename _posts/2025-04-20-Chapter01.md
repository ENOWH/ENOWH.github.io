---
title: "Chapter01"
excerpt: "ë¨¸ì‹ ëŸ¬ë‹ Chap01 ì „ì²´ íë¦„"

categories:
  - MachineLearning
tags:
  - [Machine Learning, ë¨¸ì‹ ëŸ¬ë‹]

permalink: /categories/MachineLearning/Chapter01_MachineLearning

toc: true
toc_sticky: true

date: 2025-04-20
last_modified_at: 2025-04-20
---

# ğŸ“˜ Session 1: Course Introduction

## ğŸŒ± ê°œë… íë¦„ ìš”ì•½
- ë¨¸ì‹ ëŸ¬ë‹ì€ **ë°ì´í„°ë¡œë¶€í„° ì˜ˆì¸¡ì´ë‚˜ ì¶”ë¡ ì„ ìˆ˜í–‰**í•˜ëŠ” í•™ë¬¸ì´ë‹¤.
- ë¬¸ì œ ìœ í˜•ì€ í¬ê²Œ **Supervised / Unsupervised / Weakly Supervised Learning** ìœ¼ë¡œ ë‚˜ë‰œë‹¤.
- ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì€ ì—¬ëŸ¬ ê¸°ì¤€ì— ë”°ë¼ ë‚˜ë‰œë‹¤:  
  (1) ë°ì´í„° ë¶„í¬ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ë°©ì‹ (Generative vs. Discriminative)  
  (2) ëª¨ë¸ì˜ ë³µì¡ë„ (Parametric vs. Nonparametric)  
  (3) ì…ë ¥ê³¼ ì¶œë ¥ì˜ ê´€ê³„ (Linear vs. Nonlinear)
- ì‹¤ì œ ë¬¸ì œì—ì„œëŠ” **Scalability, Privacy, Fairness, Safety** ê°™ì€ ì œì•½ì¡°ê±´ë“¤ë„ ì¤‘ìš”í•˜ë‹¤.

---

## ğŸ”‘ ê°œë…ë³„ ì •ë¦¬ (ì´ë¦„ â†” ìˆ˜ì‹ ì—°ê²° ì¤‘ì‹¬)

### âœ… Feature & Feature Vector

- **Feature (íŠ¹ì§•)**: ê´€ì¸¡ ëŒ€ìƒì˜ ìˆ˜ì¹˜ì  íŠ¹ì„± (ì˜ˆ: í‚¤, ëª¸ë¬´ê²Œ, ì ìˆ˜)
- **Feature Vector (íŠ¹ì§• ë²¡í„°)**: ì—¬ëŸ¬ featureë“¤ì„ ëª¨ì€ ë²¡í„°

ğŸ“ ìˆ˜ì‹:
$$
\mathbf{x} = \begin{bmatrix} x_1 \\ \vdots \\ x_d \end{bmatrix}, \quad \mathbf{x} \in \mathbb{R}^d
$$

ğŸ’¬ ë‹¤ë¥¸ í‘œí˜„ë“¤: attributes, predictors, covariates, inputs

---

### âœ… Supervised Learning (ì§€ë„ í•™ìŠµ)

- **ì •ì˜**: ì…ë ¥ $\mathbf{x}$ì— ëŒ€í•´ ì •ë‹µ $y$ê°€ ì£¼ì–´ì§„ ìƒíƒœì—ì„œ í•™ìŠµ
- **ëª©ì **: **ì˜ˆì¸¡ (Prediction)**

ğŸ§ª ìˆ˜ì‹ (í•™ìŠµ ë°ì´í„°):
$$
\{(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)\}
$$

ğŸ§© ì„¸ë¶€ ìœ í˜•:
- **Classification**: $y$ê°€ ì´ì‚°ê°’ (ì˜ˆ: 0~9 ìˆ«ì ë¶„ë¥˜)
- **Regression**: $y$ê°€ ì—°ì†ê°’ (ì˜ˆ: ì§‘ ê°’ ì˜ˆì¸¡)

---

### âœ… Unsupervised Learning (ë¹„ì§€ë„ í•™ìŠµ)

- **ì •ì˜**: ì •ë‹µ(label)ì´ ì—†ëŠ” ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ ì°¾ëŠ” í•™ìŠµ
- **ëª©ì **: **ì¶”ë¡  (Inference)**

ì˜ˆì‹œ ìœ í˜•:
- Clustering (êµ°ì§‘í™”)
- Dimensionality Reduction (ì°¨ì› ì¶•ì†Œ)
- Density Estimation (í™•ë¥ ë°€ë„ ì¶”ì •)

âŒ ì¶œë ¥ ë³€ìˆ˜ $y$ê°€ ì—†ê¸° ë•Œë¬¸ì— ìˆ˜ì‹ì€ ì£¼ë¡œ $\mathbf{x}$ì˜ êµ¬ì¡°ì— ì´ˆì 

---

### âœ… Weakly Supervised Learning (ì•½ì§€ë„ í•™ìŠµ)

- **ì •ì˜**: ë¼ë²¨ì´ ë¶€ì¡±í•˜ê±°ë‚˜ ë¶€ì •í™•í•œ ë°ì´í„°ë¥¼ í™œìš©í•˜ëŠ” í•™ìŠµ
- ì˜ˆì‹œ:
  - Semi-supervised Learning: ì¼ë¶€ë§Œ ë¼ë²¨ ìˆìŒ
  - Noisy Labels: ë¼ë²¨ì— ì˜¤ë¥˜ ìˆìŒ

---

### âœ… Generative vs. Discriminative

| ê°œë… | ì„¤ëª… | ê´€ë ¨ ìˆ˜ì‹ |
|------|------|-----------|
| **Generative Model** | ì…ë ¥ê³¼ ì¶œë ¥ì„ **ê³µë™ ë¶„í¬**ë¡œ ëª¨ë¸ë§ | $$ p(\mathbf{x}, y) = p(y) \cdot p(\mathbf{x} \mid y) $$ |
| **Discriminative Model** | **ì¡°ê±´ë¶€ ë¶„í¬**ë§Œ ëª¨ë¸ë§í•˜ì—¬ ì§ì ‘ ì˜ˆì¸¡ | $$ p(y \mid \mathbf{x}) $$ |

ğŸ§  GenerativeëŠ” ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆê³ , DiscriminativeëŠ” ì˜ˆì¸¡ì— ìµœì í™”ë¨.

---

### âœ… Parametric vs. Nonparametric

- **Parametric**: ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ê³ ì • (ì˜ˆ: ì„ í˜• íšŒê·€)
- **Nonparametric**: ë°ì´í„°ê°€ ë§ì•„ì§ˆìˆ˜ë¡ íŒŒë¼ë¯¸í„° ìˆ˜ ì¦ê°€ (ì˜ˆ: k-NN)

---

### âœ… Linear vs. Nonlinear

- **Linear Model**: ì¶œë ¥ì´ ì…ë ¥ì˜ **ì„ í˜• í•¨ìˆ˜**  
  $$ f(\mathbf{x}) = \mathbf{w}^\top \mathbf{x} + b $$
- **Nonlinear Model**: ìœ„ í˜•íƒœë¥¼ ë”°ë¥´ì§€ ì•ŠìŒ (ì˜ˆ: ì‹ ê²½ë§)

---

### âœ… í˜„ì‹¤ ì œì•½ ì¡°ê±´ (Constraints)

| ì œì•½ ì¡°ê±´ | ì„¤ëª… |
|-----------|------|
| **Scalability** | í° ë°ì´í„°ì…‹ì—ì„œë„ ë¹ ë¥´ê²Œ ì‘ë™ |
| **Privacy** | ê°œì¸ ì •ë³´ ë³´í˜¸ |
| **Fairness** | íŠ¹ì • ì§‘ë‹¨ì— ë¶ˆê³µì •í•˜ì§€ ì•Šê²Œ |
| **Safety** | ì˜ëª»ëœ íŒë‹¨ì„ í•˜ì§€ ì•Šë„ë¡ ì•ˆì „í•˜ê²Œ ì„¤ê³„ |


# ğŸ§® Session 2: Unconstrained Optimization

## ğŸŒ± ê°œë… íë¦„ ìš”ì•½
- ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ë§ì€ ë¬¸ì œëŠ” ì–´ë–¤ **ëª©ì  í•¨ìˆ˜ f(x)** ë¥¼ ìµœì†Œí™”í•˜ëŠ” í˜•íƒœë¡œ í‘œí˜„ëœë‹¤.
- ì´ë•Œ ì œì•½ì¡°ê±´ì´ ì—†ëŠ” ê²½ìš° â†’ **Unconstrained Optimization**
- ìµœì í™” ë¬¸ì œì—ì„œ í•´ê°€ ìµœì ì¸ì§€ë¥¼ íŒë‹¨í•˜ë ¤ë©´ **1ì°¨, 2ì°¨ ì¡°ê±´ (First/Second Order Conditions)** ì„ ì‚¬ìš©í•œë‹¤.
- íŠ¹íˆ **Convexity (í•¨ìˆ˜ì˜ ë³¼ë¡ì„±)** ëŠ” ìµœì í™”ì—ì„œ í•´ê°€ ì „ì—­í•´ì¸ì§€ íŒë‹¨í•˜ëŠ” ë° ë§¤ìš° ì¤‘ìš”í•˜ë‹¤.

---

## ğŸ”‘ ê°œë…ë³„ ì •ë¦¬ (ì´ë¦„ â†” ìˆ˜ì‹ ì—°ê²°)

### âœ… Unconstrained Optimization Problem

- **ì •ì˜**: ì œì•½ì¡°ê±´ ì—†ì´ ëª©ì í•¨ìˆ˜ $f(x)$ ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë¬¸ì œ

ğŸ“ ìˆ˜ì‹:
$$
\min_{\mathbf{x} \in \mathbb{R}^d} f(\mathbf{x})
$$

- **Local Minimum**: ì–´ë–¤ ë°˜ê²½ $r > 0$ ë‚´ì—ì„œ ê°€ì¥ ì‘ì€ ê°’
- **Global Minimum**: ì „ì²´ ì˜ì—­ì—ì„œ ê°€ì¥ ì‘ì€ ê°’

---

### âœ… Gradient & Hessian (ë¯¸ë¶„ ë„êµ¬)

- **Gradient** (1ì°¨ ë¯¸ë¶„ ë²¡í„°):
  $$
  \nabla f(\mathbf{x}) =
  \begin{bmatrix}
    \frac{\partial f}{\partial x_1} \\
    \vdots \\
    \frac{\partial f}{\partial x_d}
  \end{bmatrix}
  $$

- **Hessian** (2ì°¨ ë¯¸ë¶„ í–‰ë ¬):
  $$
  \nabla^2 f(\mathbf{x}) =
  \begin{bmatrix}
    \frac{\partial^2 f}{\partial x_1^2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_d} \\
    \vdots & \ddots & \vdots \\
    \frac{\partial^2 f}{\partial x_d \partial x_1} & \cdots & \frac{\partial^2 f}{\partial x_d^2}
  \end{bmatrix}
  $$

---

### âœ… First-order Necessary Condition (1ì°¨ ìµœì ì¡°ê±´)

- **ê°œë…**: ì–´ë–¤ ì  $\mathbf{x}^*$ ì´ local minimum ì´ë ¤ë©´,
  $$
  \nabla f(\mathbf{x}^*) = 0
  $$

- ì§ê´€: ê¸°ìš¸ê¸°(gradient)ê°€ 0ì´ë©´ ë” ë‚´ë ¤ê°ˆ ë°©í–¥ì´ ì—†ìŒ

---

### âœ… Second-order Necessary Condition (2ì°¨ ìµœì ì¡°ê±´)

- **ê°œë…**: $\nabla f(\mathbf{x}^*) = 0$ ì´ê³ , ê·¸ ì ì—ì„œ Hessianì´ **positive semi-definite** í•´ì•¼ í•¨
  $$
  \nabla^2 f(\mathbf{x}^*) \succeq 0
  $$

- ì˜ë¯¸: í•¨ìˆ˜ ê³¡ë¥ ì´ ìœ„ë¡œ ë³¼ë¡í•´ì•¼ local minimumì¼ ìˆ˜ ìˆìŒ

---

### âœ… Convexity (ë³¼ë¡ì„±)

- **Convex Function**: ì§ì„ ì´ í•­ìƒ í•¨ìˆ˜ ìœ„ì— ìˆì„ ë•Œ
  $$
  f(tx + (1 - t)y) \leq t f(x) + (1 - t)f(y), \quad \forall x, y, t \in [0,1]
  $$

- **Strictly Convex Function**: ìœ„ ì‹ì´ **ì—„ê²©í•œ ë¶€ë“±ì‹**ì¼ ë•Œ

---

### âœ… Property: Convexity â†’ Local min = Global min

- **ì •ë¦¬**: Convex í•¨ìˆ˜ì—ì„œëŠ” local minì´ë©´ ê³§ global minì´ë‹¤.

---

### âœ… First-Order Characterization of Convexity

- **ì •ì˜**: í•¨ìˆ˜ $f$ ê°€ ë¯¸ë¶„ ê°€ëŠ¥í•  ë•Œ, convex í•˜ë ¤ë©´
  $$
  f(y) \geq f(x) + \langle \nabla f(x), y - x \rangle
  $$

- ì´ ì‹ì€ ì§ì„ (ì ‘ì„ )ì´ í•­ìƒ í•¨ìˆ˜ ì•„ë˜ì— ìˆë‹¤ëŠ” ì˜ë¯¸

---

### âœ… Second-Order Characterization of Convexity

- **ì¡°ê±´**:
  - $f$ ê°€ $C^2$ í•¨ìˆ˜ (2ì°¨ ì—°ì† ë¯¸ë¶„ ê°€ëŠ¥)
  - $\nabla^2 f(x) \succeq 0$ (Hessianì´ positive semi-definite)

- **Strictly convex** í•˜ë ¤ë©´:
  $$
  \nabla^2 f(x) \succ 0 \quad \forall x
  $$

---

### âœ… Convexityê°€ ì¤‘ìš”í•œ ì´ìœ 

- **Convexí•œ ë¬¸ì œ**ëŠ”:
  - ì „ì—­ ìµœì†Œê°’ì´ ì¡´ì¬í•˜ê³ ,
  - gradient descent ê°™ì€ ë‹¨ìˆœí•œ ë°©ë²•ìœ¼ë¡œë„ í•´ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŒ
  - ì•Œê³ ë¦¬ì¦˜ì´ ë¹ ë¥´ê³  ì•ˆì •ì 

---

### âœ… ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ìì£¼ ë‚˜ì˜¤ëŠ” Convex í•¨ìˆ˜ ì˜ˆì‹œ

| ëª¨ë¸ | ëª©ì  í•¨ìˆ˜ |
|------|-----------|
| **Linear Regression** | $$ f(\mathbf{w}) = \|X\mathbf{w} - y\|^2 $$ |
| **Logistic Regression** | $$ f(\mathbf{w}) = \sum_{i=1}^n \log(1 + e^{-y_i \mathbf{w}^\top \mathbf{x}_i}) $$ |
| **SVM (Hinge Loss)** | $$ f(\mathbf{w}) = \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^n \max(0, 1 - y_i \mathbf{w}^\top \mathbf{x}_i) $$ |
| **Lasso** | $$ f(\mathbf{w}) = \|X\mathbf{w} - y\|^2 + \lambda \|\mathbf{w}\|_1 $$ |

---


# ğŸ” Session 3: Iterative Algorithms for Continuous Optimization

## ğŸŒ± ê°œë… íë¦„ ìš”ì•½
- ì•ì—ì„œ ì •ì˜í•œ ìµœì í™” ë¬¸ì œ  
  $$ \min_{\boldsymbol{\theta} \in \mathbb{R}^k} J(\boldsymbol{\theta}) $$
  ë¥¼ ì‹¤ì œë¡œ ì–´ë–»ê²Œ í’€ê¹Œ?

- ëª©ì  í•¨ìˆ˜ $J$ê°€ ì—°ì†ì´ê±°ë‚˜ ë¯¸ë¶„ ê°€ëŠ¥í•œ ê²½ìš°, **ë°˜ë³µì  ì•Œê³ ë¦¬ì¦˜**ì„ í†µí•´ í•´ë¥¼ ì°¾ëŠ”ë‹¤.
- ëŒ€í‘œì ì¸ ë°©ë²•ë“¤:
  - Gradient Descent (1ì°¨ ì •ë³´)
  - Subgradient Method (ë¹„ë¯¸ë¶„ í•¨ìˆ˜ í¬í•¨)
  - Stochastic Gradient Descent (ë°ì´í„°ê°€ ë§ì„ ë•Œ)
  - Newtonâ€™s Method (2ì°¨ ì •ë³´ í¬í•¨)
  - Majorize-Minimize (ë¹„ë¯¸ë¶„/ë³µì¡í•œ í•¨ìˆ˜ì—ë„ ê°€ëŠ¥)

---

## ğŸ”‘ ê°œë…ë³„ ì •ë¦¬ (ì´ë¦„ â†” ìˆ˜ì‹ ì—°ê²° ì¤‘ì‹¬)

### âœ… Directional Derivative (ë°©í–¥ ë„í•¨ìˆ˜)

- **ì •ì˜**: ì–´ë–¤ ë°©í–¥ $\mathbf{u}$ë¡œ $J(\boldsymbol{\theta})$ ê°€ ì–¼ë§ˆë‚˜ ì¦ê°€/ê°ì†Œí•˜ëŠ”ì§€

ğŸ“ ìˆ˜ì‹:
$$
D_{\mathbf{u}} J(\boldsymbol{\theta}_0) = \lim_{t \to 0} \frac{J(\boldsymbol{\theta}_0 + t \mathbf{u}) - J(\boldsymbol{\theta}_0)}{t}
= \langle \nabla J(\boldsymbol{\theta}_0), \mathbf{u} \rangle
$$

- **í•´ì„**: gradientì™€ $\mathbf{u}$ ì‚¬ì´ì˜ ë‚´ì ê°’

---

### âœ… Gradient Descent (ê²½ì‚¬í•˜ê°•ë²•)

- **í•µì‹¬ ì•„ì´ë””ì–´**: ê°€ì¥ ë¹ ë¥´ê²Œ í•¨ìˆ˜ê°’ì´ **ì¤„ì–´ë“œëŠ” ë°©í–¥**ìœ¼ë¡œ ì´ë™
- ë°©í–¥: $-\nabla J(\boldsymbol{\theta})$
- ë°˜ë³µì  ì—…ë°ì´íŠ¸:

ğŸ“ ìˆ˜ì‹:
$$
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \nabla J(\boldsymbol{\theta}_t)
$$

- $\eta$: **í•™ìŠµë¥  (step size)**

ğŸ’¡ ì¡°ê±´: $J$ê°€ ë¯¸ë¶„ ê°€ëŠ¥í•´ì•¼ í•¨

---

### âœ… Subgradient Method (ë¶€ë¶„ ê²½ì‚¬ë²•)

- **ë¬¸ì œ**: $J$ê°€ **ë¹„ë¯¸ë¶„**ì¼ ìˆ˜ ìˆë‹¤ (ì˜ˆ: hinge loss, L1 norm)
- **ëŒ€ì•ˆ**: gradient ëŒ€ì‹  **subgradient** ì‚¬ìš©

ğŸ“ ìˆ˜ì‹ (ì—…ë°ì´íŠ¸):
$$
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \cdot \mathbf{u}_t \quad \text{where } \mathbf{u}_t \in \partial J(\boldsymbol{\theta}_t)
$$

- $\partial J(\boldsymbol{\theta})$: í•´ë‹¹ ì§€ì ì˜ **subdifferential ì§‘í•©**

ğŸ’¬ subgradientëŠ” ì¼ë°˜ gradientì²˜ëŸ¼ í•œ ë°©í–¥ì´ ì•„ë‹Œ **ì—¬ëŸ¬ ê°€ëŠ¥ì„±ì˜ ì§‘í•©**

---

### âœ… Stochastic Gradient Descent (SGD)

- **ë¬¸ì œ**: $J(\boldsymbol{\theta}) = \sum_{i=1}^n J_i(\boldsymbol{\theta})$ ì™€ ê°™ì´ ë°ì´í„°ê°€ í´ ê²½ìš°
- **í•´ê²°ì±…**: ë§¤ ë°˜ë³µë§ˆë‹¤ ì „ì²´ê°€ ì•„ë‹Œ **ì¼ë¶€ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸**

ğŸ“ ìˆ˜ì‹:
$$
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \nabla J_i(\boldsymbol{\theta}_t)
$$

- ë˜ëŠ” mini-batchë¡œ:
$$
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \cdot \frac{1}{b} \sum_{i \in \Omega_t} \nabla J_i(\boldsymbol{\theta}_t)
$$

- $\Omega_t$: í¬ê¸° $b$ì˜ ë¬´ì‘ìœ„ ìƒ˜í”Œë§ëœ ë°ì´í„° ì¸ë±ìŠ¤

---

### âœ… Newton's Method (ë‰´í„´ ë°©ë²•)

- **Gradient Descent**ëŠ” 1ì°¨ ë¯¸ë¶„ ì •ë³´ë§Œ ì‚¬ìš© â†’ ëŠë¦´ ìˆ˜ ìˆìŒ
- Newton's methodëŠ” **Hessian (2ì°¨ ë¯¸ë¶„)**ì„ ì‚¬ìš©í•˜ì—¬ ë” ë¹ ë¥´ê²Œ ìˆ˜ë ´

ğŸ“ ìˆ˜ì‹:
$$
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \left( \nabla^2 J(\boldsymbol{\theta}_t) \right)^{-1} \nabla J(\boldsymbol{\theta}_t)
$$

- ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ì§€ë§Œ **Hessian ì—­í–‰ë ¬ ê³„ì‚°ì´ ë¹„ìŒˆ**
- ê³ ì°¨ì›ì—ì„œëŠ” **ë¹„íš¨ìœ¨ì **

---

### âœ… Majorize-Minimize (MM) Algorithm

- **ì•„ì´ë””ì–´**: ë³µì¡í•œ í•¨ìˆ˜ $J$ë¥¼ ë” ê°„ë‹¨í•œ í•¨ìˆ˜ $J_t$ë¡œ **upper bound**í•´ì„œ ë§¤ë²ˆ ìµœì†Œí™”

#### ì•Œê³ ë¦¬ì¦˜ êµ¬ì¡°:

1. Majorize step:
   - $J(\boldsymbol{\theta}) \leq J_t(\boldsymbol{\theta})$
   - $J(\boldsymbol{\theta}_t) = J_t(\boldsymbol{\theta}_t)$

2. Minimize step:
   - $$
     \boldsymbol{\theta}_{t+1} = \arg\min_{\boldsymbol{\theta}} J_t(\boldsymbol{\theta})
     $$

ğŸ’¬ ëŒ€í‘œ ì˜ˆì‹œ: EM algorithm, IRLS, robust regression ë“±

---

### âœ… Descent Property (í•˜ê°• ë³´ì¥)

- Gradient Descent:  
  $$
  J(\boldsymbol{\theta}_{t+1}) \leq J(\boldsymbol{\theta}_t)
  $$
  ë‹¨, $\eta$ ì‘ê³  $\nabla J \neq 0$ ì¼ ë•Œ

- MM Algorithm:
  $$
  J(\boldsymbol{\theta}_{t+1}) \leq J_t(\boldsymbol{\theta}_{t+1}) \leq J_t(\boldsymbol{\theta}_t) = J(\boldsymbol{\theta}_t)
  $$

---

## âœ… ê° ì•Œê³ ë¦¬ì¦˜ ìš”ì•½ ë¹„êµ

| ì•Œê³ ë¦¬ì¦˜ | ì¡°ê±´ | íŠ¹ì§• | ìˆ˜ì‹ |
|----------|------|------|------|
| Gradient Descent | $J$ ë¯¸ë¶„ ê°€ëŠ¥ | ë‹¨ìˆœ, ì•ˆì •ì  | $$ \boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \nabla J(\boldsymbol{\theta}_t) $$ |
| Subgradient | $J$ convex | ë¹„ë¯¸ë¶„ í•¨ìˆ˜ ëŒ€ì‘ | $$ \boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \cdot \mathbf{u}_t $$ |
| SGD | $J = \sum J_i$ | ë¹ ë¥´ê³  íš¨ìœ¨ì  | $$ \boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \nabla J_i(\boldsymbol{\theta}_t) $$ |
| Newton | $J$ twice diff. | ë¹ ë¥¸ ìˆ˜ë ´, ë¹„ìš© ë†’ìŒ | $$ \boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - (\nabla^2 J)^{-1} \nabla J $$ |
| MM | ì¼ë°˜ í•¨ìˆ˜ ê°€ëŠ¥ | gradient ì—†ì´ë„ ìµœì í™” ê°€ëŠ¥ | $$ \boldsymbol{\theta}_{t+1} = \arg\min J_t(\boldsymbol{\theta}) $$ |



# ğŸ‘£ Session 4: Nearest Neighbor Classifier

## ğŸŒ± ê°œë… íë¦„ ìš”ì•½
- Nearest NeighborëŠ” **í•™ìŠµ(training)** ì—†ì´, **ê±°ë¦¬ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡**í•˜ëŠ” ë°©ë²•ì´ë‹¤.
- í•µì‹¬ ì•„ì´ë””ì–´: **ì…ë ¥ ë²¡í„° $\mathbf{x}$ì™€ ê°€ì¥ ê°€ê¹Œìš´ í›ˆë ¨ ë°ì´í„°ì˜ ë ˆì´ë¸”ì„ ë”°ë¼ê°„ë‹¤.**
- ì¼ë°˜í™”ëœ ë²„ì „ì¸ **k-NN**ì€ ê°€ì¥ ê°€ê¹Œìš´ kê°œì˜ ì´ì›ƒì˜ **ë‹¤ìˆ˜ê²° íˆ¬í‘œ**ë¡œ ê²°ì •í•œë‹¤.
- k ê°’ì— ë”°ë¼ **bias-variance trade-off**ê°€ ë°œìƒí•œë‹¤.
- ì„±ëŠ¥ ì¸¡ì •ì€ **test error**ë¥¼ í†µí•´ í•˜ê³ , ìµœì ì˜ këŠ” **validation set**ìœ¼ë¡œ ì¡°ì •í•œë‹¤.

---

## ğŸ”‘ ê°œë…ë³„ ì •ë¦¬ (ì´ë¦„ â†” ìˆ˜ì‹ ì—°ê²°)

### âœ… Nearest Neighbor Classifier

- **ë¬¸ì œ ì„¸íŒ…**:  
  í•™ìŠµ ë°ì´í„° $ \{(\mathbf{x}_1, y_1), \dots, (\mathbf{x}_n, y_n)\} $  
  - $\mathbf{x}_i \in \mathbb{R}^d$ (ì…ë ¥ ë²¡í„°)
  - $y_i \in \{-1, +1\}$ (ì´ì§„ ë¶„ë¥˜ ì˜ˆì‹œ)

- **ì˜ˆì¸¡ ë°©ë²•**: ì…ë ¥ $\mathbf{x}$ì— ëŒ€í•´ ê°€ì¥ ê°€ê¹Œìš´ $\mathbf{x}_i$ ë¥¼ ì°¾ì•„, ê·¸ì— í•´ë‹¹í•˜ëŠ” $y_i$ ë¥¼ ì˜ˆì¸¡ìœ¼ë¡œ ì‚¬ìš©

ğŸ“ ìˆ˜ì‹:
$$
\hat{y} = y_{i^*}, \quad \text{where } i^* = \arg\min_i \|\mathbf{x} - \mathbf{x}_i\|
$$

ğŸ’¡ í•™ìŠµ X, ì €ì¥ë§Œ í•œë‹¤.

---

### âœ… k-Nearest Neighbors (k-NN)

- **ì •ì˜**: $\mathbf{x}$ì™€ ê°€ê¹Œìš´ **kê°œì˜ ì´ì›ƒ**ì˜ ë ˆì´ë¸”ì„ ê°€ì ¸ì™€ **ë‹¤ìˆ˜ê²°**ë¡œ ê²°ì •
- **këŠ” í™€ìˆ˜**ê°€ ì¼ë°˜ì  (ë™ë¥  ë°©ì§€)

ğŸ“ ìˆ˜ì‹:
$$
\hat{y} = \operatorname{sign} \left( \sum_{j=1}^{k} y_{(j)} \right)
$$

- ì—¬ê¸°ì„œ $y_{(j)}$ëŠ” $\mathbf{x}$ì™€ ê°€ì¥ ê°€ê¹Œìš´ $k$ê°œì˜ $\mathbf{x}_i$ ì¤‘ $j$ë²ˆì§¸ ì´ì›ƒì˜ ë ˆì´ë¸”

---

### âœ… Bias-Variance Trade-off in k-NN

| k ê°’ | Bias | Variance | íŠ¹ì„± |
|------|------|----------|------|
| **ì‘ì„ìˆ˜ë¡ (ì˜ˆ: k=1)** | ë‚®ì€ bias | ë†’ì€ variance | ë³µì¡í•œ íŒ¨í„´ë„ í¬ì°©, ë…¸ì´ì¦ˆ ë¯¼ê° |
| **í´ìˆ˜ë¡ (ì˜ˆ: k=15)** | ë†’ì€ bias | ë‚®ì€ variance | ë” ì•ˆì •ì , êµ­ì†Œ ì •ë³´ ë†“ì¹  ìˆ˜ ìˆìŒ |

---

### âœ… Test Error (í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜)

- ì‹¤ì œ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ $m$ê°œì— ëŒ€í•´, ì˜ˆì¸¡ì´ í‹€ë¦° ë¹„ìœ¨

ğŸ“ ìˆ˜ì‹:
$$
\text{Test Error} = \frac{1}{m} \sum_{i=1}^{m} \mathbf{1} \{ f_k(\mathbf{x}_{n+i}) \ne y_{n+i} \}
$$

- $\mathbf{1}\{\cdot\}$: indicator function (ì°¸ì´ë©´ 1, ê±°ì§“ì´ë©´ 0)

---

### âœ… Training Error vs. Test Error

- **Training Error**:
  - k=1ì´ë©´ í•­ìƒ 0 (ìê¸° ìì‹ ì´ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒì´ë¯€ë¡œ)
  - í•˜ì§€ë§Œ **Overfitting** ê°€ëŠ¥ì„±

- **Test Error**:
  - ì¼ë°˜í™” ëŠ¥ë ¥ ì¸¡ì •
  - **Validation Set**ì„ ì´ìš©í•´ **ìµœì  k** ê²°ì •

---

### âœ… ê¸°íƒ€ íŠ¹ì„± ì •ë¦¬

| í•­ëª© | ë‚´ìš© |
|------|------|
| **í•™ìŠµ ê³¼ì •** | ì—†ìŒ (Non-parametric) |
| **ê³„ì‚° ë¹„ìš©** | ì˜ˆì¸¡ ì‹œ ê±°ë¦¬ ê³„ì‚° í•„ìš” â†’ ëŠë¦´ ìˆ˜ ìˆìŒ |
| **ëª¨ë¸ ì €ì¥** | ì „ì²´ ë°ì´í„°ë¥¼ ì €ì¥í•´ì•¼ í•¨ |
| **ì¥ì ** | ì§ê´€ì , ë¶„ë¥˜ ì„±ëŠ¥ ì¢‹ìŒ |
| **ë‹¨ì ** | ê³ ì°¨ì›ì—ì„œ ì„±ëŠ¥ ì €í•˜ (curse of dimensionality) |

---