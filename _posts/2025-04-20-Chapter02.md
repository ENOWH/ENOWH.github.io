---
title: "Chapter02"
excerpt: "머신러닝 Chap02 전체 흐름"

categories:
  - MachineLearning
tags:
  - [Machine Learning, 머신러닝]

permalink: /categories/MachineLearning/Chapter02_MachineLearning

toc: true
toc_sticky: true
use_math: true

date: 2025-04-20
last_modified_at: 2025-04-20
---


# 📘 Session 1: Bayes Classifiers

## 📌 세션 개요

이 세션은 머신러닝 분류 문제를 **확률론적 관점**에서 바라보는 접근 방식인  
**베이즈 분류기(Bayes Classifier)**에 대해 다룬다.  
확률 분포, 사후확률(posterior), 사전확률(prior) 등의 개념을 기반으로  
**최적의 분류기**란 무엇인지 이론적으로 도출하며,  
결국 실전에서는 이를 근사하는 **Plug-in Classifier**를 어떻게 사용하는지를 학습한다.

---

## ✅ 핵심 개념 정리

### 🔹 Classification as Function Approximation
- **정의**:
  $$
  f: \mathcal{X} \rightarrow \mathcal{Y}
  $$
  where  
  $begin:math:text$ \\mathcal{X} $end:math:text$: feature/input space  
  $begin:math:text$ \\mathcal{Y} $end:math:text$: label space (e.g., $begin:math:text$ \\{1, ..., K\\} $end:math:text$)

---

### 🔹 Probabilistic Setting
- **Joint distribution** over input and label:
  $$
  p(x, y)
  $$
- 분류는 사실상 **사후확률 $begin:math:text$ p(y|x) $end:math:text$** 를 기반으로 예측하는 문제로 볼 수 있다.

---

### 🔹 Bayes Rule (베이즈 정리)
- **수식**:
  $$
  p(y|x) = \frac{p(x|y) \cdot p(y)}{p(x)}
  $$
- **설명**:  
  $begin:math:text$ p(y) $end:math:text$: prior  
  $begin:math:text$ p(x|y) $end:math:text$: class-conditional density  
  $begin:math:text$ p(y|x) $end:math:text$: posterior  
  $begin:math:text$ p(x) = \\sum_{y'} p(x|y') p(y') $end:math:text$

---

### 🔹 Bayes Optimal Classifier (BOC)
- **정의**:
  $$
  h^*(x) = \arg\max_y \, p(y|x)
  $$
- **설명**:  
  사후확률이 가장 높은 클래스를 선택하는 분류기  
  가장 낮은 오류율(Bayes risk)을 가지는 **이론적 최적 분류기**

---

### 🔹 Bayes Risk
- **Expected 0-1 Loss**:
  $$
  R(h) = \mathbb{E}_{(x, y) \sim p(x, y)} \left[ \mathbf{1}(h(x) \ne y) \right]
  $$
- **설명**:  
  Bayes classifier $begin:math:text$ h^* $end:math:text$ 는 모든 분류기 중 $begin:math:text$ R(h) $end:math:text$를 최소화함

---

### 🔹 Plug-in Classifier
- **아이디어**:  
  실제 $begin:math:text$ p(y|x) $end:math:text$를 알 수 없기 때문에, 추정된 분포 $begin:math:text$ \\hat{p}(y|x) $end:math:text$를 사용해 근사 분류기를 만든다.
- **수식**:
  $$
  \hat{h}(x) = \arg\max_y \, \hat{p}(y|x)
  $$
- **설명**:  
  실제 머신러닝 모델 대부분이 이 방식을 따름  
  (예: Logistic Regression, Naive Bayes 등)

---

### 🔹 Classification Error
- Bayes Classifier는 이론적으로 달성 가능한 **최소 오류율**을 제공한다.
- 어떤 학습 알고리즘도 이 오류보다 작을 수는 없다.
- 이 오류율을 **Bayes Error**라고 한다.

---

## 🔁 세션 흐름 요약

1. 분류 문제를 확률 분포 $begin:math:text$ p(x, y) $end:math:text$ 기반으로 정의함  
2. 베이즈 정리를 통해 **사후확률 $begin:math:text$ p(y|x) $end:math:text$** 을 계산  
3. 사후확률이 가장 높은 클래스를 선택하는 **Bayes Optimal Classifier** 도입  
4. 실제 분포를 알 수 없으므로 **Plug-in Classifier**로 근사  
5. 모든 분류기의 기준점이 되는 **Bayes Risk** 개념 정립

---

> 🧠 이 세션은 머신러닝의 분류 문제를 **확률론적 최적화 문제**로 바라보는 관점을 제공하며,  
> 이후 등장할 LDA, Naïve Bayes, Logistic Regression 등의 이론적 기반을 다진다.