---
title: "Chapter02"
excerpt: "머신러닝 Chap02 전체 흐름"

categories:
  - MachineLearning
tags:
  - [Machine Learning, 머신러닝]

permalink: /categories/MachineLearning/Chapter02_MachineLearning

toc: true
toc_sticky: true

date: 2025-04-20
last_modified_at: 2025-04-20
---

# 📘 Chapter 2: Classification with Linear Models (`slide_ch2_modified.pdf`)

## 📑 세션별 구성 및 주요 주제 정리

---

### 🟦 Session 1: Bayes Classifiers

- **목표**: 오류 확률 $R(f) = \Pr(f(\mathbf{X}) \ne Y)$을 최소화하는 이상적인 분류기 정의
- **Bayes classifier**:
  $$
  f^*(\mathbf{x}) = \arg\max_k \Pr(Y = k \mid \mathbf{X} = \mathbf{x}) = \arg\max_k \pi_k \cdot g_k(\mathbf{x})
  $$
- 실제에서는 분포를 알 수 없기 때문에 **Plug-in classifier** 사용
- 이 세션은 이후의 모든 분류기(LDA, Naïve Bayes, Logistic Regression 등)의 **이론적 토대**

---

### 🟦 Session 2: Linear Discriminant Analysis (LDA)

- **가정**: 클래스별로 공통 공분산 $\Sigma$를 갖는 Gaussian 분포
- LDA는 Bayes classifier의 근사로, 결정 경계가 **선형**
- 파라미터 추정:
  - 클래스 평균 $\mu_k$, 공통 공분산 $\Sigma$, prior $\pi_k$
- 분류기 형태 (binary):
  $$
  \hat{f}(\mathbf{x}) = \operatorname{sign}(\mathbf{w}^\top \mathbf{x} + b)
  $$
- **Multiclass LDA** 역시 동일한 구조로 확장 가능
- Mahalanobis 거리 관점으로도 해석 가능

---

### 🟦 Session 3: Naïve Bayes Classifier

- **조건부 독립 가정**: 클래스 $Y$가 주어지면 각 feature $X_j$는 서로 독립
- Likelihood 계산:
  $$
  \Pr(\mathbf{x} \mid Y=k) = \prod_{j=1}^d \Pr(x_j \mid Y=k)
  $$
- 분류기:
  $$
  \hat{f}(\mathbf{x}) = \arg\max_k \pi_k \cdot \prod_j g_{kj}(x_j)
  $$
- **Laplace smoothing** 으로 zero probability 문제 해결
- 텍스트 분류(Bag-of-Words), 스팸 필터링 등에서 활용

---

### 🟦 Session 4: Logistic Regression

- Posterior 확률 직접 모델링:
  $$
  \eta(\mathbf{x}) = \Pr(Y=1 \mid \mathbf{x}) = \frac{1}{1 + \exp(-\mathbf{w}^\top \mathbf{x} - b)}
  $$
- 결정 경계: 선형
  $$
  \hat{y} = \begin{cases} 1 & \text{if } \eta(\mathbf{x}) \geq 0.5 \\ 0 & \text{otherwise} \end{cases}
  $$
- 파라미터 학습: **Maximum Likelihood Estimation**
- 정규화 (Ridge):
  $$
  \hat{\theta} = \arg\min_\theta \left[ -\ell(\theta) + \lambda \|\theta\|^2 \right]
  $$
- 확률 예측까지 가능한 선형 분류기

---

### 🟦 Session 5: Separating Hyperplanes & SVM

- 확률 없이 **기하학적으로 데이터 분리**
- Linear classifier: $f(\mathbf{x}) = \operatorname{sign}(\mathbf{w}^\top \mathbf{x} + b)$
- Margin 정의:
  $$
  \rho = \min_i \frac{y_i(\mathbf{w}^\top \mathbf{x}_i + b)}{\|\mathbf{w}\|}
  $$
- 최적화 문제 (Hard-margin):
  $$
  \min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2 \quad \text{s.t. } y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1
  $$
- Soft-margin (Slack 변수 $\xi_i$ 도입):
  $$
  \min_{\mathbf{w}, b, \xi} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum \xi_i
  $$

---

### 🟦 Session 6: Empirical Risk Minimization (ERM)

- 머신러닝 모델 설계를 위한 **통일된 이론적 프레임워크**
- 경험 위험 최소화:
  $$
  \hat{R}_L(f) = \frac{1}{n} \sum_{i=1}^n L(y_i, f(\mathbf{x}_i))
  $$
- 정규화 추가:
  $$
  \min_f \hat{R}_L(f) + \lambda \cdot \Omega(f)
  $$
- Surrogate loss (e.g., Logistic, Hinge)로 0-1 loss 근사
- 다양한 모델이 ERM 구조에 포함됨:

| 모델 | Loss | 정규화 |
|------|------|--------|
| Linear Regression | $(y - f(x))^2$ | X |
| Ridge Regression | $(y - f(x))^2$ | $\lambda \|w\|^2$ |
| Logistic Regression | $\log(1 + e^{-yt})$ | $\lambda \|w\|^2$ |
| SVM | $\max(0, 1 - yt)$ | $\lambda \|w\|^2$ |

---

## 🧭 전체 흐름 요약

1. **Bayes Optimal 분류기** 이해 → 실제 구현 어려움
2. 이를 근사하는 모델들:
   - LDA: 공통 Gaussian 가정 + 선형 결정 경계
   - Naïve Bayes: 조건부 독립 가정
   - Logistic Regression: Posterior 확률 직접 모델링
3. **확률 없이 분류**하는 기하학적 모델: SVM
4. 이 모든 모델은 결국 **ERM 프레임**으로 통합됨!



# 📚 Session 1: Bayes Classifiers

## 🌱 개념 흐름 요약
- 분류 문제를 **확률적 관점**으로 접근
- 목표는: **오류 확률을 최소화하는 분류기**를 찾는 것
- 이론적으로 가장 이상적인 분류기는 **Bayes Classifier**
- 실제 분류기에서는 **Bayes Classifier를 근사**하는 방법들을 사용 (ex. LDA, Naïve Bayes, Logistic Regression)

---

## 🔑 개념별 정리 (개념 ↔ 수식 연결)

### ✅ 확률 모델 기반 분류 문제 세팅

- Feature vector:  
  $$ \mathbf{X} = [X_1, \ldots, X_d]^\top \in \mathbb{R}^d $$
- Class label:  
  $$ Y \in \{1, \ldots, K\} $$
- **가정**: $(\mathbf{X}, Y)$는 공동 분포를 따르는 확률 변수 쌍

📐 두 가지 관점의 확률 분해:
1. **Generative View** (joint를 prior × likelihood로 분해):
   $$
   p(\mathbf{X}, Y) = p(Y) \cdot p(\mathbf{X} \mid Y)
   $$
2. **Discriminative View** (posterior 중심):
   $$
   p(\mathbf{X}, Y) = p(\mathbf{X}) \cdot p(Y \mid \mathbf{X})
   $$

---

### ✅ Posterior, Prior, Likelihood (용어 정리)

| 개념 | 수식 | 설명 |
|------|------|------|
| **Prior** | $$ \pi_k = \Pr(Y = k) $$ | 클래스별 사전 확률 |
| **Likelihood** | $$ g_k(\mathbf{x}) = \Pr(\mathbf{X} = \mathbf{x} \mid Y = k) $$ | 클래스별 조건부 확률 밀도 |
| **Posterior** | $$ \eta_k(\mathbf{x}) = \Pr(Y = k \mid \mathbf{X} = \mathbf{x}) $$ | 입력 $\mathbf{x}$에 대한 클래스 확률 |

---

### ✅ Bayes Classifier 정의

- **목표**: 오류 확률 최소화
- **위험 함수** (error 확률):
  $$
  R(f) = \Pr(f(\mathbf{X}) \neq Y)
  $$

- **Bayes Risk**:
  $$
  R^* = \min_f R(f)
  $$

- **Bayes Classifier 수식**:

📐 ① Posterior 기반 표현:
$$
f^*(\mathbf{x}) = \arg\max_{k} \eta_k(\mathbf{x})
$$

📐 ② Prior × Likelihood 기반 표현:
$$
f^*(\mathbf{x}) = \arg\max_{k} \pi_k \cdot g_k(\mathbf{x})
$$

---

### ✅ 왜 위 식이 Bayes Optimal인지 (직관)

- $R(f)$를 전개해보면:
  $$
  R(f) = \mathbb{E}_{\mathbf{X}} \left[ 1 - \Pr(Y = f(\mathbf{X}) \mid \mathbf{X}) \right]
  $$
- 이를 최소화하려면:  
  $\Pr(Y = f(\mathbf{x}) \mid \mathbf{x})$를 **최대로** 만들어야 하므로,  
  가장 확률이 높은 클래스를 선택하는 것이 최적.

---

### ✅ Plug-in Classifier

- 실제에선 $(\mathbf{X}, Y)$의 분포를 **알 수 없음**
- 따라서 학습 데이터를 통해 $\eta_k(\mathbf{x})$, $g_k(\mathbf{x})$, $\pi_k$ 등을 추정하고,  
  이를 Bayes Classifier 공식에 **"끼워 넣는다"** → **Plug-in Classifier**

---

### ✅ 대표적인 Plug-in 방법들

| 방법 | 추정 방식 | 주요 가정 |
|------|-----------|----------|
| **LDA** | $\pi_k$, $g_k(\mathbf{x})$ 추정 | $g_k$는 공통 공분산 $\Sigma$ 가진 Gaussian |
| **Naïve Bayes** | $\pi_k$, $g_k$ 추정 | feature들이 클래스 주어진 조건에서 독립 |
| **Logistic Regression** | $\eta_k(\mathbf{x})$ 직접 추정 | posterior를 로지스틱 함수로 가정 |

---

## ✅ 핵심 요약

- **Bayes Classifier**는 이론적으로 오류 확률이 최소인 최적 분류기
- 하지만 실제 데이터에서는 분포를 모름 → **Plug-in** 방식으로 근사
- 이후 배울 LDA, Naïve Bayes, Logistic Regression은 모두 이 **Bayes Classifier의 근사 버전들**임!


# 🧮 Session 2: Linear Discriminant Analysis (LDA)

## 🌱 개념 흐름 요약

- LDA는 **Bayes classifier**를 근사하는 방법 중 하나이다.
- 각 클래스의 분포를 **공통 공분산 $\Sigma$를 가진 Gaussian 분포**로 가정한다.
- 이 가정 덕분에 **클래스 결정 경계가 선형**이 된다.
- Posterior (사후 확률)을 직접 구하는 대신, Prior × Likelihood 형태로 풀어냄

---

## 🔑 개념별 정리 (개념 ↔ 수식 연결)

### ✅ LDA의 핵심 가정

- 클래스별 조건부 분포:
  $$
  \mathbf{X} \mid Y = k \sim \mathcal{N}(\boldsymbol{\mu}_k, \Sigma)
  $$

- 모든 클래스가 **같은 공분산 행렬 $\Sigma$를 공유**한다고 가정
- $\pi_k = \Pr(Y = k)$: 클래스 k의 prior 확률

---

### ✅ LDA에서의 Bayes 분류기 수식

- 일반 Bayes classifier:
  $$
  f^*(\mathbf{x}) = \arg\max_k \pi_k \cdot g_k(\mathbf{x})
  $$

- LDA에서는 각 $g_k(\mathbf{x})$가 Gaussian 이므로:
  $$
  g_k(\mathbf{x}) = \phi(\mathbf{x}; \boldsymbol{\mu}_k, \Sigma)
  $$

📐 log-likelihood로 바꾸면:
$$
f(\mathbf{x}) = \arg\max_k \left[ \log \pi_k - \frac{1}{2} (\mathbf{x} - \boldsymbol{\mu}_k)^T \Sigma^{-1} (\mathbf{x} - \boldsymbol{\mu}_k) \right]
$$

- 정리하면:
  $$
  f(\mathbf{x}) = \arg\max_k \mathbf{w}_k^T \mathbf{x} + b_k
  $$

---

### ✅ 파라미터 추정 (MLE 기반)

- 학습 데이터로부터 추정:

1. Prior:
   $$
   \hat{\pi}_k = \frac{n_k}{n}
   $$

2. 클래스별 평균:
   $$
   \hat{\boldsymbol{\mu}}_k = \frac{1}{n_k} \sum_{i: y_i = k} \mathbf{x}_i
   $$

3. 공통 공분산:
   $$
   \hat{\Sigma} = \frac{1}{n} \sum_{i=1}^n (\mathbf{x}_i - \hat{\boldsymbol{\mu}}_{y_i})(\mathbf{x}_i - \hat{\boldsymbol{\mu}}_{y_i})^T
   $$

---

### ✅ Binary Case에서의 결정 함수

- 클래스 라벨: $Y \in \{-1, +1\}$
- 결정 함수는 선형 함수의 부호:
  $$
  \hat{f}(\mathbf{x}) = \operatorname{sign}(\mathbf{w}^T \mathbf{x} + b)
  $$

- 여기서:
  $$
  \mathbf{w} = \hat{\Sigma}^{-1}(\hat{\boldsymbol{\mu}}_1 - \hat{\boldsymbol{\mu}}_{-1})
  $$

  $$
  b = \log \frac{\hat{\pi}_1}{\hat{\pi}_{-1}} + \frac{1}{2} \left( \hat{\boldsymbol{\mu}}_{-1}^T \hat{\Sigma}^{-1} \hat{\boldsymbol{\mu}}_{-1} - \hat{\boldsymbol{\mu}}_1^T \hat{\Sigma}^{-1} \hat{\boldsymbol{\mu}}_1 \right)
  $$

---

### ✅ Multiclass Case (K > 2)

- 결정 함수:
  $$
  \hat{f}(\mathbf{x}) = \arg\max_k \mathbf{w}_k^T \mathbf{x} + b_k
  $$

- 각 클래스에 대해:
  $$
  \mathbf{w}_k = \hat{\Sigma}^{-1} \hat{\boldsymbol{\mu}}_k
  $$
  $$
  b_k = \log \hat{\pi}_k - \frac{1}{2} \hat{\boldsymbol{\mu}}_k^T \hat{\Sigma}^{-1} \hat{\boldsymbol{\mu}}_k
  $$

---

### ✅ Mahalanobis Distance 해석

- LDA는 Mahalanobis 거리를 기반으로 분류함
- 클래스 중심 $\mu_k$ 와의 거리:
  $$
  D_{\Sigma^{-1}}(\mathbf{x}, \mu_k) = \sqrt{(\mathbf{x} - \mu_k)^T \Sigma^{-1} (\mathbf{x} - \mu_k)}
  $$

- LDA의 결정은 결국 Mahalanobis 거리 기준으로 가장 가까운 클래스를 선택하는 것과 유사함  
  (단, prior $\pi_k$ 도 고려됨)

---

## ✅ 핵심 요약

- LDA는 Bayes classifier의 **선형 근사 버전**
- 클래스별 Gaussian 분포 + 공통 공분산이라는 가정을 바탕으로  
  결정 경계를 **선형 형태**로 유도
- 파라미터는 MLE로 추정하고, Posterior 대신 log-likelihood로 비교
- Multiclass, Binary 모두 일관된 구조로 확장 가능


# 🤖 Session 3: Naïve Bayes Classifier

## 🌱 개념 흐름 요약

- Naïve Bayes는 Bayes classifier의 간단한 근사 방법
- 핵심 가정: **클래스가 주어졌을 때, feature 간은 조건부 독립이다**
- 이 가정 덕분에 고차원 feature space에서도 **likelihood 계산이 쉬워짐**
- 주로 **문서 분류(text classification)** 같은 분야에서 효과적으로 사용됨

---

## 🔑 개념별 정리 (개념 ↔ 수식 연결)

### ✅ Naïve Bayes의 조건부 독립 가정

- 입력 벡터:
  $$
  \mathbf{X} = [X_1, X_2, \dots, X_d]^T
  $$

- **가정**: 클래스 $Y = k$ 가 주어졌을 때, 각 feature는 서로 독립
  $$
  \Pr(\mathbf{X} = \mathbf{x} \mid Y = k) = \prod_{j=1}^d \Pr(X_j = x_j \mid Y = k)
  $$

- 이 식 덕분에 계산 복잡도가 **지수에서 선형으로** 감소함!

---

### ✅ Naïve Bayes 분류기 수식

- Posterior:
  $$
  \eta_k(\mathbf{x}) = \Pr(Y = k \mid \mathbf{X} = \mathbf{x}) \propto \pi_k \cdot \prod_{j=1}^d g_{kj}(x_j)
  $$

- 분류기:
  $$
  \hat{f}(\mathbf{x}) = \arg\max_k \pi_k \cdot \prod_{j=1}^d g_{kj}(x_j)
  $$

📌 여기서:
- $\pi_k$: 클래스 $k$의 prior 확률 추정
- $g_{kj}(x_j) = \Pr(X_j = x_j \mid Y = k)$: 각 feature별 조건부 분포

---

### ✅ 텍스트 분류 예시 (Bag-of-Words)

- 각 문서를 **단어의 등장 횟수 벡터**로 표현:
  $$
  \mathbf{x} = [x_1, x_2, \ldots, x_d]^T
  $$

- 각 단어 $j$가 클래스 $k$에서 등장할 확률을 추정:
  $$
  \hat{g}_{kj}(l) = \frac{n_{kjl}}{n_k}
  $$

  - $n_{kjl}$: 클래스 $k$에서 단어 $j$가 $l$번 등장한 문서 수
  - $n_k$: 클래스 $k$에 속한 전체 문서 수

---

### ✅ Prior 확률 추정

- 클래스 $k$의 prior 확률은 다음과 같이 추정:
  $$
  \hat{\pi}_k = \frac{n_k}{n}
  $$

---

### ✅ Additive (Laplace) Smoothing

- 문제: 학습 데이터에 없는 단어 → 확률 0 → 전체 확률이 0이 되는 문제 발생
- 해결: 모든 경우에 작은 값을 더함

📐 수식:
$$
\hat{g}_{kj}(l) = \frac{n_{kjl} + \alpha}{n_k + \alpha L}
$$

- $\alpha > 0$: smoothing parameter (보통 1 사용)
- $L$: 가능한 값의 개수 (예: 단어 등장 횟수 경우의 수)

---

### ✅ 예제 요약

- 훈련 데이터:
  - Y = {Sunny, Rainy}
  - X1 = Umbrella, X2 = Sunglasses
- Step:
  1. Prior 계산
  2. 각 feature의 조건부 확률 계산
  3. 새로운 샘플에 대한 posterior 계산
  4. 더 높은 posterior를 갖는 클래스로 예측

- 예측 수식:
  $$
  \Pr(Y=k \mid \mathbf{x}) \propto \hat{\pi}_k \cdot \prod_{j=1}^d \hat{g}_{kj}(x_j)
  $$

---

## ✅ 핵심 요약

- Naïve Bayes는 **조건부 독립**이라는 단순한 가정 하에 Bayes Classifier를 근사
- 계산이 매우 빠르고 간단하며, 고차원에서도 효과적
- Laplace Smoothing을 통해 **zero probability 문제**를 해결
- 텍스트 분류, 이메일 스팸 필터링 등에서 많이 쓰임



# 📈 Session 4: Logistic Regression

## 🌱 개념 흐름 요약

- **로지스틱 회귀는 posterior 확률 $\Pr(Y=1|\mathbf{x})$를 직접 모델링**
- 확률 모델은 **sigmoid(시그모이드) 함수**를 사용
- 분류 기준은: 확률이 0.5 이상이면 1, 아니면 0
- 파라미터는 **최대우도추정 (MLE)** 을 통해 학습
- 과적합 방지를 위해 **정규화 (regularization)** 를 추가할 수 있음

---

## 🔑 개념별 정리 (개념 ↔ 수식 연결)

### ✅ 분류 문제 세팅

- Binary classification:
  $$
  Y \in \{0, 1\},\quad \mathbf{x} \in \mathbb{R}^d
  $$

- 목표: 주어진 $\mathbf{x}$에 대해 $Y=1$일 확률 예측
  $$
  \eta(\mathbf{x}) := \Pr(Y = 1 \mid \mathbf{x})
  $$

---

### ✅ 로지스틱 모델 정의

- Posterior 확률을 sigmoid 함수로 표현:
  $$
  \eta(\mathbf{x}) = \frac{1}{1 + \exp(-(\mathbf{w}^\top \mathbf{x} + b))}
  $$

- 여기서:
  - $\mathbf{w} \in \mathbb{R}^d$: weight vector
  - $b \in \mathbb{R}$: bias (절편)
  - $\sigma(z) = \frac{1}{1 + e^{-z}}$: sigmoid 함수

---

### ✅ 분류 기준 (결정 함수)

- 예측 결과:
  $$
  \hat{y} = \begin{cases}
  1 & \text{if } \eta(\mathbf{x}) \geq \frac{1}{2} \\
  0 & \text{otherwise}
  \end{cases}
  $$

- 선형 결정 경계:
  $$
  \eta(\mathbf{x}) \geq \frac{1}{2} \iff \mathbf{w}^\top \mathbf{x} + b \geq 0
  $$

- 즉, logistic regression은 **linear classifier**이다!

---

### ✅ 파라미터 학습 (Maximum Likelihood Estimation)

- 각 데이터는 다음의 Bernoulli 분포를 따름:
  $$
  \Pr(y_i \mid \mathbf{x}_i; \theta) = \eta(\mathbf{x}_i)^{y_i} (1 - \eta(\mathbf{x}_i))^{1 - y_i}
  $$

- 전체 데이터 likelihood:
  $$
  L(\theta) = \prod_{i=1}^n \eta(\mathbf{x}_i)^{y_i} (1 - \eta(\mathbf{x}_i))^{1 - y_i}
  $$

- log-likelihood:
  $$
  \ell(\theta) = \sum_{i=1}^n \left[ y_i \log \eta(\mathbf{x}_i) + (1 - y_i) \log (1 - \eta(\mathbf{x}_i)) \right]
  $$

---

### ✅ 정규화 (Regularization)

- 과적합 방지를 위해 가중치에 패널티 부여:
  $$
  J(\theta) = -\ell(\theta) + \lambda \|\theta\|^2
  $$

- $\lambda$: 정규화 강도 조절 (크면 모델이 단순해짐)

- 최종 최적화 문제:
  $$
  \hat{\theta} = \arg\min_\theta \left[ -\ell(\theta) + \lambda \|\theta\|^2 \right]
  $$

---

### ✅ 결정 경계와 확률 추정

- 분류 결과는 선형 결정 경계를 따르지만,
- 출력 값은 **확률**이기 때문에 **의사결정 임계값 조절** 가능
  $$
  \hat{\eta}(\mathbf{x}) = \frac{1}{1 + \exp(-(\hat{\mathbf{w}}^\top \mathbf{x} + \hat{b}))}
  $$

---

## ✅ 핵심 요약

- **로지스틱 회귀는 확률 분류기**
  - $\Pr(Y = 1 \mid \mathbf{x})$ 자체를 모델링
- **결정 경계는 선형**
  - sigmoid 함수의 중심인 $0.5$를 기준으로 분류
- **파라미터 학습은 최대우도추정(MLE)** 으로 수행
- **정규화**를 통해 모델의 복잡도 조절 가능
- 분류 + 확률 추정 모두 가능한 실용적인 모델!



# 🧱 Session 5: Separating Hyperplanes & SVM

## 🌱 개념 흐름 요약

- 앞선 모델들은 확률 분포 기반 (Bayes, Logistic 등)
- 이번에는 분포 추정 없이 **기하학적으로 분리하는 경계**를 찾는 방법
- 목표: **가장 margin이 큰 hyperplane을 찾는 것**
- 선형 분리 불가능한 경우를 위해 **Soft Margin + Slack 변수**를 도입
- 이 개념은 **SVM (Support Vector Machine)** 으로 발전

---

## 🔑 개념별 정리 (개념 ↔ 수식 연결)

### ✅ Hyperplane 정의

- $d$차원 공간에서의 hyperplane:
  $$
  H = \left\{ \mathbf{x} \in \mathbb{R}^d \mid \mathbf{w}^\top \mathbf{x} + b = 0 \right\}
  $$

- $\mathbf{w}$: normal vector (평면에 수직인 벡터)
- $b$: offset (절편)

---

### ✅ 선형 분리 조건

- 이진 분류: $Y \in \{-1, +1\}$
- 어떤 hyperplane이 데이터를 분리한다는 조건:
  $$
  y_i (\mathbf{w}^\top \mathbf{x}_i + b) > 0 \quad \forall i
  $$

---

### ✅ Margin: 분리 경계로부터의 거리

- 어떤 샘플 $\mathbf{x}_i$의 **signed distance**:
  $$
  r_i = \frac{\mathbf{w}^\top \mathbf{x}_i + b}{\|\mathbf{w}\|}
  $$

- 전체 데이터에서의 **margin**:
  $$
  \rho = \min_i \frac{y_i (\mathbf{w}^\top \mathbf{x}_i + b)}{\|\mathbf{w}\|}
  $$

---

### ✅ Maximum Margin Hyperplane

- 목표: margin $\rho$를 최대화
- 하지만 $\|\mathbf{w}\|$이 바뀌면 margin도 변함 → **scale 고정 필요**

📐 Canonical form 가정:
$$
y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 \quad \forall i
$$

- 이 때 margin은:
  $$
  \rho = \frac{1}{\|\mathbf{w}\|}
  $$

---

### ✅ 최적화 문제 (Hard-margin SVM)

📐 최적화 문제 정리:
$$
\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2 \\
\text{subject to } y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1, \forall i
$$

- 목적: margin 최대화 ↔ $\|\mathbf{w}\|$ 최소화
- 이 문제는 convex quadratic optimization 문제임

---

### ✅ Soft-margin SVM (슬랙 변수 도입)

- 데이터가 완벽히 선형 분리 안 되는 경우
- **Slack 변수 $\xi_i \geq 0$** 를 도입하여 일부 오차 허용

📐 Soft-margin 최적화 문제:
$$
\min_{\mathbf{w}, b, \boldsymbol{\xi}} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^n \xi_i \\
\text{subject to } y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0
$$

- $C$: slack penalty 조절 → 클수록 오차를 덜 허용함 (regularization 역할)

---

### ✅ Functional & Geometric Margin

- **Functional Margin**:
  $$
  \hat{\gamma}_i = y_i (\mathbf{w}^\top \mathbf{x}_i + b)
  $$

- **Geometric Margin** (거리 기반):
  $$
  \gamma_i = \frac{\hat{\gamma}_i}{\|\mathbf{w}\|}
  $$

- 전체 margin:
  $$
  \gamma = \min_i \gamma_i = \frac{1}{\|\mathbf{w}\|} \min_i \hat{\gamma}_i
  $$

---

### ✅ Canonical Form 변환 예시

- 예를 들어 $w = [1, 1]^T$, $b = -1$로 분리 가능할 때,
- 모든 $y_i(w^\top x_i + b) \geq 2$이면 → scale factor 2로 나눠 canonical form 만들 수 있음

---

## ✅ 핵심 요약

- **SVM은 확률 분포 없이도 분류 경계를 구함**
- 가장 margin이 큰 hyperplane을 찾아 일반화 능력 극대화
- **Hard-margin**은 완벽히 분리되는 데이터에 사용
- **Soft-margin**은 오차 허용 → 현실 문제에 적합
- 최적화 문제는 convex이며, 효율적인 해법 존재



# 🎯 Session 6: Empirical Risk Minimization (ERM)

## 🌱 개념 흐름 요약

- **Supervised learning** 문제는 결국 “좋은 예측 함수 $f$”를 찾는 것
- “좋다”는 기준은 **Loss Function** $L(y, f(x))$으로 정의
- **Expected Risk** $R_L(f)$를 최소화하는 것이 목표지만, 데이터 분포를 모르기 때문에 **Empirical Risk**를 대신 최소화함
- 이를 **ERM (Empirical Risk Minimization)**이라 부르며, 머신러닝의 핵심 틀임
- 다양한 알고리즘들은 **ERM + Regularization**의 형태로 통일 가능

---

## 🔑 개념별 정리 (개념 ↔ 수식 연결)

### ✅ ERM의 기본 구조

- 입력: 훈련 데이터 $ \{(\mathbf{x}_i, y_i)\}_{i=1}^n $
- 학습 목표: Loss를 최소화하는 함수 $f$ 찾기

📐 기대 위험:
$$
R_L(f) = \mathbb{E}_{X,Y} \left[ L(Y, f(X)) \right]
$$

📐 경험 위험 (empirical risk):
$$
\hat{R}_L(f) = \frac{1}{n} \sum_{i=1}^n L(y_i, f(\mathbf{x}_i))
$$

- 실제 최적화 문제:
$$
\min_{f \in \mathcal{F}} \left[ \hat{R}_L(f) + \lambda \cdot \Omega(f) \right]
$$

- $\mathcal{F}$: 함수 공간 (예: 선형 함수)
- $\Omega(f)$: 정규화 항 (regularization term)
- $\lambda$: 정규화 강도

---

### ✅ 다양한 Loss 함수 예시

#### 🔹 회귀 문제 (Regression)
- **Squared loss** (MSE):
  $$
  L(y, f(x)) = (y - f(x))^2
  $$

- **Absolute deviation loss**:
  $$
  L(y, f(x)) = |y - f(x)|
  $$

#### 🔹 이진 분류 문제 (Binary Classification)

- **0-1 Loss**:
  $$
  L(y, t) = \mathbf{1}\{ y \neq \text{sign}(t) \}
  $$
  → 직접 최적화 어렵고, 비선형/비차분적임

- **Logistic Loss**:
  $$
  L(y, t) = \log(1 + e^{-yt})
  $$

- **Hinge Loss** (SVM):
  $$
  L(y, t) = \max(0, 1 - yt)
  $$

---

### ✅ Surrogate Loss: 왜 쓰는가?

- 0-1 loss는 convex도 아니고, gradient도 없음 → 최적화 어려움
- → 이를 **convex surrogate loss**로 대체

| Loss | 특징 | 사용 모델 |
|------|------|-----------|
| Logistic | smooth, convex | Logistic Regression |
| Hinge | convex but non-smooth | SVM |

모두 margin $yt$에 기반한 **margin-based loss**임

---

### ✅ ERM 프레임워크로 본 대표 모델들

| 모델 | Loss | Regularization |
|------|------|----------------|
| **Linear Regression** | $(y - f(x))^2$ | X |
| **Ridge Regression** | $(y - f(x))^2$ | $ \lambda \|w\|^2 $ |
| **Logistic Regression** | $ \log(1 + e^{-yt}) $ | $ \lambda \|w\|^2 $ |
| **SVM** | $ \max(0, 1 - yt) $ | $ \lambda \|w\|^2 $ |

---

### ✅ SVM 최적화 문제 다시 보기

ERM 관점에서의 SVM:

📐 ERM-Hinge Form:
$$
\min_{\mathbf{w}, b} \frac{\lambda}{2} \|\mathbf{w}\|^2 + \frac{1}{n} \sum_{i=1}^n \max(0, 1 - y_i(\mathbf{w}^\top \mathbf{x}_i + b))
$$

📐 Soft-margin SVM (제약식 기반):
$$
\min_{\mathbf{w}, b, \boldsymbol{\xi}} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^n \xi_i \\
\text{s.t. } y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0
$$

- 두 방식은 $\lambda = \frac{1}{C}$일 때 **동치**

---

## ✅ 핵심 요약

- ERM은 머신러닝 모델을 설계하는 **공통 이론 틀**
- 각 모델은:
  - Loss $L(y, f(x))$
  - Function class $\mathcal{F}$
  - Regularizer $\Omega(f)$
  를 선택하여 설계됨
- Convex surrogate loss를 통해 **학습 가능하고 안정적인 모델 구현**
- **Logistic Regression, SVM, Ridge, Lasso** 모두 ERM에 포함됨


