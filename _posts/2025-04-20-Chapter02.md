---
title: "Chapter02"
excerpt: "ë¨¸ì‹ ëŸ¬ë‹ Chap02 ì „ì²´ íë¦„"

categories:
  - MachineLearning
tags:
  - [Machine Learning, ë¨¸ì‹ ëŸ¬ë‹]

permalink: /categories/MachineLearning/Chapter02_MachineLearning

toc: true
toc_sticky: true

date: 2025-04-20
last_modified_at: 2025-04-20
---

# ğŸ“˜ Chapter 2: Classification with Linear Models (`slide_ch2_modified.pdf`)

## ğŸ“‘ ì„¸ì…˜ë³„ êµ¬ì„± ë° ì£¼ìš” ì£¼ì œ ì •ë¦¬

---

### ğŸŸ¦ Session 1: Bayes Classifiers

- **ëª©í‘œ**: ì˜¤ë¥˜ í™•ë¥  $R(f) = \Pr(f(\mathbf{X}) \ne Y)$ì„ ìµœì†Œí™”í•˜ëŠ” ì´ìƒì ì¸ ë¶„ë¥˜ê¸° ì •ì˜
- **Bayes classifier**:
  $$
  f^*(\mathbf{x}) = \arg\max_k \Pr(Y = k \mid \mathbf{X} = \mathbf{x}) = \arg\max_k \pi_k \cdot g_k(\mathbf{x})
  $$
- ì‹¤ì œì—ì„œëŠ” ë¶„í¬ë¥¼ ì•Œ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— **Plug-in classifier** ì‚¬ìš©
- ì´ ì„¸ì…˜ì€ ì´í›„ì˜ ëª¨ë“  ë¶„ë¥˜ê¸°(LDA, NaÃ¯ve Bayes, Logistic Regression ë“±)ì˜ **ì´ë¡ ì  í† ëŒ€**

---

### ğŸŸ¦ Session 2: Linear Discriminant Analysis (LDA)

- **ê°€ì •**: í´ë˜ìŠ¤ë³„ë¡œ ê³µí†µ ê³µë¶„ì‚° $\Sigma$ë¥¼ ê°–ëŠ” Gaussian ë¶„í¬
- LDAëŠ” Bayes classifierì˜ ê·¼ì‚¬ë¡œ, ê²°ì • ê²½ê³„ê°€ **ì„ í˜•**
- íŒŒë¼ë¯¸í„° ì¶”ì •:
  - í´ë˜ìŠ¤ í‰ê·  $\mu_k$, ê³µí†µ ê³µë¶„ì‚° $\Sigma$, prior $\pi_k$
- ë¶„ë¥˜ê¸° í˜•íƒœ (binary):
  $$
  \hat{f}(\mathbf{x}) = \operatorname{sign}(\mathbf{w}^\top \mathbf{x} + b)
  $$
- **Multiclass LDA** ì—­ì‹œ ë™ì¼í•œ êµ¬ì¡°ë¡œ í™•ì¥ ê°€ëŠ¥
- Mahalanobis ê±°ë¦¬ ê´€ì ìœ¼ë¡œë„ í•´ì„ ê°€ëŠ¥

---

### ğŸŸ¦ Session 3: NaÃ¯ve Bayes Classifier

- **ì¡°ê±´ë¶€ ë…ë¦½ ê°€ì •**: í´ë˜ìŠ¤ $Y$ê°€ ì£¼ì–´ì§€ë©´ ê° feature $X_j$ëŠ” ì„œë¡œ ë…ë¦½
- Likelihood ê³„ì‚°:
  $$
  \Pr(\mathbf{x} \mid Y=k) = \prod_{j=1}^d \Pr(x_j \mid Y=k)
  $$
- ë¶„ë¥˜ê¸°:
  $$
  \hat{f}(\mathbf{x}) = \arg\max_k \pi_k \cdot \prod_j g_{kj}(x_j)
  $$
- **Laplace smoothing** ìœ¼ë¡œ zero probability ë¬¸ì œ í•´ê²°
- í…ìŠ¤íŠ¸ ë¶„ë¥˜(Bag-of-Words), ìŠ¤íŒ¸ í•„í„°ë§ ë“±ì—ì„œ í™œìš©

---

### ğŸŸ¦ Session 4: Logistic Regression

- Posterior í™•ë¥  ì§ì ‘ ëª¨ë¸ë§:
  $$
  \eta(\mathbf{x}) = \Pr(Y=1 \mid \mathbf{x}) = \frac{1}{1 + \exp(-\mathbf{w}^\top \mathbf{x} - b)}
  $$
- ê²°ì • ê²½ê³„: ì„ í˜•
  $$
  \hat{y} = \begin{cases} 1 & \text{if } \eta(\mathbf{x}) \geq 0.5 \\ 0 & \text{otherwise} \end{cases}
  $$
- íŒŒë¼ë¯¸í„° í•™ìŠµ: **Maximum Likelihood Estimation**
- ì •ê·œí™” (Ridge):
  $$
  \hat{\theta} = \arg\min_\theta \left[ -\ell(\theta) + \lambda \|\theta\|^2 \right]
  $$
- í™•ë¥  ì˜ˆì¸¡ê¹Œì§€ ê°€ëŠ¥í•œ ì„ í˜• ë¶„ë¥˜ê¸°

---

### ğŸŸ¦ Session 5: Separating Hyperplanes & SVM

- í™•ë¥  ì—†ì´ **ê¸°í•˜í•™ì ìœ¼ë¡œ ë°ì´í„° ë¶„ë¦¬**
- Linear classifier: $f(\mathbf{x}) = \operatorname{sign}(\mathbf{w}^\top \mathbf{x} + b)$
- Margin ì •ì˜:
  $$
  \rho = \min_i \frac{y_i(\mathbf{w}^\top \mathbf{x}_i + b)}{\|\mathbf{w}\|}
  $$
- ìµœì í™” ë¬¸ì œ (Hard-margin):
  $$
  \min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2 \quad \text{s.t. } y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1
  $$
- Soft-margin (Slack ë³€ìˆ˜ $\xi_i$ ë„ì…):
  $$
  \min_{\mathbf{w}, b, \xi} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum \xi_i
  $$

---

### ğŸŸ¦ Session 6: Empirical Risk Minimization (ERM)

- ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì„¤ê³„ë¥¼ ìœ„í•œ **í†µì¼ëœ ì´ë¡ ì  í”„ë ˆì„ì›Œí¬**
- ê²½í—˜ ìœ„í—˜ ìµœì†Œí™”:
  $$
  \hat{R}_L(f) = \frac{1}{n} \sum_{i=1}^n L(y_i, f(\mathbf{x}_i))
  $$
- ì •ê·œí™” ì¶”ê°€:
  $$
  \min_f \hat{R}_L(f) + \lambda \cdot \Omega(f)
  $$
- Surrogate loss (e.g., Logistic, Hinge)ë¡œ 0-1 loss ê·¼ì‚¬
- ë‹¤ì–‘í•œ ëª¨ë¸ì´ ERM êµ¬ì¡°ì— í¬í•¨ë¨:

| ëª¨ë¸ | Loss | ì •ê·œí™” |
|------|------|--------|
| Linear Regression | $(y - f(x))^2$ | X |
| Ridge Regression | $(y - f(x))^2$ | $\lambda \|w\|^2$ |
| Logistic Regression | $\log(1 + e^{-yt})$ | $\lambda \|w\|^2$ |
| SVM | $\max(0, 1 - yt)$ | $\lambda \|w\|^2$ |

---

## ğŸ§­ ì „ì²´ íë¦„ ìš”ì•½

1. **Bayes Optimal ë¶„ë¥˜ê¸°** ì´í•´ â†’ ì‹¤ì œ êµ¬í˜„ ì–´ë ¤ì›€
2. ì´ë¥¼ ê·¼ì‚¬í•˜ëŠ” ëª¨ë¸ë“¤:
   - LDA: ê³µí†µ Gaussian ê°€ì • + ì„ í˜• ê²°ì • ê²½ê³„
   - NaÃ¯ve Bayes: ì¡°ê±´ë¶€ ë…ë¦½ ê°€ì •
   - Logistic Regression: Posterior í™•ë¥  ì§ì ‘ ëª¨ë¸ë§
3. **í™•ë¥  ì—†ì´ ë¶„ë¥˜**í•˜ëŠ” ê¸°í•˜í•™ì  ëª¨ë¸: SVM
4. ì´ ëª¨ë“  ëª¨ë¸ì€ ê²°êµ­ **ERM í”„ë ˆì„**ìœ¼ë¡œ í†µí•©ë¨!



# ğŸ“š Session 1: Bayes Classifiers

## ğŸŒ± ê°œë… íë¦„ ìš”ì•½
- ë¶„ë¥˜ ë¬¸ì œë¥¼ **í™•ë¥ ì  ê´€ì **ìœ¼ë¡œ ì ‘ê·¼
- ëª©í‘œëŠ”: **ì˜¤ë¥˜ í™•ë¥ ì„ ìµœì†Œí™”í•˜ëŠ” ë¶„ë¥˜ê¸°**ë¥¼ ì°¾ëŠ” ê²ƒ
- ì´ë¡ ì ìœ¼ë¡œ ê°€ì¥ ì´ìƒì ì¸ ë¶„ë¥˜ê¸°ëŠ” **Bayes Classifier**
- ì‹¤ì œ ë¶„ë¥˜ê¸°ì—ì„œëŠ” **Bayes Classifierë¥¼ ê·¼ì‚¬**í•˜ëŠ” ë°©ë²•ë“¤ì„ ì‚¬ìš© (ex. LDA, NaÃ¯ve Bayes, Logistic Regression)

---

## ğŸ”‘ ê°œë…ë³„ ì •ë¦¬ (ê°œë… â†” ìˆ˜ì‹ ì—°ê²°)

### âœ… í™•ë¥  ëª¨ë¸ ê¸°ë°˜ ë¶„ë¥˜ ë¬¸ì œ ì„¸íŒ…

- Feature vector:  
  $$ \mathbf{X} = [X_1, \ldots, X_d]^\top \in \mathbb{R}^d $$
- Class label:  
  $$ Y \in \{1, \ldots, K\} $$
- **ê°€ì •**: $(\mathbf{X}, Y)$ëŠ” ê³µë™ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” í™•ë¥  ë³€ìˆ˜ ìŒ

ğŸ“ ë‘ ê°€ì§€ ê´€ì ì˜ í™•ë¥  ë¶„í•´:
1. **Generative View** (jointë¥¼ prior Ã— likelihoodë¡œ ë¶„í•´):
   $$
   p(\mathbf{X}, Y) = p(Y) \cdot p(\mathbf{X} \mid Y)
   $$
2. **Discriminative View** (posterior ì¤‘ì‹¬):
   $$
   p(\mathbf{X}, Y) = p(\mathbf{X}) \cdot p(Y \mid \mathbf{X})
   $$

---

### âœ… Posterior, Prior, Likelihood (ìš©ì–´ ì •ë¦¬)

| ê°œë… | ìˆ˜ì‹ | ì„¤ëª… |
|------|------|------|
| **Prior** | $$ \pi_k = \Pr(Y = k) $$ | í´ë˜ìŠ¤ë³„ ì‚¬ì „ í™•ë¥  |
| **Likelihood** | $$ g_k(\mathbf{x}) = \Pr(\mathbf{X} = \mathbf{x} \mid Y = k) $$ | í´ë˜ìŠ¤ë³„ ì¡°ê±´ë¶€ í™•ë¥  ë°€ë„ |
| **Posterior** | $$ \eta_k(\mathbf{x}) = \Pr(Y = k \mid \mathbf{X} = \mathbf{x}) $$ | ì…ë ¥ $\mathbf{x}$ì— ëŒ€í•œ í´ë˜ìŠ¤ í™•ë¥  |

---

### âœ… Bayes Classifier ì •ì˜

- **ëª©í‘œ**: ì˜¤ë¥˜ í™•ë¥  ìµœì†Œí™”
- **ìœ„í—˜ í•¨ìˆ˜** (error í™•ë¥ ):
  $$
  R(f) = \Pr(f(\mathbf{X}) \neq Y)
  $$

- **Bayes Risk**:
  $$
  R^* = \min_f R(f)
  $$

- **Bayes Classifier ìˆ˜ì‹**:

ğŸ“ â‘  Posterior ê¸°ë°˜ í‘œí˜„:
$$
f^*(\mathbf{x}) = \arg\max_{k} \eta_k(\mathbf{x})
$$

ğŸ“ â‘¡ Prior Ã— Likelihood ê¸°ë°˜ í‘œí˜„:
$$
f^*(\mathbf{x}) = \arg\max_{k} \pi_k \cdot g_k(\mathbf{x})
$$

---

### âœ… ì™œ ìœ„ ì‹ì´ Bayes Optimalì¸ì§€ (ì§ê´€)

- $R(f)$ë¥¼ ì „ê°œí•´ë³´ë©´:
  $$
  R(f) = \mathbb{E}_{\mathbf{X}} \left[ 1 - \Pr(Y = f(\mathbf{X}) \mid \mathbf{X}) \right]
  $$
- ì´ë¥¼ ìµœì†Œí™”í•˜ë ¤ë©´:  
  $\Pr(Y = f(\mathbf{x}) \mid \mathbf{x})$ë¥¼ **ìµœëŒ€ë¡œ** ë§Œë“¤ì–´ì•¼ í•˜ë¯€ë¡œ,  
  ê°€ì¥ í™•ë¥ ì´ ë†’ì€ í´ë˜ìŠ¤ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì´ ìµœì .

---

### âœ… Plug-in Classifier

- ì‹¤ì œì—ì„  $(\mathbf{X}, Y)$ì˜ ë¶„í¬ë¥¼ **ì•Œ ìˆ˜ ì—†ìŒ**
- ë”°ë¼ì„œ í•™ìŠµ ë°ì´í„°ë¥¼ í†µí•´ $\eta_k(\mathbf{x})$, $g_k(\mathbf{x})$, $\pi_k$ ë“±ì„ ì¶”ì •í•˜ê³ ,  
  ì´ë¥¼ Bayes Classifier ê³µì‹ì— **"ë¼ì›Œ ë„£ëŠ”ë‹¤"** â†’ **Plug-in Classifier**

---

### âœ… ëŒ€í‘œì ì¸ Plug-in ë°©ë²•ë“¤

| ë°©ë²• | ì¶”ì • ë°©ì‹ | ì£¼ìš” ê°€ì • |
|------|-----------|----------|
| **LDA** | $\pi_k$, $g_k(\mathbf{x})$ ì¶”ì • | $g_k$ëŠ” ê³µí†µ ê³µë¶„ì‚° $\Sigma$ ê°€ì§„ Gaussian |
| **NaÃ¯ve Bayes** | $\pi_k$, $g_k$ ì¶”ì • | featureë“¤ì´ í´ë˜ìŠ¤ ì£¼ì–´ì§„ ì¡°ê±´ì—ì„œ ë…ë¦½ |
| **Logistic Regression** | $\eta_k(\mathbf{x})$ ì§ì ‘ ì¶”ì • | posteriorë¥¼ ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜ë¡œ ê°€ì • |

---

## âœ… í•µì‹¬ ìš”ì•½

- **Bayes Classifier**ëŠ” ì´ë¡ ì ìœ¼ë¡œ ì˜¤ë¥˜ í™•ë¥ ì´ ìµœì†Œì¸ ìµœì  ë¶„ë¥˜ê¸°
- í•˜ì§€ë§Œ ì‹¤ì œ ë°ì´í„°ì—ì„œëŠ” ë¶„í¬ë¥¼ ëª¨ë¦„ â†’ **Plug-in** ë°©ì‹ìœ¼ë¡œ ê·¼ì‚¬
- ì´í›„ ë°°ìš¸ LDA, NaÃ¯ve Bayes, Logistic Regressionì€ ëª¨ë‘ ì´ **Bayes Classifierì˜ ê·¼ì‚¬ ë²„ì „ë“¤**ì„!


# ğŸ§® Session 2: Linear Discriminant Analysis (LDA)

## ğŸŒ± ê°œë… íë¦„ ìš”ì•½

- LDAëŠ” **Bayes classifier**ë¥¼ ê·¼ì‚¬í•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ì´ë‹¤.
- ê° í´ë˜ìŠ¤ì˜ ë¶„í¬ë¥¼ **ê³µí†µ ê³µë¶„ì‚° $\Sigma$ë¥¼ ê°€ì§„ Gaussian ë¶„í¬**ë¡œ ê°€ì •í•œë‹¤.
- ì´ ê°€ì • ë•ë¶„ì— **í´ë˜ìŠ¤ ê²°ì • ê²½ê³„ê°€ ì„ í˜•**ì´ ëœë‹¤.
- Posterior (ì‚¬í›„ í™•ë¥ )ì„ ì§ì ‘ êµ¬í•˜ëŠ” ëŒ€ì‹ , Prior Ã— Likelihood í˜•íƒœë¡œ í’€ì–´ëƒ„

---

## ğŸ”‘ ê°œë…ë³„ ì •ë¦¬ (ê°œë… â†” ìˆ˜ì‹ ì—°ê²°)

### âœ… LDAì˜ í•µì‹¬ ê°€ì •

- í´ë˜ìŠ¤ë³„ ì¡°ê±´ë¶€ ë¶„í¬:
  $$
  \mathbf{X} \mid Y = k \sim \mathcal{N}(\boldsymbol{\mu}_k, \Sigma)
  $$

- ëª¨ë“  í´ë˜ìŠ¤ê°€ **ê°™ì€ ê³µë¶„ì‚° í–‰ë ¬ $\Sigma$ë¥¼ ê³µìœ **í•œë‹¤ê³  ê°€ì •
- $\pi_k = \Pr(Y = k)$: í´ë˜ìŠ¤ kì˜ prior í™•ë¥ 

---

### âœ… LDAì—ì„œì˜ Bayes ë¶„ë¥˜ê¸° ìˆ˜ì‹

- ì¼ë°˜ Bayes classifier:
  $$
  f^*(\mathbf{x}) = \arg\max_k \pi_k \cdot g_k(\mathbf{x})
  $$

- LDAì—ì„œëŠ” ê° $g_k(\mathbf{x})$ê°€ Gaussian ì´ë¯€ë¡œ:
  $$
  g_k(\mathbf{x}) = \phi(\mathbf{x}; \boldsymbol{\mu}_k, \Sigma)
  $$

ğŸ“ log-likelihoodë¡œ ë°”ê¾¸ë©´:
$$
f(\mathbf{x}) = \arg\max_k \left[ \log \pi_k - \frac{1}{2} (\mathbf{x} - \boldsymbol{\mu}_k)^T \Sigma^{-1} (\mathbf{x} - \boldsymbol{\mu}_k) \right]
$$

- ì •ë¦¬í•˜ë©´:
  $$
  f(\mathbf{x}) = \arg\max_k \mathbf{w}_k^T \mathbf{x} + b_k
  $$

---

### âœ… íŒŒë¼ë¯¸í„° ì¶”ì • (MLE ê¸°ë°˜)

- í•™ìŠµ ë°ì´í„°ë¡œë¶€í„° ì¶”ì •:

1. Prior:
   $$
   \hat{\pi}_k = \frac{n_k}{n}
   $$

2. í´ë˜ìŠ¤ë³„ í‰ê· :
   $$
   \hat{\boldsymbol{\mu}}_k = \frac{1}{n_k} \sum_{i: y_i = k} \mathbf{x}_i
   $$

3. ê³µí†µ ê³µë¶„ì‚°:
   $$
   \hat{\Sigma} = \frac{1}{n} \sum_{i=1}^n (\mathbf{x}_i - \hat{\boldsymbol{\mu}}_{y_i})(\mathbf{x}_i - \hat{\boldsymbol{\mu}}_{y_i})^T
   $$

---

### âœ… Binary Caseì—ì„œì˜ ê²°ì • í•¨ìˆ˜

- í´ë˜ìŠ¤ ë¼ë²¨: $Y \in \{-1, +1\}$
- ê²°ì • í•¨ìˆ˜ëŠ” ì„ í˜• í•¨ìˆ˜ì˜ ë¶€í˜¸:
  $$
  \hat{f}(\mathbf{x}) = \operatorname{sign}(\mathbf{w}^T \mathbf{x} + b)
  $$

- ì—¬ê¸°ì„œ:
  $$
  \mathbf{w} = \hat{\Sigma}^{-1}(\hat{\boldsymbol{\mu}}_1 - \hat{\boldsymbol{\mu}}_{-1})
  $$

  $$
  b = \log \frac{\hat{\pi}_1}{\hat{\pi}_{-1}} + \frac{1}{2} \left( \hat{\boldsymbol{\mu}}_{-1}^T \hat{\Sigma}^{-1} \hat{\boldsymbol{\mu}}_{-1} - \hat{\boldsymbol{\mu}}_1^T \hat{\Sigma}^{-1} \hat{\boldsymbol{\mu}}_1 \right)
  $$

---

### âœ… Multiclass Case (K > 2)

- ê²°ì • í•¨ìˆ˜:
  $$
  \hat{f}(\mathbf{x}) = \arg\max_k \mathbf{w}_k^T \mathbf{x} + b_k
  $$

- ê° í´ë˜ìŠ¤ì— ëŒ€í•´:
  $$
  \mathbf{w}_k = \hat{\Sigma}^{-1} \hat{\boldsymbol{\mu}}_k
  $$
  $$
  b_k = \log \hat{\pi}_k - \frac{1}{2} \hat{\boldsymbol{\mu}}_k^T \hat{\Sigma}^{-1} \hat{\boldsymbol{\mu}}_k
  $$

---

### âœ… Mahalanobis Distance í•´ì„

- LDAëŠ” Mahalanobis ê±°ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶„ë¥˜í•¨
- í´ë˜ìŠ¤ ì¤‘ì‹¬ $\mu_k$ ì™€ì˜ ê±°ë¦¬:
  $$
  D_{\Sigma^{-1}}(\mathbf{x}, \mu_k) = \sqrt{(\mathbf{x} - \mu_k)^T \Sigma^{-1} (\mathbf{x} - \mu_k)}
  $$

- LDAì˜ ê²°ì •ì€ ê²°êµ­ Mahalanobis ê±°ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ê°€ê¹Œìš´ í´ë˜ìŠ¤ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒê³¼ ìœ ì‚¬í•¨  
  (ë‹¨, prior $\pi_k$ ë„ ê³ ë ¤ë¨)

---

## âœ… í•µì‹¬ ìš”ì•½

- LDAëŠ” Bayes classifierì˜ **ì„ í˜• ê·¼ì‚¬ ë²„ì „**
- í´ë˜ìŠ¤ë³„ Gaussian ë¶„í¬ + ê³µí†µ ê³µë¶„ì‚°ì´ë¼ëŠ” ê°€ì •ì„ ë°”íƒ•ìœ¼ë¡œ  
  ê²°ì • ê²½ê³„ë¥¼ **ì„ í˜• í˜•íƒœ**ë¡œ ìœ ë„
- íŒŒë¼ë¯¸í„°ëŠ” MLEë¡œ ì¶”ì •í•˜ê³ , Posterior ëŒ€ì‹  log-likelihoodë¡œ ë¹„êµ
- Multiclass, Binary ëª¨ë‘ ì¼ê´€ëœ êµ¬ì¡°ë¡œ í™•ì¥ ê°€ëŠ¥


# ğŸ¤– Session 3: NaÃ¯ve Bayes Classifier

## ğŸŒ± ê°œë… íë¦„ ìš”ì•½

- NaÃ¯ve BayesëŠ” Bayes classifierì˜ ê°„ë‹¨í•œ ê·¼ì‚¬ ë°©ë²•
- í•µì‹¬ ê°€ì •: **í´ë˜ìŠ¤ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, feature ê°„ì€ ì¡°ê±´ë¶€ ë…ë¦½ì´ë‹¤**
- ì´ ê°€ì • ë•ë¶„ì— ê³ ì°¨ì› feature spaceì—ì„œë„ **likelihood ê³„ì‚°ì´ ì‰¬ì›Œì§**
- ì£¼ë¡œ **ë¬¸ì„œ ë¶„ë¥˜(text classification)** ê°™ì€ ë¶„ì•¼ì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ ì‚¬ìš©ë¨

---

## ğŸ”‘ ê°œë…ë³„ ì •ë¦¬ (ê°œë… â†” ìˆ˜ì‹ ì—°ê²°)

### âœ… NaÃ¯ve Bayesì˜ ì¡°ê±´ë¶€ ë…ë¦½ ê°€ì •

- ì…ë ¥ ë²¡í„°:
  $$
  \mathbf{X} = [X_1, X_2, \dots, X_d]^T
  $$

- **ê°€ì •**: í´ë˜ìŠ¤ $Y = k$ ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ê° featureëŠ” ì„œë¡œ ë…ë¦½
  $$
  \Pr(\mathbf{X} = \mathbf{x} \mid Y = k) = \prod_{j=1}^d \Pr(X_j = x_j \mid Y = k)
  $$

- ì´ ì‹ ë•ë¶„ì— ê³„ì‚° ë³µì¡ë„ê°€ **ì§€ìˆ˜ì—ì„œ ì„ í˜•ìœ¼ë¡œ** ê°ì†Œí•¨!

---

### âœ… NaÃ¯ve Bayes ë¶„ë¥˜ê¸° ìˆ˜ì‹

- Posterior:
  $$
  \eta_k(\mathbf{x}) = \Pr(Y = k \mid \mathbf{X} = \mathbf{x}) \propto \pi_k \cdot \prod_{j=1}^d g_{kj}(x_j)
  $$

- ë¶„ë¥˜ê¸°:
  $$
  \hat{f}(\mathbf{x}) = \arg\max_k \pi_k \cdot \prod_{j=1}^d g_{kj}(x_j)
  $$

ğŸ“Œ ì—¬ê¸°ì„œ:
- $\pi_k$: í´ë˜ìŠ¤ $k$ì˜ prior í™•ë¥  ì¶”ì •
- $g_{kj}(x_j) = \Pr(X_j = x_j \mid Y = k)$: ê° featureë³„ ì¡°ê±´ë¶€ ë¶„í¬

---

### âœ… í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì˜ˆì‹œ (Bag-of-Words)

- ê° ë¬¸ì„œë¥¼ **ë‹¨ì–´ì˜ ë“±ì¥ íšŸìˆ˜ ë²¡í„°**ë¡œ í‘œí˜„:
  $$
  \mathbf{x} = [x_1, x_2, \ldots, x_d]^T
  $$

- ê° ë‹¨ì–´ $j$ê°€ í´ë˜ìŠ¤ $k$ì—ì„œ ë“±ì¥í•  í™•ë¥ ì„ ì¶”ì •:
  $$
  \hat{g}_{kj}(l) = \frac{n_{kjl}}{n_k}
  $$

  - $n_{kjl}$: í´ë˜ìŠ¤ $k$ì—ì„œ ë‹¨ì–´ $j$ê°€ $l$ë²ˆ ë“±ì¥í•œ ë¬¸ì„œ ìˆ˜
  - $n_k$: í´ë˜ìŠ¤ $k$ì— ì†í•œ ì „ì²´ ë¬¸ì„œ ìˆ˜

---

### âœ… Prior í™•ë¥  ì¶”ì •

- í´ë˜ìŠ¤ $k$ì˜ prior í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ì´ ì¶”ì •:
  $$
  \hat{\pi}_k = \frac{n_k}{n}
  $$

---

### âœ… Additive (Laplace) Smoothing

- ë¬¸ì œ: í•™ìŠµ ë°ì´í„°ì— ì—†ëŠ” ë‹¨ì–´ â†’ í™•ë¥  0 â†’ ì „ì²´ í™•ë¥ ì´ 0ì´ ë˜ëŠ” ë¬¸ì œ ë°œìƒ
- í•´ê²°: ëª¨ë“  ê²½ìš°ì— ì‘ì€ ê°’ì„ ë”í•¨

ğŸ“ ìˆ˜ì‹:
$$
\hat{g}_{kj}(l) = \frac{n_{kjl} + \alpha}{n_k + \alpha L}
$$

- $\alpha > 0$: smoothing parameter (ë³´í†µ 1 ì‚¬ìš©)
- $L$: ê°€ëŠ¥í•œ ê°’ì˜ ê°œìˆ˜ (ì˜ˆ: ë‹¨ì–´ ë“±ì¥ íšŸìˆ˜ ê²½ìš°ì˜ ìˆ˜)

---

### âœ… ì˜ˆì œ ìš”ì•½

- í›ˆë ¨ ë°ì´í„°:
  - Y = {Sunny, Rainy}
  - X1 = Umbrella, X2 = Sunglasses
- Step:
  1. Prior ê³„ì‚°
  2. ê° featureì˜ ì¡°ê±´ë¶€ í™•ë¥  ê³„ì‚°
  3. ìƒˆë¡œìš´ ìƒ˜í”Œì— ëŒ€í•œ posterior ê³„ì‚°
  4. ë” ë†’ì€ posteriorë¥¼ ê°–ëŠ” í´ë˜ìŠ¤ë¡œ ì˜ˆì¸¡

- ì˜ˆì¸¡ ìˆ˜ì‹:
  $$
  \Pr(Y=k \mid \mathbf{x}) \propto \hat{\pi}_k \cdot \prod_{j=1}^d \hat{g}_{kj}(x_j)
  $$

---

## âœ… í•µì‹¬ ìš”ì•½

- NaÃ¯ve BayesëŠ” **ì¡°ê±´ë¶€ ë…ë¦½**ì´ë¼ëŠ” ë‹¨ìˆœí•œ ê°€ì • í•˜ì— Bayes Classifierë¥¼ ê·¼ì‚¬
- ê³„ì‚°ì´ ë§¤ìš° ë¹ ë¥´ê³  ê°„ë‹¨í•˜ë©°, ê³ ì°¨ì›ì—ì„œë„ íš¨ê³¼ì 
- Laplace Smoothingì„ í†µí•´ **zero probability ë¬¸ì œ**ë¥¼ í•´ê²°
- í…ìŠ¤íŠ¸ ë¶„ë¥˜, ì´ë©”ì¼ ìŠ¤íŒ¸ í•„í„°ë§ ë“±ì—ì„œ ë§ì´ ì“°ì„



# ğŸ“ˆ Session 4: Logistic Regression

## ğŸŒ± ê°œë… íë¦„ ìš”ì•½

- **ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” posterior í™•ë¥  $\Pr(Y=1|\mathbf{x})$ë¥¼ ì§ì ‘ ëª¨ë¸ë§**
- í™•ë¥  ëª¨ë¸ì€ **sigmoid(ì‹œê·¸ëª¨ì´ë“œ) í•¨ìˆ˜**ë¥¼ ì‚¬ìš©
- ë¶„ë¥˜ ê¸°ì¤€ì€: í™•ë¥ ì´ 0.5 ì´ìƒì´ë©´ 1, ì•„ë‹ˆë©´ 0
- íŒŒë¼ë¯¸í„°ëŠ” **ìµœëŒ€ìš°ë„ì¶”ì • (MLE)** ì„ í†µí•´ í•™ìŠµ
- ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ **ì •ê·œí™” (regularization)** ë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆìŒ

---

## ğŸ”‘ ê°œë…ë³„ ì •ë¦¬ (ê°œë… â†” ìˆ˜ì‹ ì—°ê²°)

### âœ… ë¶„ë¥˜ ë¬¸ì œ ì„¸íŒ…

- Binary classification:
  $$
  Y \in \{0, 1\},\quad \mathbf{x} \in \mathbb{R}^d
  $$

- ëª©í‘œ: ì£¼ì–´ì§„ $\mathbf{x}$ì— ëŒ€í•´ $Y=1$ì¼ í™•ë¥  ì˜ˆì¸¡
  $$
  \eta(\mathbf{x}) := \Pr(Y = 1 \mid \mathbf{x})
  $$

---

### âœ… ë¡œì§€ìŠ¤í‹± ëª¨ë¸ ì •ì˜

- Posterior í™•ë¥ ì„ sigmoid í•¨ìˆ˜ë¡œ í‘œí˜„:
  $$
  \eta(\mathbf{x}) = \frac{1}{1 + \exp(-(\mathbf{w}^\top \mathbf{x} + b))}
  $$

- ì—¬ê¸°ì„œ:
  - $\mathbf{w} \in \mathbb{R}^d$: weight vector
  - $b \in \mathbb{R}$: bias (ì ˆí¸)
  - $\sigma(z) = \frac{1}{1 + e^{-z}}$: sigmoid í•¨ìˆ˜

---

### âœ… ë¶„ë¥˜ ê¸°ì¤€ (ê²°ì • í•¨ìˆ˜)

- ì˜ˆì¸¡ ê²°ê³¼:
  $$
  \hat{y} = \begin{cases}
  1 & \text{if } \eta(\mathbf{x}) \geq \frac{1}{2} \\
  0 & \text{otherwise}
  \end{cases}
  $$

- ì„ í˜• ê²°ì • ê²½ê³„:
  $$
  \eta(\mathbf{x}) \geq \frac{1}{2} \iff \mathbf{w}^\top \mathbf{x} + b \geq 0
  $$

- ì¦‰, logistic regressionì€ **linear classifier**ì´ë‹¤!

---

### âœ… íŒŒë¼ë¯¸í„° í•™ìŠµ (Maximum Likelihood Estimation)

- ê° ë°ì´í„°ëŠ” ë‹¤ìŒì˜ Bernoulli ë¶„í¬ë¥¼ ë”°ë¦„:
  $$
  \Pr(y_i \mid \mathbf{x}_i; \theta) = \eta(\mathbf{x}_i)^{y_i} (1 - \eta(\mathbf{x}_i))^{1 - y_i}
  $$

- ì „ì²´ ë°ì´í„° likelihood:
  $$
  L(\theta) = \prod_{i=1}^n \eta(\mathbf{x}_i)^{y_i} (1 - \eta(\mathbf{x}_i))^{1 - y_i}
  $$

- log-likelihood:
  $$
  \ell(\theta) = \sum_{i=1}^n \left[ y_i \log \eta(\mathbf{x}_i) + (1 - y_i) \log (1 - \eta(\mathbf{x}_i)) \right]
  $$

---

### âœ… ì •ê·œí™” (Regularization)

- ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ ê°€ì¤‘ì¹˜ì— íŒ¨ë„í‹° ë¶€ì—¬:
  $$
  J(\theta) = -\ell(\theta) + \lambda \|\theta\|^2
  $$

- $\lambda$: ì •ê·œí™” ê°•ë„ ì¡°ì ˆ (í¬ë©´ ëª¨ë¸ì´ ë‹¨ìˆœí•´ì§)

- ìµœì¢… ìµœì í™” ë¬¸ì œ:
  $$
  \hat{\theta} = \arg\min_\theta \left[ -\ell(\theta) + \lambda \|\theta\|^2 \right]
  $$

---

### âœ… ê²°ì • ê²½ê³„ì™€ í™•ë¥  ì¶”ì •

- ë¶„ë¥˜ ê²°ê³¼ëŠ” ì„ í˜• ê²°ì • ê²½ê³„ë¥¼ ë”°ë¥´ì§€ë§Œ,
- ì¶œë ¥ ê°’ì€ **í™•ë¥ **ì´ê¸° ë•Œë¬¸ì— **ì˜ì‚¬ê²°ì • ì„ê³„ê°’ ì¡°ì ˆ** ê°€ëŠ¥
  $$
  \hat{\eta}(\mathbf{x}) = \frac{1}{1 + \exp(-(\hat{\mathbf{w}}^\top \mathbf{x} + \hat{b}))}
  $$

---

## âœ… í•µì‹¬ ìš”ì•½

- **ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” í™•ë¥  ë¶„ë¥˜ê¸°**
  - $\Pr(Y = 1 \mid \mathbf{x})$ ìì²´ë¥¼ ëª¨ë¸ë§
- **ê²°ì • ê²½ê³„ëŠ” ì„ í˜•**
  - sigmoid í•¨ìˆ˜ì˜ ì¤‘ì‹¬ì¸ $0.5$ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜
- **íŒŒë¼ë¯¸í„° í•™ìŠµì€ ìµœëŒ€ìš°ë„ì¶”ì •(MLE)** ìœ¼ë¡œ ìˆ˜í–‰
- **ì •ê·œí™”**ë¥¼ í†µí•´ ëª¨ë¸ì˜ ë³µì¡ë„ ì¡°ì ˆ ê°€ëŠ¥
- ë¶„ë¥˜ + í™•ë¥  ì¶”ì • ëª¨ë‘ ê°€ëŠ¥í•œ ì‹¤ìš©ì ì¸ ëª¨ë¸!



# ğŸ§± Session 5: Separating Hyperplanes & SVM

## ğŸŒ± ê°œë… íë¦„ ìš”ì•½

- ì•ì„  ëª¨ë¸ë“¤ì€ í™•ë¥  ë¶„í¬ ê¸°ë°˜ (Bayes, Logistic ë“±)
- ì´ë²ˆì—ëŠ” ë¶„í¬ ì¶”ì • ì—†ì´ **ê¸°í•˜í•™ì ìœ¼ë¡œ ë¶„ë¦¬í•˜ëŠ” ê²½ê³„**ë¥¼ ì°¾ëŠ” ë°©ë²•
- ëª©í‘œ: **ê°€ì¥ marginì´ í° hyperplaneì„ ì°¾ëŠ” ê²ƒ**
- ì„ í˜• ë¶„ë¦¬ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ë¥¼ ìœ„í•´ **Soft Margin + Slack ë³€ìˆ˜**ë¥¼ ë„ì…
- ì´ ê°œë…ì€ **SVM (Support Vector Machine)** ìœ¼ë¡œ ë°œì „

---

## ğŸ”‘ ê°œë…ë³„ ì •ë¦¬ (ê°œë… â†” ìˆ˜ì‹ ì—°ê²°)

### âœ… Hyperplane ì •ì˜

- $d$ì°¨ì› ê³µê°„ì—ì„œì˜ hyperplane:
  $$
  H = \left\{ \mathbf{x} \in \mathbb{R}^d \mid \mathbf{w}^\top \mathbf{x} + b = 0 \right\}
  $$

- $\mathbf{w}$: normal vector (í‰ë©´ì— ìˆ˜ì§ì¸ ë²¡í„°)
- $b$: offset (ì ˆí¸)

---

### âœ… ì„ í˜• ë¶„ë¦¬ ì¡°ê±´

- ì´ì§„ ë¶„ë¥˜: $Y \in \{-1, +1\}$
- ì–´ë–¤ hyperplaneì´ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•œë‹¤ëŠ” ì¡°ê±´:
  $$
  y_i (\mathbf{w}^\top \mathbf{x}_i + b) > 0 \quad \forall i
  $$

---

### âœ… Margin: ë¶„ë¦¬ ê²½ê³„ë¡œë¶€í„°ì˜ ê±°ë¦¬

- ì–´ë–¤ ìƒ˜í”Œ $\mathbf{x}_i$ì˜ **signed distance**:
  $$
  r_i = \frac{\mathbf{w}^\top \mathbf{x}_i + b}{\|\mathbf{w}\|}
  $$

- ì „ì²´ ë°ì´í„°ì—ì„œì˜ **margin**:
  $$
  \rho = \min_i \frac{y_i (\mathbf{w}^\top \mathbf{x}_i + b)}{\|\mathbf{w}\|}
  $$

---

### âœ… Maximum Margin Hyperplane

- ëª©í‘œ: margin $\rho$ë¥¼ ìµœëŒ€í™”
- í•˜ì§€ë§Œ $\|\mathbf{w}\|$ì´ ë°”ë€Œë©´ marginë„ ë³€í•¨ â†’ **scale ê³ ì • í•„ìš”**

ğŸ“ Canonical form ê°€ì •:
$$
y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 \quad \forall i
$$

- ì´ ë•Œ marginì€:
  $$
  \rho = \frac{1}{\|\mathbf{w}\|}
  $$

---

### âœ… ìµœì í™” ë¬¸ì œ (Hard-margin SVM)

ğŸ“ ìµœì í™” ë¬¸ì œ ì •ë¦¬:
$$
\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2 \\
\text{subject to } y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1, \forall i
$$

- ëª©ì : margin ìµœëŒ€í™” â†” $\|\mathbf{w}\|$ ìµœì†Œí™”
- ì´ ë¬¸ì œëŠ” convex quadratic optimization ë¬¸ì œì„

---

### âœ… Soft-margin SVM (ìŠ¬ë™ ë³€ìˆ˜ ë„ì…)

- ë°ì´í„°ê°€ ì™„ë²½íˆ ì„ í˜• ë¶„ë¦¬ ì•ˆ ë˜ëŠ” ê²½ìš°
- **Slack ë³€ìˆ˜ $\xi_i \geq 0$** ë¥¼ ë„ì…í•˜ì—¬ ì¼ë¶€ ì˜¤ì°¨ í—ˆìš©

ğŸ“ Soft-margin ìµœì í™” ë¬¸ì œ:
$$
\min_{\mathbf{w}, b, \boldsymbol{\xi}} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^n \xi_i \\
\text{subject to } y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0
$$

- $C$: slack penalty ì¡°ì ˆ â†’ í´ìˆ˜ë¡ ì˜¤ì°¨ë¥¼ ëœ í—ˆìš©í•¨ (regularization ì—­í• )

---

### âœ… Functional & Geometric Margin

- **Functional Margin**:
  $$
  \hat{\gamma}_i = y_i (\mathbf{w}^\top \mathbf{x}_i + b)
  $$

- **Geometric Margin** (ê±°ë¦¬ ê¸°ë°˜):
  $$
  \gamma_i = \frac{\hat{\gamma}_i}{\|\mathbf{w}\|}
  $$

- ì „ì²´ margin:
  $$
  \gamma = \min_i \gamma_i = \frac{1}{\|\mathbf{w}\|} \min_i \hat{\gamma}_i
  $$

---

### âœ… Canonical Form ë³€í™˜ ì˜ˆì‹œ

- ì˜ˆë¥¼ ë“¤ì–´ $w = [1, 1]^T$, $b = -1$ë¡œ ë¶„ë¦¬ ê°€ëŠ¥í•  ë•Œ,
- ëª¨ë“  $y_i(w^\top x_i + b) \geq 2$ì´ë©´ â†’ scale factor 2ë¡œ ë‚˜ëˆ  canonical form ë§Œë“¤ ìˆ˜ ìˆìŒ

---

## âœ… í•µì‹¬ ìš”ì•½

- **SVMì€ í™•ë¥  ë¶„í¬ ì—†ì´ë„ ë¶„ë¥˜ ê²½ê³„ë¥¼ êµ¬í•¨**
- ê°€ì¥ marginì´ í° hyperplaneì„ ì°¾ì•„ ì¼ë°˜í™” ëŠ¥ë ¥ ê·¹ëŒ€í™”
- **Hard-margin**ì€ ì™„ë²½íˆ ë¶„ë¦¬ë˜ëŠ” ë°ì´í„°ì— ì‚¬ìš©
- **Soft-margin**ì€ ì˜¤ì°¨ í—ˆìš© â†’ í˜„ì‹¤ ë¬¸ì œì— ì í•©
- ìµœì í™” ë¬¸ì œëŠ” convexì´ë©°, íš¨ìœ¨ì ì¸ í•´ë²• ì¡´ì¬



# ğŸ¯ Session 6: Empirical Risk Minimization (ERM)

## ğŸŒ± ê°œë… íë¦„ ìš”ì•½

- **Supervised learning** ë¬¸ì œëŠ” ê²°êµ­ â€œì¢‹ì€ ì˜ˆì¸¡ í•¨ìˆ˜ $f$â€ë¥¼ ì°¾ëŠ” ê²ƒ
- â€œì¢‹ë‹¤â€ëŠ” ê¸°ì¤€ì€ **Loss Function** $L(y, f(x))$ìœ¼ë¡œ ì •ì˜
- **Expected Risk** $R_L(f)$ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ ëª©í‘œì§€ë§Œ, ë°ì´í„° ë¶„í¬ë¥¼ ëª¨ë¥´ê¸° ë•Œë¬¸ì— **Empirical Risk**ë¥¼ ëŒ€ì‹  ìµœì†Œí™”í•¨
- ì´ë¥¼ **ERM (Empirical Risk Minimization)**ì´ë¼ ë¶€ë¥´ë©°, ë¨¸ì‹ ëŸ¬ë‹ì˜ í•µì‹¬ í‹€ì„
- ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ë“¤ì€ **ERM + Regularization**ì˜ í˜•íƒœë¡œ í†µì¼ ê°€ëŠ¥

---

## ğŸ”‘ ê°œë…ë³„ ì •ë¦¬ (ê°œë… â†” ìˆ˜ì‹ ì—°ê²°)

### âœ… ERMì˜ ê¸°ë³¸ êµ¬ì¡°

- ì…ë ¥: í›ˆë ¨ ë°ì´í„° $ \{(\mathbf{x}_i, y_i)\}_{i=1}^n $
- í•™ìŠµ ëª©í‘œ: Lossë¥¼ ìµœì†Œí™”í•˜ëŠ” í•¨ìˆ˜ $f$ ì°¾ê¸°

ğŸ“ ê¸°ëŒ€ ìœ„í—˜:
$$
R_L(f) = \mathbb{E}_{X,Y} \left[ L(Y, f(X)) \right]
$$

ğŸ“ ê²½í—˜ ìœ„í—˜ (empirical risk):
$$
\hat{R}_L(f) = \frac{1}{n} \sum_{i=1}^n L(y_i, f(\mathbf{x}_i))
$$

- ì‹¤ì œ ìµœì í™” ë¬¸ì œ:
$$
\min_{f \in \mathcal{F}} \left[ \hat{R}_L(f) + \lambda \cdot \Omega(f) \right]
$$

- $\mathcal{F}$: í•¨ìˆ˜ ê³µê°„ (ì˜ˆ: ì„ í˜• í•¨ìˆ˜)
- $\Omega(f)$: ì •ê·œí™” í•­ (regularization term)
- $\lambda$: ì •ê·œí™” ê°•ë„

---

### âœ… ë‹¤ì–‘í•œ Loss í•¨ìˆ˜ ì˜ˆì‹œ

#### ğŸ”¹ íšŒê·€ ë¬¸ì œ (Regression)
- **Squared loss** (MSE):
  $$
  L(y, f(x)) = (y - f(x))^2
  $$

- **Absolute deviation loss**:
  $$
  L(y, f(x)) = |y - f(x)|
  $$

#### ğŸ”¹ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œ (Binary Classification)

- **0-1 Loss**:
  $$
  L(y, t) = \mathbf{1}\{ y \neq \text{sign}(t) \}
  $$
  â†’ ì§ì ‘ ìµœì í™” ì–´ë µê³ , ë¹„ì„ í˜•/ë¹„ì°¨ë¶„ì ì„

- **Logistic Loss**:
  $$
  L(y, t) = \log(1 + e^{-yt})
  $$

- **Hinge Loss** (SVM):
  $$
  L(y, t) = \max(0, 1 - yt)
  $$

---

### âœ… Surrogate Loss: ì™œ ì“°ëŠ”ê°€?

- 0-1 lossëŠ” convexë„ ì•„ë‹ˆê³ , gradientë„ ì—†ìŒ â†’ ìµœì í™” ì–´ë ¤ì›€
- â†’ ì´ë¥¼ **convex surrogate loss**ë¡œ ëŒ€ì²´

| Loss | íŠ¹ì§• | ì‚¬ìš© ëª¨ë¸ |
|------|------|-----------|
| Logistic | smooth, convex | Logistic Regression |
| Hinge | convex but non-smooth | SVM |

ëª¨ë‘ margin $yt$ì— ê¸°ë°˜í•œ **margin-based loss**ì„

---

### âœ… ERM í”„ë ˆì„ì›Œí¬ë¡œ ë³¸ ëŒ€í‘œ ëª¨ë¸ë“¤

| ëª¨ë¸ | Loss | Regularization |
|------|------|----------------|
| **Linear Regression** | $(y - f(x))^2$ | X |
| **Ridge Regression** | $(y - f(x))^2$ | $ \lambda \|w\|^2 $ |
| **Logistic Regression** | $ \log(1 + e^{-yt}) $ | $ \lambda \|w\|^2 $ |
| **SVM** | $ \max(0, 1 - yt) $ | $ \lambda \|w\|^2 $ |

---

### âœ… SVM ìµœì í™” ë¬¸ì œ ë‹¤ì‹œ ë³´ê¸°

ERM ê´€ì ì—ì„œì˜ SVM:

ğŸ“ ERM-Hinge Form:
$$
\min_{\mathbf{w}, b} \frac{\lambda}{2} \|\mathbf{w}\|^2 + \frac{1}{n} \sum_{i=1}^n \max(0, 1 - y_i(\mathbf{w}^\top \mathbf{x}_i + b))
$$

ğŸ“ Soft-margin SVM (ì œì•½ì‹ ê¸°ë°˜):
$$
\min_{\mathbf{w}, b, \boldsymbol{\xi}} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^n \xi_i \\
\text{s.t. } y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0
$$

- ë‘ ë°©ì‹ì€ $\lambda = \frac{1}{C}$ì¼ ë•Œ **ë™ì¹˜**

---

## âœ… í•µì‹¬ ìš”ì•½

- ERMì€ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ì„¤ê³„í•˜ëŠ” **ê³µí†µ ì´ë¡  í‹€**
- ê° ëª¨ë¸ì€:
  - Loss $L(y, f(x))$
  - Function class $\mathcal{F}$
  - Regularizer $\Omega(f)$
  ë¥¼ ì„ íƒí•˜ì—¬ ì„¤ê³„ë¨
- Convex surrogate lossë¥¼ í†µí•´ **í•™ìŠµ ê°€ëŠ¥í•˜ê³  ì•ˆì •ì ì¸ ëª¨ë¸ êµ¬í˜„**
- **Logistic Regression, SVM, Ridge, Lasso** ëª¨ë‘ ERMì— í¬í•¨ë¨


