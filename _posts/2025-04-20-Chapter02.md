---
title: "Chapter02"
excerpt: "머신러닝 Chap02 전체 흐름"

categories:
  - MachineLearning
tags:
  - [Machine Learning, 머신러닝]

permalink: /categories/MachineLearning/Chapter01_MachineLearning

toc: true
toc_sticky: true

date: 2025-04-20
last_modified_at: 2025-04-20
---


 # 📘 Session 1: Bayes Classifiers

## 📌 세션 개요

이 세션은 머신러닝 분류 문제를 **확률론적 관점**에서 바라보는 접근 방식인  
**베이즈 분류기(Bayes Classifier)**에 대해 다룬다.  
확률 분포, 사후확률(posterior), 사전확률(prior) 등의 개념을 기반으로  
**최적의 분류기**란 무엇인지 이론적으로 도출하며,  
결국 실전에서는 이를 근사하는 **Plug-in Classifier**를 어떻게 사용하는지를 학습한다.

---

## ✅ 핵심 개념 정리

### 🔹 Classification as Function Approximation
- **정의**:
  ```
  f: 𝒳 → 𝒴
  ```
  where:
  - 𝒳: feature/input space
  - 𝒴: label space (e.g., {1, ..., K})

---

### 🔹 Probabilistic Setting
- **Joint distribution** over input and label:
  ```
  p(x, y)
  ```
- 분류는 사실상 **사후확률 $begin:math:text$ p(y|x) $end:math:text$** 를 기반으로 예측하는 문제로 볼 수 있다.

---

### 🔹 Bayes Rule (베이즈 정리)
- **수식**:
  ```
  p(y|x) = p(x|y) p(y) / p(x)
  ```
- **설명**:
  - $begin:math:text$ p(y) $end:math:text$: prior
  - $begin:math:text$ p(x|y) $end:math:text$: class-conditional density
  - $begin:math:text$ p(y|x) $end:math:text$: posterior
  - $begin:math:text$ p(x) = \\sum_{y'} p(x|y') p(y') $end:math:text$

---

### 🔹 Bayes Optimal Classifier (BOC)
- **정의**:
  ```
  h*(x) = argmax_y p(y|x)
  ```
- **설명**:
  - 사후확률이 가장 높은 클래스를 선택
  - 이론적으로 가장 낮은 오류율(베이즈 리스크)을 가지는 분류기

---

### 🔹 Bayes Risk
- **Expected 0-1 Loss**:
  ```
  R(h) = E_{(x, y) ∼ p(x, y)} [𝟙(h(x) ≠ y)]
  ```
- **Bayes classifier $begin:math:text$ h^* $end:math:text$** 는 모든 분류기 중 $begin:math:text$ R(h) $end:math:text$를 최소화함

---

### 🔹 Plug-in Classifier
- **아이디어**:
  - 실제 $begin:math:text$ p(y|x) $end:math:text$를 모른다면, 추정된 분포 $begin:math:text$ \\hat{p}(y|x) $end:math:text$를 사용하여  
    아래와 같이 근사 분류기를 구성함:
  ```
  ĥ(x) = argmax_y ˆp(y|x)
  ```
- **설명**: 실제 분류 모델들은 대부분 이 방식으로 작동함 (ex: Logistic Regression, Naive Bayes 등)

---

### 🔹 Classification Error
- **개념**:
  - Bayes Classifier는 이론적으로 가장 낮은 오류를 가지며,
    어떤 모델이든 이 오류보다 작을 수는 없다.
- **용어**:
  - **Bayes Error** = Minimum achievable error rate

---

## 🔁 세션 흐름 요약

1. 분류 문제를 확률 분포 $begin:math:text$ p(x, y) $end:math:text$ 기반으로 정의함  
2. 베이즈 정리를 통해 **사후확률 $begin:math:text$ p(y|x) $end:math:text$** 을 계산  
3. 이를 기반으로 사후확률이 가장 높은 클래스를 고르는  
   **Bayes Optimal Classifier**가 가장 이상적임을 수학적으로 보임  
4. 현실에서는 분포를 모르기 때문에  
   추정치를 사용하는 **Plug-in Classifier**를 사용함  
5. 머신러닝 분류기의 기본 원칙이 되는 중요한 이론 기반을 학습

---

> 🧠 이 세션은 머신러닝의 분류 문제를 **확률론적 최적화 문제**로 보는 시각을 제공하며,  
> 이후 등장할 LDA, Naïve Bayes, Logistic Regression 등의 이론적 기반이 된다.