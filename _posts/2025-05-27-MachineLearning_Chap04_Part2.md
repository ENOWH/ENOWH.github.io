---
title: "Chapter04_Part2"
excerpt: "MachineLearning_Chap04_Part2"

categories:
  - MachineLearning
tags:
  - [Machine Learning, ë¨¸ì‹ ëŸ¬ë‹]

permalink: /categories/MachineLearning/Chapter04_Part2_MachineLearning

toc: true
toc_sticky: true

date: 2025-05-27
last_modified_at: 2025-05-27
---

# ğŸ“˜ Chapter 4 Part 2: Advanced Unsupervised Learning

## Part 1. Kernel Density Estimation (KDE)
1. Introduction to Density Estimation
2. Applications: Classification, Clustering, Anomaly Detection
3. Kernel Density Estimation
   - Kernel Functions and Properties
   - Effect of Bandwidth (Ïƒ)
4. Model Selection in KDE
   - Integrated Squared Error (ISE)
   - Leave-One-Out Cross Validation (LOOCV)
   - LS-LOOCV Criterion

---

## Part 2. Kernel Principal Component Analysis (Kernel PCA)
1. Motivation and Limitations of Linear PCA
2. Kernel Trick and Nonlinear Feature Mapping
3. KPCA Algorithm
   - Eigenvalue Problem in Feature Space
   - Computing Î± using Kernel Matrix
   - Projection of New Data Points
4. Summary and Implementation Details

---

## Part 3. Spectral Clustering
1. Motivation and Introduction
2. Similarity Graphs and Laplacian
3. Spectral Clustering Algorithm
   - Eigenvectors of Graph Laplacian
   - Embedding and Clustering via k-means
4. Graph Cut Perspective (RatioCut & Ncut)
5. Perturbation Theory and Generalization
6. Nonlinear Dimensionality Reduction Interpretation
7. Model Selection: Number of Clusters

---

# Part 1. Kernel Density Estimation (KDE)

## âœ… ì£¼ìš” ê°œë… íë¦„
Kernel Density Estimationì€ ë¹„ëª¨ìˆ˜ì  ë°©ì‹ìœ¼ë¡œ í™•ë¥  ë°€ë„ í•¨ìˆ˜ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì´ë‹¤.  
í´ë˜ìŠ¤ ë¶„ë¥˜, í´ëŸ¬ìŠ¤í„°ë§, ì´ìƒíƒì§€ ë“± ë‹¤ì–‘í•œ ë¹„ì§€ë„ í•™ìŠµ ì‘ìš©ì—ì„œ ì‚¬ìš©ëœë‹¤.  
ë°ì´í„°ì— ì»¤ë„ í•¨ìˆ˜ë¥¼ ì–¹ì–´ì„œ ë¶€ë“œëŸ¬ìš´ í™•ë¥  ë°€ë„ ê³¡ì„ ì„ ë§Œë“ ë‹¤.

---

## âœ… ê°œë…ê³¼ ìˆ˜ì‹ ì •ë¦¬

### ğŸ¯ ë°€ë„ ì¶”ì • (Density Estimation)
- ëª©í‘œ: ë°ì´í„° $$ \{X_1, \dots, X_n\} $$ë¡œë¶€í„° ë¶„í¬ $$ f(x) $$ ì¶”ì •

---

### ğŸ¯ KDE ê¸°ë³¸ ìˆ˜ì‹

$$
\hat{f}_\sigma(x) = \frac{1}{n} \sum_{i=1}^n k_\sigma(x - X_i)
$$

- $$ k_\sigma(y) = \frac{1}{\sigma^d} k\left(\frac{y}{\sigma}\right) $$
- $$ \sigma $$: Bandwidth (í•µì‹¬ ì¡°ì ˆ íŒŒë¼ë¯¸í„°)

---

### ğŸ¯ ì»¤ë„ í•¨ìˆ˜ì˜ ì¡°ê±´

- ì •ê·œí™”: $$ \int k(y)\,dy = 1 $$
- ë¹„ìŒì„±: $$ k(y) \geq 0 $$
- ëŒ€ì¹­: $$ k(y) = \psi(\|y\|) $$ (radial kernel)

---

### ğŸ¯ ì»¤ë„ í•¨ìˆ˜ ì˜ˆì‹œ

- **Gaussian**:
$$
k(y) = \frac{1}{(2\pi)^{d/2}} \exp\left(-\frac{1}{2} \|y\|^2 \right)
$$

- **Uniform**:
$$
k(y) = \frac{1}{C} \cdot \mathbf{1}_{\|y\| \leq 1}
$$

- ê¸°íƒ€: Epanechnikov, Triangular, Quartic

---

### ğŸ¯ KDEì˜ ì§ê´€

- ê° ë°ì´í„° í¬ì¸íŠ¸ì— ì»¤ë„ì„ í•˜ë‚˜ì”© ì–¹ì–´ ë”í•œ í˜•íƒœ
- ë°ì´í„° ë°€ë„ê°€ ë†’ì€ êµ¬ê°„ì— ë” ë†’ì€ $$ \hat{f}(x) $$ ê°’

---

### ğŸ¯ Bandwidth (Ïƒ)ì˜ ì—­í• 

- **ì‘ì€ Ïƒ** â†’ ë¾°ì¡±í•œ ì¶”ì • (high variance)
- **í° Ïƒ** â†’ ë¶€ë“œëŸ¬ìš´ ì¶”ì • (high bias)

â†’ Bias-Variance íŠ¸ë ˆì´ë“œì˜¤í”„ ì¡´ì¬

---

### ğŸ¯ KDE vs Histogram

| í•­ëª© | KDE | Histogram |
|------|-----|-----------|
| í˜•íƒœ | ì—°ì†ì  | ì´ì‚°ì  |
| ë¯¼ê°ë„ | Ïƒ | bin width / location |
| ë¶€ë“œëŸ¬ì›€ | ë†’ìŒ | ë‚®ìŒ |

---

## âœ… KDE ê¸°ë°˜ ì‘ìš©

### ğŸ”¹ ë¶„ë¥˜ (Classification)
- Bayes classifier:
$$
x \rightarrow \arg\max_k \pi_k g_k(x)
$$

- ì¶”ì •ëœ ë°€ë„ ì‚¬ìš© (Plug-in classifier):
$$
x \rightarrow \arg\max_k \hat{\pi}_k \hat{g}_k(x)
$$

---

### ğŸ”¹ í´ëŸ¬ìŠ¤í„°ë§ (Density-based)
- ëª¨ë“œ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§: ë†’ì€ ë°€ë„ ì˜ì—­ ì¤‘ì‹¬ìœ¼ë¡œ êµ°ì§‘ í˜•ì„±
- Mean-Shift ì•Œê³ ë¦¬ì¦˜ í™œìš© ê°€ëŠ¥

---

### ğŸ”¹ ì´ìƒ íƒì§€ (Anomaly Detection)
- ê·œì¹™:
$$
x \text{ is anomaly if } \hat{f}(x) < \gamma
$$

- $$ \gamma $$: threshold ê°’

---

## âœ… ëª¨ë¸ ì„ íƒ (Bandwidth ì„ íƒ)

### ğŸ¯ ì„±ëŠ¥ ê¸°ì¤€: ISE (Integrated Squared Error)

$$
ISE(\sigma) = \int \left(\hat{f}_\sigma(x) - f(x) \right)^2 dx
$$

â†’ $$ \sigma $$ë¥¼ ISE ìµœì†Œí™”í•˜ë„ë¡ ì„ íƒ

---

### ğŸ¯ ISE ì „ê°œ

$$
ISE(\sigma) = \int \hat{f}_\sigma(x)^2 dx - 2 \int \hat{f}_\sigma(x) f(x) dx + \int f(x)^2 dx
$$

â†’ ë§ˆì§€ë§‰ í•­ì€ $$ \sigma $$ì™€ ë¬´ê´€ â†’ ë¬´ì‹œ

---

### ğŸ¯ ì²« ë²ˆì§¸ í•­ ì¶”ì •

(Gaussian ì»¤ë„ì¼ ê²½ìš°)

$$
\int \hat{f}_\sigma(x)^2 dx = \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1}^n k_{\sqrt{2}\sigma}(X_i - X_j)
$$

---

### ğŸ¯ ë‘ ë²ˆì§¸ í•­ ì¶”ì • (LOOCV ì‚¬ìš©)

$$
\int \hat{f}_\sigma(x) f(x)\,dx \approx \frac{1}{n} \sum_{i=1}^n \hat{f}_{\sigma}^{(-i)}(X_i)
$$

- Leave-one-out KDE:
$$
\hat{f}_{\sigma}^{(-i)}(x) = \frac{1}{n-1} \sum_{j \ne i} k_\sigma(x - X_j)
$$

---

### ğŸ¯ ìµœì¢… ì„±ëŠ¥ í‰ê°€ í•¨ìˆ˜: LS-LOOCV

$$
LS\text{-}LOOCV(\sigma) = \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1}^n k_{\sqrt{2}\sigma}(X_i - X_j) - \frac{2}{n(n - 1)} \sum_{i=1}^n \sum_{j \ne i} k_\sigma(X_i - X_j)
$$

â†’ $$ \sigma $$ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê°’ ì„ íƒ

---

## âœ… ì •ë¦¬ í¬ì¸íŠ¸

- KDEëŠ” ë°ì´í„°ë¡œë¶€í„° ë¶€ë“œëŸ¬ìš´ ë°€ë„ ì¶”ì •ì„ ì œê³µí•˜ëŠ” ë¹„ëª¨ìˆ˜ ê¸°ë²•
- í•µì‹¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” **Bandwidth Ïƒ**
- **ë¶„ë¥˜, í´ëŸ¬ìŠ¤í„°ë§, ì´ìƒíƒì§€**ì— í™œìš©ë¨
- ì ì ˆí•œ Ïƒ ì„ íƒì€ **Cross-Validation ê¸°ë°˜ ISE ìµœì†Œí™”**ë¡œ ìˆ˜í–‰

---

# Part 2. Kernel PCA (KPCA)

## âœ… ì£¼ìš” ê°œë… íë¦„
PCAëŠ” ì„ í˜• êµ¬ì¡°ë¥¼ ê°€ì •í•˜ë¯€ë¡œ ë¹„ì„ í˜• ë°ì´í„°ì— í•œê³„ê°€ ìˆìŒ.  
KPCAëŠ” ë°ì´í„°ë¥¼ **ë¹„ì„ í˜• ê³ ì°¨ì› ê³µê°„**ìœ¼ë¡œ ë§¤í•‘í•œ ë’¤, ê·¸ ê³µê°„ì—ì„œ **ì„ í˜• PCA**ë¥¼ ìˆ˜í–‰í•¨.  
ì´ ë•Œ **ì»¤ë„ íŠ¸ë¦­(kernel trick)**ì„ í†µí•´ ê³ ì°¨ì› ë§¤í•‘ ì—†ì´ë„ ë‚´ì  ê³„ì‚°ë§Œìœ¼ë¡œ PCAë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆìŒ.

---

## âœ… ê°œë…ê³¼ ìˆ˜ì‹ ì •ë¦¬

### ğŸ¯ PCA ìš”ì•½ (ì„ í˜• PCA)

- ê³µë¶„ì‚° í–‰ë ¬:
$$
S = \frac{1}{n} \sum_{i=1}^n \mathbf{x}_i \mathbf{x}_i^\top
$$

- ê³ ìœ ê°’ ë¶„í•´:
$$
S = U \Lambda U^\top
$$

â†’ $$ u_1, u_2, \dots $$: ì£¼ì„±ë¶„ ë°©í–¥  
â†’ $$ \mathbf{x}_i $$ë¥¼ $$ u_j $$ì— íˆ¬ì˜í•˜ì—¬ ì°¨ì› ì¶•ì†Œ

---

### ğŸ¯ KPCA í•µì‹¬ ì•„ì´ë””ì–´

1. ë°ì´í„° $$ \mathbf{x}_i \in \mathbb{R}^d $$ë¥¼ ê³ ì°¨ì› ê³µê°„ìœ¼ë¡œ ë§¤í•‘:  
   $$ \Phi(\mathbf{x}_i) \in \mathcal{H} $$

2. ê³µë¶„ì‚° í–‰ë ¬:
$$
\bar{S} = \frac{1}{n} \sum_{i=1}^n \Phi(\mathbf{x}_i) \Phi(\mathbf{x}_i)^\top
$$

3. ì£¼ì„±ë¶„ ë°©í–¥ $$ u_j $$ëŠ” ë‹¤ìŒì„ ë§Œì¡±:
$$
\bar{S} u_j = \lambda_j u_j
$$

â†’ ë‹¨, $$ u_j \in \text{span}\{\Phi(x_1), \dots, \Phi(x_n)\} $$ ì´ë¯€ë¡œ:
$$
u_j = \sum_{i=1}^n \alpha_{ji} \Phi(x_i)
$$

---

### ğŸ¯ ê³ ìœ ê°’ ë¬¸ì œ ë³€í™˜ (ì»¤ë„ ê¸°ë°˜)

- ì»¤ë„ í–‰ë ¬ ì •ì˜:
$$
K_{ij} = k(x_i, x_j) = \langle \Phi(x_i), \Phi(x_j) \rangle
$$

- ìµœì¢… ê³ ìœ ê°’ ë¬¸ì œ:
$$
K \alpha_j = n \lambda_j \alpha_j
$$

â†’ $$ \alpha_j $$: ì»¤ë„ ê³µê°„ì—ì„œì˜ ê³ ìœ ë²¡í„°

---

### ğŸ¯ ì •ê·œí™” ì¡°ê±´

- ê³ ì°¨ì› ê³µê°„ì—ì„œì˜ ì •ê·œí™”:
$$
\|u_j\|^2 = \alpha_j^\top K \alpha_j = 1
$$

- ê³ ìœ ë²¡í„° norm:
$$
\|\alpha_j\|^2 = \frac{1}{n \lambda_j}
$$

---

### ğŸ¯ ìƒˆë¡œìš´ ë°ì´í„°ì˜ íˆ¬ì˜

- ìƒˆë¡œìš´ ë°ì´í„° $$ x $$ë¥¼ jë²ˆì§¸ ì„±ë¶„ìœ¼ë¡œ íˆ¬ì˜:
$$
y_j = u_j^\top \Phi(x) = \sum_{i=1}^n \alpha_{ji} k(x_i, x)
$$

â†’ ê³ ì°¨ì› ê³µê°„ ê³„ì‚° ì—†ì´ **ì»¤ë„ë§Œìœ¼ë¡œ íˆ¬ì˜ê°’ ê³„ì‚° ê°€ëŠ¥**

---

### ğŸ¯ KPCA ìš”ì•½ ì•Œê³ ë¦¬ì¦˜

1. ì»¤ë„ í–‰ë ¬ K ê³„ì‚° (Gaussian, Polynomial ë“±)
2. í•„ìš”ì‹œ ì¤‘ì‹¬í™”:
$$
K_{\text{centered}} = K - \frac{1}{n}K\mathbf{1} - \frac{1}{n}\mathbf{1}K + \frac{1}{n^2}\mathbf{1}K\mathbf{1}
$$

3. ê³ ìœ ê°’ ë¶„í•´:  
   $$ K = A \Lambda A^\top $$
4. ê³ ìœ ë²¡í„° ì •ê·œí™”:
   $$ \alpha_j = \frac{1}{\sqrt{n \lambda_j}} \tilde{\alpha}_j $$
5. ìƒˆë¡œìš´ ìƒ˜í”Œ $$ x $$ íˆ¬ì˜:
   $$ y_j = \sum_{i=1}^n \alpha_{ji} k(x_i, x) $$

---

## âœ… ì •ë¦¬ í¬ì¸íŠ¸

- KPCAëŠ” **ì„ í˜• PCAì˜ ì»¤ë„ í™•ì¥**
- ê³ ì°¨ì› ë§¤í•‘ ì—†ì´ ë‚´ì  ê³„ì‚°ë§Œìœ¼ë¡œ **ë¹„ì„ í˜• êµ¬ì¡° ë°˜ì˜ ê°€ëŠ¥**
- ì»¤ë„ í–‰ë ¬ì˜ ê³ ìœ ê°’ ë¶„í•´ë¥¼ í†µí•´ ì£¼ì„±ë¶„ íšë“
- ìƒˆë¡œìš´ ë°ì´í„° íˆ¬ì˜ë„ ì»¤ë„ í•¨ìˆ˜ë§Œìœ¼ë¡œ ê³„ì‚° ê°€ëŠ¥

---

# Part 3. Spectral Clustering

## âœ… ì£¼ìš” ê°œë… íë¦„
Spectral Clusteringì€ **ê·¸ë˜í”„ ê¸°ë°˜ì˜ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜**ìœ¼ë¡œ,  
ë³µì¡í•˜ê³  ë¹„ì„ í˜•ì ì¸ ë°ì´í„° êµ¬ì¡°ì—ì„œë„ íš¨ê³¼ì ìœ¼ë¡œ êµ°ì§‘í™”í•  ìˆ˜ ìˆë‹¤.  
í•µì‹¬ ì•„ì´ë””ì–´ëŠ” **ìœ ì‚¬ë„ ê·¸ë˜í”„ â†’ ë¼í”Œë¼ì‹œì•ˆ í–‰ë ¬ â†’ ê³ ìœ ë²¡í„°**ë¥¼ ì´ìš©í•˜ì—¬  
ë°ì´í„°ë¥¼ ì„ë² ë”©í•œ í›„ **k-means**ë¡œ êµ°ì§‘í™”í•˜ëŠ” ê²ƒì´ë‹¤.

---

## âœ… ê°œë…ê³¼ ìˆ˜ì‹ ì •ë¦¬

### ğŸ¯ ì…ë ¥ ë°ì´í„°

$$
X = \{x_1, x_2, \dots, x_n\} \subset \mathbb{R}^d
$$

---

### ğŸ¯ ìœ ì‚¬ë„ ê·¸ë˜í”„ & ì»¤ë„ í•¨ìˆ˜

- ì»¤ë„ (ì˜ˆ: Gaussian)
$$
w_{ij} = \exp\left( -\frac{\|x_i - x_j\|^2}{2\sigma^2} \right)
$$

- ìœ ì‚¬ë„ í–‰ë ¬ W (ëŒ€ì¹­)
- degree matrix:
$$
D_{ii} = \sum_{j} w_{ij}
$$

---

### ğŸ¯ Graph Laplacian

- **Unnormalized Laplacian**:
$$
L = D - W
$$

- **Properties**:
  - $$ L $$ì€ symmetric, PSD
  - $$ \lambda_1 = 0, \quad u_1 = \mathbf{1} $$
  - $$ f^\top L f = \frac{1}{2} \sum_{i,j} w_{ij} (f_i - f_j)^2 $$

---

### ğŸ¯ Spectral Clustering ì•Œê³ ë¦¬ì¦˜

1. **ìœ ì‚¬ë„ í–‰ë ¬ W** ìƒì„±
2. **ê·¸ë˜í”„ ë¼í”Œë¼ì‹œì•ˆ L** ê³„ì‚°
3. Lì˜ **ì‘ì€ ê³ ìœ ê°’ Kê°œ**ì— ëŒ€ì‘í•˜ëŠ” **ê³ ìœ ë²¡í„° uâ‚,â€¦,u_K** ì¶”ì¶œ
4. í–‰ë ¬ $$ Y = [u_1,\dots,u_K] \in \mathbb{R}^{n \times K} $$
5. $$ Y $$ì˜ í–‰ ë²¡í„°ë“¤ì— ëŒ€í•´ **k-means** ìˆ˜í–‰
6. ìµœì¢… êµ°ì§‘ ê²°ê³¼ ë°˜í™˜

---

### ğŸ¯ Graph Cut ê´€ì 

- **RatioCut**:
$$
\text{RatioCut}(A_1,\dots,A_K) = \frac{1}{2} \sum_{k=1}^K \frac{W(A_k, \bar{A}_k)}{|A_k|}
$$

- **Ncut (Normalized Cut)**:
$$
\text{Ncut}(A_1,\dots,A_K) = \frac{1}{2} \sum_{k=1}^K \frac{W(A_k, \bar{A}_k)}{\text{vol}(A_k)}
$$

â†’ Spectral Clusteringì€ ì´ëŸ¬í•œ cut ë¬¸ì œë¥¼ **ì™„í™”(relaxation)**í•œ í˜•íƒœë¡œ í•´ê²°

---

### ğŸ¯ RatioCut 2-way ë¶„í•  í•´ì„

- ë²¡í„° $$ f_A $$ ì •ì˜:
$$
f_{Ai} =
\begin{cases}
+ \sqrt{\frac{|\bar{A}|}{|A|}}, & i \in A \\
- \sqrt{\frac{|A|}{|\bar{A}|}}, & i \in \bar{A}
\end{cases}
$$

- ì´ ë•Œ:
$$
f_A^\top L f_A = n \cdot \text{RatioCut}(A, \bar{A})
$$

---

### ğŸ¯ ìµœì í™” ë¬¸ì œ ì™„í™” (Relaxation)

- ì›ë˜ëŠ” NP-hard â†’ ì—°ì† ìµœì í™”ë¡œ ì™„í™”:
$$
\min_{f \in \mathbb{R}^n,\ f \perp \mathbf{1},\ \|f\|^2 = n} \frac{f^\top L f}{f^\top f}
$$

- í•´:
$$
f^* = u_2
$$

â†’ ë‘ ë²ˆì§¸ ê³ ìœ ë²¡í„°ë¡œ ë¶„í•  ê°€ëŠ¥

---

### ğŸ¯ Perturbation ê´€ì 

- ì´ìƒì ì¸ ê²½ìš°: Lì€ block diagonal â†’ null space ì°¨ì› = K
- í˜„ì‹¤ì—ì„œëŠ” block êµ¬ì¡°ì— ì¡ìŒì´ ì¡´ì¬í•¨
- ê·¸ë˜ë„ ê³ ìœ ë²¡í„° $$ u_1,\dots,u_K $$ëŠ” ì—¬ì „íˆ ê° í´ëŸ¬ìŠ¤í„° ì •ë³´ë¥¼ ë‹´ê³  ìˆìŒ
- í–‰ $$ y_i = [u_1(i), u_2(i), ..., u_K(i)] $$ë¥¼ k-meansë¡œ ë¶„ë¥˜

---

### ğŸ¯ Spectral Clustering = ë¹„ì„ í˜• ì°¨ì› ì¶•ì†Œ + k-means

- ìœ ì‚¬ë„ ì»¤ë„ â†’ ë¹„ì„ í˜• êµ¬ì¡° ë°˜ì˜
- Laplacianì˜ ê³ ìœ ë²¡í„° â†’ ë¹„ì„ í˜• ì„ë² ë”©
- ì´í›„ k-means ì ìš©

---

## âœ… ì •ë¦¬ í¬ì¸íŠ¸

- Spectral Clusteringì€ ê·¸ë˜í”„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜
- Laplacian ê³ ìœ ë²¡í„°ë¥¼ ì´ìš©í•´ ë°ì´í„° ì„ë² ë”© í›„ k-means ì ìš©
- RatioCut, Ncut ë“± ê·¸ë˜í”„ ì»·ì„ ì™„í™”í•˜ì—¬ ê³ ìœ ê°’ ë¬¸ì œë¡œ í•´ê²°
- Laplacianì˜ ê³ ìœ ë²¡í„°ëŠ” í´ëŸ¬ìŠ¤í„° êµ¬ì¡°ë¥¼ ë‚´í¬í•¨
- GMM/k-meansê°€ ì˜ ëª»í•˜ëŠ” ë³µì¡ êµ¬ì¡°ë„ ì˜ ë¶„ë¦¬ ê°€ëŠ¥

---

